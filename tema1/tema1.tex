
\section{Introducción. Orden de convergencia}
\label{sec:intro-orden-convergencia}

En matemáticas y en disciplinas relacionadas con el cálculo científico
aparece con frecuencia el problema de hallar los \textit{ceros de una
  función}.
\begin{center}
  \begin{tikzpicture}
    \begin{axis}[ \axisXYmiddle, xtick=\empty, ytick=\empty, legend
      pos = south east ]
      % Draw a curve
      \addplot[domain=-1:3, blue, ultra thick] {-(x+3)*(x-1)*(x-5)};
      % Plot a label at curve root
      \node[coordinate, medium dot, pin=-30:{$\cero$}] at (axis
      cs:1,0) {};
      % Draw the name of curve
      \legend {$f$};
    \end{axis}
  \end{tikzpicture}
\end{center}

Recordemos que, dada una función real de una variable real, $f$, con
dominio $I\subset\Rset$, una \textit{raíz} (o un \textit{cero}) de $f$
es una solución del siguiente problema:
\begin{equation}
  \label{eq:raiz}
  \tag{P}
  \text{Hallar $\cero\in I$ tal que} \quad f(\cero)=0.
\end{equation}

% El cero es simple si
% $f'(\cero)=0$ y múltiple si $f'(\cero)=0$.

Este no es un problema sencillo y, salvo en algunos casos concretos no
disponemos de algoritmos que nos permitan obtener raíces de una
ecuación en un número finito de pasos. Por ejemplo, no existen
fórmulas explícitas para hallar ceros de funciones polinómicas
arbitrarias de grado $n\ge 5$ y el problema es aún más difícil si $f$
no es polinómica. En general, no podemos plantearnos el hallar las
raíces exactas de una ecuación. Más aún cuando en numerosos <<problemas
reales>>, esta función es conocida sólo de forma aproximada.

Recurriremos a métodos numéricos que, usualmente, vendrán dados en
forma de \textit{algoritmos iterativos}, es decir, partiendo de uno o
más datos iniciales, intentaremos construir una sucesión
$\{x_k\}_{k=0}^{\infty}$ tal que
$$
x_k \to \cero,
$$
done $\cero$ es una raíz de $f$. La idea es elegir $N\in\Nset$
<<suficientemente grande>> de forma que $x_N$ sea <<una buena
aproximación>> de $\cero$; escribiremos
$$
\cero\approx x_N.
$$

El \textit{error de truncamiento} (o simplemente, el
\textit{error}) del método en la etapa $k$ se define como:
$$
e_k = |x_k - \alpha|.
$$

Nuestro objetivo es estudiar algoritmos eficientes que nos permitan
determinar de forma aproximada los ceros de una función $f$ en un
intervalo $I=[a,b]$, de tal forma que el error de truncamiento
respecto a la solución exacta sea tan pequeño como deseemos.

Como veremos, no existe <<\textit{el mejor método}>> para el cálculo
de ceros de funciones: algunos son más rápidos, otros requieren una
buena estimación inicial de la raíz, o regularidad en la función\dots En
general, un buen método será \emph{muy general} (es decir, podrá
utilizarse para un rango de funciones muy amplio) y a la vez
\emph{poco costoso} (exigirá pocos recursos de ordenador hacer pequeño
el error con la solución exacta). En general identificaremos los
recursos computacionales con el número de operaciones requeridas por
el algoritmo\footnote{También con la cantidad de memoria en el
  ordenador utilizada, aunque en los algoritmos para resolución de
  ecuaciones 1D suelen poco exigente en cuanto a requerimientos de
  memoria.}. Pero en general, la naturaleza de la función $f$ se
escapa de nuestro control y no conoceremos cuantas operaciones
requiere su evaluación.  Por tanto, será frecuente el \emph{estimar el
  coste del algoritmo en términos del número de evaluaciones de $f$}
(y no del número de operaciones en coma flotante).

% En todo caso, el precio de un aumento de los recursos empleados por un
% algoritmo nos puede compensar si, a cambio, aumenta el orden de
% convergencia.


\subsection*{Orden de convergencia}

Consideremos un método iterativo definido por una sucesión $\{x_k\}$
tal que $\lim x_k=\alpha$.
Para medir la ``rapidez con la que converge el método''
utilizaremos el concepto de orden de convergencia.

\begin{definition}
  \label{def:orden-convergencia}
  Sea $\{x_k\}$ una sucesión convergente a $\alpha$ y sea $p\ge
  1$.
  Decimos que la sucesión tiene \resaltar{orden de convergencia
    exactamente igual} a $p\ge 1$ si existe una constante $C>0$ (y
  $C<1$ si $p=1$) tal que
  \begin{equation}
    \label{eq:orden-convergencia}
    \lim_{k\to+\infty} \frac{e_{k+1}}{e_k^p} = C.
  \end{equation}
  En este caso, a $C$ se le llama constante asintótica de error.
\end{definition}

\begin{remark}
  Asumiendo que $x_k \to \alpha$, tendremos que $e_k \to 0$ (y en
  particular existe $k_o$ tal que $0<e_k<1$, para todo $k\ge
  k_o$).
  Cuando La ecuación~(\ref{eq:orden-convergencia}) se puede
  interpretar en el siguiente sentido: en cada iteración, el error
  disminuye como la potencia $p$ del error en la iteración anterior.
  Este hecho lo podemos escribir, de forma rigurosa, como:
  \begin{equation}
    \label{eq:orden-convergencia-aprox}
    e_{k+1} \approx C e_k^p
  \end{equation}
  (en el caso en el que $p=1$, imponemos que $C<1$ para que la
  sucesión de errores tienda a cero).

  Para dar un sentido más preciso a la
  expresión~(\ref{eq:orden-convergencia-aprox}), obsérvese que una
  sucesión con orden de convergencia $p$
  verifica~\eqref{eq:orden-convergencia}, es decir que para todo
  $\varepsilon>0$ existe $k_\epsilon\in\Nset$ tal que si
  $k\ge k_\epsilon$ se tiene
  ${e_{k+1}}/{e_k^p} \in (C-\varepsilon,C+\varepsilon)$.  Por tanto, el
  error en la etapa $k+1$ está acotado superior e inferiormente por el
  error en la etapa $k$:
  \begin{align*}
    (C-\varepsilon) e_k^p < e_{k+1} <
    (C+\varepsilon)  e_k^p, \quad \forall k \ge k_\epsilon.
  \end{align*}
  Nótese que $(C-\varepsilon)$ y $(C+\varepsilon)$ convergen a $C$
  cuando $\varepsilon\to 0$.
  % La desigualdad anterior significa que (fijando la notación
  % $C^\flat_\varepsilon=C-\varepsilon$ y
  % $C^\sharp_\varepsilon=C+\varepsilon$), el error en la etapa $k+1$
  % está acotado inferior y superiormente por el error en la etapa $k$
  % de la siguiente forma:
  % \begin{align*}
  %   C^\flat_\varepsilon \; e_k^p \le e_{k+1},\\
  %   e_{k+1} \le C^\sharp_\varepsilon \; e_k^p.
  % \end{align*}
\end{remark}

En el caso $p=1$ decimos que la convergencia es (exactamente)
\textit{lineal}. Los casos $p>1$ se llaman de convergencia
\textit{superlineal} (cuadrática para $p=2$, supercuadrática si $p>2$,
cúbica para $p=3$, etc.).

\begin{example}
  \label{rk:2}
  Es muy sencillo comprobar que la sucesión $x_k=2^{-k}$ converge a cero
  con orden de convergencia lineal ($p=1$), con constante asintótica $C=1/2$.
\end{example}


% el que exige $C<1$ para garantizar la disminución del error)
Pero en general, el determinar de forma exacta el orden de
convergencia de un algoritmo no es una tarea trivial. Por suerte,
habitualmente, no será necesario hacerlo y bastará con calcular el
orden en un sentido más débil.

En concreto, es fácil comprobar que si una sucesión tiene orden de
convergencia exactamente igual a $p$, entonces existen una constante a
la que también llamaremos $C>0$ y $k_0\in\Nset$ tales que el error en
la etapa $k+1$ está acotado por el error en la etapa anterior de la
siguiente forma:
\begin{equation}
  \label{eq:orden-convergencia-al-menos-p}
  e_{k+1} \le C e_k^p, \quad \forall k\ge k_0.
\end{equation}

\begin{definition}
  Diremos que un método iterativo (definido por una sucesión $\{x_k\}$
  tal que $\lim x_k=\alpha$) tiene \resaltar{orden de convergencia}
  (al menos) $p\ge 1$ si existen una constante $C>0$ y un número
  entero $k_0>0$ para los que se
  verifica~(\ref{eq:orden-convergencia-al-menos-p})
  \label{def:orden-convergencia-al-menos-p}
\end{definition}


\begin{remark}
  Algunas propiedades:
  \begin{enumerate}
  \item Como se ha comentado, toda sucesión de orden exactamente igual
    a $p$ (es decir, que verifique~(\ref{eq:orden-convergencia}))
    también verifica~(\ref{eq:orden-convergencia-al-menos-p}), es
    decir, tiene orden $p$.
  \item De hecho, se verifica la siguiente propiedad: si un método
    iterativo es de orden exactamente $p$, entonces es de orden $q$,
    para todo $1\le q \le p$. La demostración de esta propiedad se
    deja como ejercicio.
  \end{enumerate}
\end{remark}



% \begin{definition}
%   Diremos que un método iterativo (definido por una sucesión $\{x_k\}$ tal
%   que $\lim x_k=\alpha$) tiene \resaltar{orden de convergencia (al menos)}
%   $p\ge 1$ si existen una constante $C>0$ y un número entero $k_0>0$
%   tal que
%   \begin{equation}
%     \label{eq:orden-convergencia-al-menos-p}
%     e_{k+1} \le C e_k^p \quad \forall k\ge k_0.
%   \end{equation}
%   \label{def:orden-convergencia-al-menos-p}
% \end{definition}

% % \begin{remark}
% %   La desigualdad~(\ref{eq:orden-convergencia-2}) significa que todo
% %   método iterativo con orden de convergencia (exactamente) $p$ tiene
% %   orden de convergencia al menos $p$ para una constante ligeramente
% %   mayor, $C+\epsilon$.
%   Algunos comentarios:
%   \begin{itemize}
%   \item Un método iterativo de orden (al menos) $p$ podría tener,
%     exactamente, un orden mayor que $p$. Visto de otra forma, un método
%     iterativo de orden exactamente $p$ es de orden (al menos) $q$, para
%     todo $1\le q < p$ ($C<1$ si $q=1$).
%     \begin{flushright}
%       \vspace{-0.75em}
%       \scriptsize \em La demostración de lo anterior se deja como ejercicio.
%       \vspace{-0.75em}
%     \end{flushright}
%   \item En relación con lo anterior, aunque la sucesión $x_k$
%     verifique la desigualdad~(\ref{eq:orden-convergencia-al-menos-p}),
%     no tiene por qué existir el límite~(\ref{eq:orden-convergencia}). En
%     concreto de~(\ref{eq:orden-convergencia-al-menos-p}) sólo podemos
%     concluir que
%     \begin{equation*}
%       \limsup_{k\to+\infty} \frac{e_{k+1}}{e_k^p} < +\infty.
%     \end{equation*}
%     De hecho, no es difícil comprobar que la condición anterior es no
%     solo necesaria, sino suficiente para que la sucesión $x_{k}$ tenga
%     orden (al menos) $p$.
%   \item Análogamente a la
%     observación~\ref{rk:interpretacion-orden-convergencia}, la
%     desigualdad~(\ref{eq:orden-convergencia-al-menos-p}) garantiza que
%     en un esquema de orden (al menos) $p$ tiene lugar el decrecimiento del
%     error absoluto en términos de su potencia $p$ (o del factor
%     $C<1$ si $p=1$).
%   \end{itemize}

El resto de este capítulo se estructura de la siguiente forma: En una
primera sección, repasamos algunos resultados teóricos para asegurar
la existencia y unicidad de solución de~(\ref{eq:raiz}). En el resto
de las secciones se estudian distintos métodos iterativos para el
cálculo de ceros de funciones de una variable, partiendo del que es,
conceptualmente, más sencillo (el método de Bisección) y llegando
hasta el método de Newton y algunas de sus variantes.  La última
sección ofrece algunas indicaciones sobre la generalización de estos
métodos para resolver sistemas de ecuaciones no lineales.

\section{Existencia y unicidad de solución. Separación de ceros}
\label{sec:tema1:exist-y-unic}

Antes abordar la resolución numérica de cualquier problema es
fundamental el realizar, en una etapa previa, un análisis del mismo
que nos permita determinar la existencia de solución. En caso
afirmativo, en la mayor parte de los casos será muy importante
asegurarnos de que esta solución es única\footnote{Por ejemplo,
  la falta de unicidad de sistemas de ecuaciones lineales está
  asociada con la singularidad de las matrices asociadas. O, en
  métodos iterativos, la no unicidad puede dar pie a
  oscilaciones espurias, es decir, a sucesiones oscilantes que no
  converjan a una solución.}.

Con frecuencia, el proceso previo para determinar los ceros de una
función consiste en localizar intervalos en los que podamos
garantizar la existencia de una única solución (proceso que recibe el
nombre de \textit{separación de ceros o de soluciones}). Los
siguientes resultados nos proporcionan unas herramientas muy útiles
para ello.

Como primera aproximación, para localizar los ceros puede resultar
útil la representación gráfica de la función, aunque \textit{en
  ningún caso debemos confiar exclusivamente en las gráficas generadas
  por programas informáticos}: éstas deben estar respaldadas por un
análisis que garantice la existencia y unicidad de solución.

En general, este análisis se apoya en siguientes teoremas:
\begin{theorem}[Bolzano]
  \label{thm:bolzano}
  Sea $f:[a,b]\subset \Rset\to\Rset$ una función continua en el
  intervalo $[a, b]$ y supongamos que $f (a)\cdot f (b) < 0$.
  Entonces, existe $c\in(a, b)$ tal que $f (c) = 0$.
\end{theorem}

\begin{theorem}[Rolle]
  \label{thm:rolle}
  Sea $f:[a,b]\subset \Rset\to\Rset$ una función continua en $[a, b]$ y derivable en
  $(a, b)$ tal que $f(a) = f(b)$.
  Entonces existe al menos un valor $c \in (a, b)$ tal que $f'(c) = 0$.
\end{theorem}

El teorema de Bolzano proporciona existencia de solución,
pero no unicidad (una función continua con distinto signo en
$a$ y en $b$ podría tener muchos ceros en $(a,b)$). El teorema de
Rolle puede ser utilizado para obtener el siguiente resultado de
unicidad:

% \end{remark}

% Combinando ambos teoremas, podemos deducir el siguiente resultado de
% existencia y unicidad:

\begin{corollary}
  \label{cor:tema1:exist+unic}
  Sea $f:[a,b]\subset \Rset\to\Rset$ continua en $[a, b]$ y derivable
  en $(a, b)$. Si $f'(x)\ne 0$, para todo $x\in (a, b)$, entonces
  existe a lo sumo una solución de~\eqref{eq:raiz} en el intervalo
  $I=[a,b]$.
\end{corollary}

\begin{proof}
  Por reducción al absurdo, si existieran dos soluciones
  de~\eqref{eq:raiz}, $\cero_1$ y $\cero_2$ (con $\cero_1<\cero_2$),
  tendríamos que $0=f(\cero_1)=f(\cero_2)$. Puesto que $f$ se
  encuentra en las hipótesis del teorema de Rolle en el subintervalo
  $[\cero_1,\cero_2]$, esto implicaría que existe $c$ entre $\cero_1$
  y $\cero_2$ en el que $f'(c)=0$, lo que contradice las hipótesis.
\end{proof}

\begin{remark}
  \label{rk:tema1:exist+unic}

  El teorema anterior puede generalizarse a intervalos de la forma
  $(-\infty,b]$, $[a,+\infty)$ o $(-\infty,+\infty)$. En efecto, en
  estos casos podríamos repetir la demostración anterior (si
  existieran dos raíces, $\cero_1<\cero_2$, el teorema de Rolle
  implicaría que existe $c\in (\cero_1,\cero_2)$ tal que $f'(c)=0$).
\end{remark}

En realidad, el corolario anterior se basa en el hecho de que (como
consecuencia del Teorema de Rolle), entre dos raíces de $f$ hay al
menos una raíz de $f'$.

\begin{remark}[Separación de ceros]
  \label{rk:tema1:separac-ceros}
  % algoritmo para la separación de
  % soluciones de~\eqref{eq:raiz} que será utilizado en los próximos
  % ejemplos.
  Suponiendo continuidad y derivabilidad de $f$ (para
  aplicar el Corolario~\ref{cor:tema1:exist+unic}), % suponemos que la
  % derivada, $f'$, \emph{es continua en $(a,b)$},
  podemos enunciar un algoritmo para la localización de las raíces de
  $f$:
  \begin{enumerate}
  \item Hallar todas las raíces de $f'$, a las que llamaremos
    $\beta_1<\dots<\beta_n$.
  \item Considerar los intervalos $(a,\beta_1)$, $(\beta_1,\beta_2)$,
    $(\beta_2,\beta_3)$, \dots,$(\beta_n,b)$. Puesto que, dentro de
    ellos, $f'(x)\neq 0$, sabemos (según el Corolario~\ref{cor:tema1:exist+unic})
    que existe, a lo sumo, una raíz de $f$ en cada uno de estos
    intervalos.
  \item Utilizar el teorema de Bolzano para detectar en cuales de
    estos intervalos existe realmente una raíz de $f$.
  \end{enumerate}
  El proceso anterior es válido incluso si $a=-\infty$ o $b=+\infty$,
  según la observación~\ref{rk:tema1:exist+unic}.  Sin embargo, aunque es
  útil desde el punto de vista teórico y en algunos casos
  particulares, presenta un importante inconveniente en la práctica:
  la necesidad de determinar las raíces de  $f'$.  En
  realidad, estamos convirtiendo el problema del cálculo de ceros de
  $f$ en otro similar, el cálculo de ceros de $f'$, que podría ser tan
  complicado como el anterior (o más aún).

  Con frecuencia, se utilizan otros razonamientos para determinar
  todos los intervalos, $(\beta_i,\beta_{i+1})$ en los que
  $f(\beta_i)\cdot f(\beta_{i+1})<0$. Por ejemplo, procedimientos de
  tipo bisección (que analizaremos en la próxima sección).
\end{remark}

A continuación, veremos algunos ejemplos en los que se aplican los
resultados anteriores:

\begin{example}
  \label{ex:tema1:separ-soluc-1}
  Nos planteamos el hallar las raíces de la ecuación
  $$
  f(x)=x^3-9x+3 =0.
  $$
  Siguiendo los pasos de la observación~\ref{rk:tema1:separac-ceros},
  calculamos la derivada de $f$,
  $$
  f'(x)=3x^2-9,
  $$
  que sólo se anula en $x=-\sqrt 3,
  x=+\sqrt 3$. Por lo tanto existen, a lo sumo, tres ceros de $f$ que
  están localizados en
  $$
  (-\infty,-\sqrt 3), (-\sqrt 3, +\sqrt 3) \text{ y } (+\sqrt 3,
  +\infty).
  $$

  Solamente resta utilizar el teorema de Bolzano para detectar en
  cuáles de estos intervalos existe una raíz de $f(x)=0$. Como apoyo,
  podemos utilizar un entorno informático para representar la gráfica
  (figura~\ref{fig:tema1:ejemplo-separ-soluc-1}), que en este caso nos
  sugiere que las raíces pueden ser localizadas, concretamente, en los
  intervalos $[-4,-3]\subset (-\infty,-\sqrt 3)$,
  $[0,1]\subset (-\sqrt 3, +\sqrt 3)$ y
  $[2,3]\subset (+\sqrt 3, +\infty)$.
  \begin{figure}
    \begin{graficaTikz}[width=23em, height=15em]
      \begin{axis}[\axisXYmiddle]
        % Draw a curve
        \addplot[domain=-4.0:4.0, blue, ultra thick, samples=40]
        {x^3-9*x+3};
        % Plot a label at curve root
        \node[coordinate, medium dot, pin=95:{$\cero_1$}] at
        (axis cs:-3.15,0) {};
        \node[coordinate, medium dot, pin=85:{$\cero_2$}] at
        (axis cs:0.33,0) {};
        \node[coordinate, medium dot, pin=95:{ $\cero_3$}] at
        (axis cs:2.81,0) {};
      \end{axis}
    \end{graficaTikz}
    \caption{Gráfica de $f(x)=x^3-9x+3$}
    \label{fig:tema1:ejemplo-separ-soluc-1}
  \end{figure}
  Este hecho lo podemos confirmar utilizando el teorema de Bolzano:
  \begin{itemize}
  \item En $[-4,-3]$: $f(-4)=-25$ y $f(-3)=3$, por lo tanto existe (al
    menos) un valor   $\cero_1 \in (-4,3)$ tal que $f(\cero_1)=0$.
  \item En $[0,1]$: $f(0)=3$ y $f(1)=-5$, luego existe
    $\cero_2 \in (0,1)$ tal que $f(\cero_2)=0$.
  \item En $[2,3]$: $f(2)=-7$ y $f(3)=3$, por tanto existe
    $\cero_3 \in (2,3)$ tal que $f(\cero_3)=0$.
  \end{itemize}
\end{example}

\begin{example}
  Determinaremos el número de valores $x\in\Rset$ tales que
  $2x=cos(x)$ y localizaremos estos valores en intervalos (lo que, en
  las próximas secciones, se usará para aplicar métodos numéricos para
  aproximar las soluciones).

  Para ello, planteamos el problema~\eqref{eq:raiz} para la función $f$
  (continua y derivable) definida por
  $$
  f(x)=2x-\cos(x)
  $$
  y, siguiendo las ideas de la
  observación~\ref{rk:tema1:separac-ceros}, comenzamos estudiando la
  derivada
  $$
  f'(x)=2+\sen(x).
  $$
  Puesto que $|\sen(x)|\le 1$, tenemos $f'(x)\neq 0$ para todo
  $x\in\Rset$, luego existe, como máximo, una raíz de $f$ en
  $(-\infty,+\infty)$.

  \begin{figure}
    \begin{graficaTikz}[width=23em, height=15em]
      \begin{axis}[\axisXYmiddle]
        % Draw a curve
        \addplot[domain=-pi:pi+0.3, blue, ultra thick, samples=40]
        {{2*x-cos(deg(x))}};
        % Plot a label at curve root
        \node[coordinate, medium dot, pin=-87:{$\cero$}] at (axis cs:0.45,0) {};
      \end{axis}
    \end{graficaTikz}
    \caption{Gráfica de $f(x)=2x-\cos(x)$}
    \label{fig:tema1:ejemplo-separ-soluc-2}
  \end{figure}
  La gráfica de $f$ (figura~\ref{fig:tema1:ejemplo-separ-soluc-2}) nos
  sugiere que esta raíz, $\alpha$, es positiva. Por ejemplo si
  aplicamos el teorema de bolzano en el intervalo $[0,\pi/2]$ (elegido
  por resultar sencilla la evaluación de $f$) tenemos: $f(0)=-1$ y
  $f(\pi/2)=\pi$.  Así, $x=\alpha$ es el único valor, concretamente
  localizado en $(0,\pi/2)$, tal que $2x=\cos(x)$.
\end{example}

\section{El método de bisección}
\label{sec:tema1:bisecc}

Comenzamos presentando el método más sencillo, basado en el teorema de
Bolzano. Para ello, supongamos que $f$ es una función continua en
$[a,b]$ tal que $f(a)f(b)<0$, por tanto existe algún cero
de $f$ en $(a,b)$, al que llamaremos $\alpha$.

Por simplicidad, supondremos que $\alpha$ es el único cero de $f$ en
$(a,b)$; en otro caso podríamos separar los ceros en subintervalos,
véase la sección~\ref{sec:tema1:exist-y-unic}. Aunque veremos que este
método sigue siendo válido aunque no haya unicidad de solución
(observación~\ref{rk:tema1:unicidad-bisecc}).

La idea del método de bisección es dividir por la mitad el intervalo y
considerar los dos subintervalos resultantes, para seleccionar aquel
en el que $f$ cambia de signo (y que contiene a la raíz, según el
teorema de Bolzano). Con más detalle: el método consiste en construir
una sucesión de intervalos,
\begin{equation}
  \label{eq:tema1:bisecc:0}
  [a,b]=[a_0,b_0] \supset [a_1,b_1] \supset [a_2,b_2] \cdots \supset
  [a_n,b_n] \supset \cdots
\end{equation}
definida a partir de sus puntos medios, $x_n=(a_n+b_n)/2$, tal y como sigue:
\begin{itemize}
\item Como inicialización, tomamos $a_0=a$ y $b_0=b$ y calculamos
  $x_0=(a_0+b_0)/2$.
\item En cada etapa $k\ge 1$, construimos un intervalo $[a_k,b_k]$ a
  partir del anterior, $[a_{k-1}, b_{k-1}]$, de la siguiente forma:
  \begin{enumerate}
  \item Calculamos el punto medio del intervalo anterior,
    $x_{k-1}=(a_{k-1}+b_{k-1})/2$. Si $f(x_{k-1})=0$ hemos terminado
    ($x_{k-1}$ es el cero de $f$).
  \item En otro caso definimos $a_k$ y $b_k$ de forma que $f$ cambie de
    signo en $[a_k,b_k]$:
    \begin{enumerate}
    \item Si $f(a_{k-1})f(x_{k-1})<0$, elegimos $[a_k,b_k]=[a_{k-1}, x_{k-1}]$.
    \item Si $f(x_{k-1})f(b_{k-1})<0$, elegimos $[a_k,b_k]=[x_{k-1}, b_{k-1}]$.
    \end{enumerate}
  \item A continuación, pasamos a la siguiente etapa (incrementamos
    $k$).
  \end{enumerate}
\end{itemize}

En la práctica, es muy improbable que $f(x_k)=0$ y deben establecerse
condiciones para evitar bucles infinitos. Puede verse una
implementación\footnote{Con el fin que las
  implementaciones de los algoritmos sean sencillas, se utilizará el
  lenguaje Python.} de bisección en el
Programa~\ref{pro:metodo-biseccion}.

\begin{example}
  Aplicaremos el método de bisección a la función $f(x)=x^3-9x+3$ en el intervalo
  $[0,1]$, donde sabemos que existe un único cero (según vimos en el
  ejemplo~\ref{ex:tema1:separ-soluc-1}). El método consiste en los
  siguientes pasos (resumidos en el cuadro~\ref{tab:tema1:bisecc}):

  \begin{itemize}
  \item Empezamos seleccionando $[a_0,b_0]=[0,1]$.
  \item En la primera etapa, $k=1$, calculamos
    $x_0=(1+0)/2=0.5$  y
    $f(x_0)=f(1/2)=-11/8=-1.375$. Como $f(a_0)=f(0)=3$, elegimos
    $[a_1,b_1]=[a_0,x_0]=[0,0.5]$.
  \item Repetimos el proceso para $k=2,3,\dots$, obteniendo los resultados que se
    muestran en el cuadro~\ref{tab:tema1:bisecc}.
  \end{itemize}

\end{example}
\begin{table}
  \centering
  \rule{0.99\linewidth}{1.6pt}
  \begin{equation*}
    \begin{array}{l<{\quad}rrrrrr}%{>{$}r<{$}>{$}r<{$}>{$}r<{$}>{$}r<{$}>{$}r<{$}}
      k &  a_k & b_k & x_k & f(a_k) & f(b_k) & f(x_k)
      \\ \toprule%\mbox{}% \mbox{} for avoiding bug with "["
      0 & 0 &1  &  0.5 & 3 & -5 & -1.375
      \\ \noalign{\smallskip}
      1 &  0& 0.5 &  0.25 & 3 & -1.375 & 0.76
      \\ \smallskip
      2 & 0.25 & 0.5 & 0.375 & 0.76 & -1.375 & -0.32
      \\ \smallskip
      3& 0.25 & 0.375 & 0.3125 & 0.76 & -0.32 & 0.21
      \\
      \hfill \vdots \hfill~ & \hfill \vdots \hfill~ &
                                                      \hfill \vdots \hfill~ & \hfill \vdots \hfill~ &
                                                                                                      \hfill \vdots \hfill~ & \hfill \vdots \hfill~
    \end{array}
  \end{equation*}
  \rule{0.99\linewidth}{1.5pt}
  \caption{Método de bisección para $f(x)=x^3-9x-3$ en $[0,1]$.}
  \label{tab:tema1:bisecc}
\end{table}

Como se puede observar en el ejemplo anterior, el método de bisección
genera una sucesión de intervalos $[a_k,b_k]$ que
contienen la solución de~\eqref{eq:raiz} (pues, por construcción,
$f(a_k)f(b_k)<0$). Además, el tamaño $b_k-a_k$ converge a cero (puesto
que, en cada paso, el tamaño se divide por dos), de hecho:
\begin{equation}
  \label{eq:tema1:bisec:1}
  b_k-a_k = \frac{b-a}{2^k} \quad \forall k=0,1,2,\dots
\end{equation}

Así, se tiene el siguiente resultado:
\begin{theorem}
  \label{thm:tema1:bisecc}
  Sea $f$ una función continua en $[a,b]$ tal que $f(a)f(b)<0$.
  \begin{enumerate}
  \item La sucesión $\{x_k\}_{k=0}^\infty$ (definida por los puntos
    medios de los intervalos en el método de bisección) es convergente
    y su límite, $\cero$, es un cero de $f$ en $(a,b)$.
  \item De hecho, se verifica la siguiente cota del error:
    \begin{equation}
      \label{eq:tema1:bisecc:2}
      |x_k-\cero| < \frac{b-a}{2^{k}}.
    \end{equation}
  \end{enumerate}
\end{theorem}

\begin{proof}
  Los intervalos $[a_k,b_k]$ generados por el método de bisección
  verifican~\eqref{eq:tema1:bisecc:0} y $f(a_k)f(b_k)<0$, por lo tanto
  todos ellos contienen algún cero $\alpha$. Y la sucesión $\{x_k\}$
  verifica~(\ref{eq:tema1:bisecc:2}). Teniendo en cuenta que
  $x_k,\cero\in [a_k,b_k]$,
  $$
  |x_k-\cero|\le |b_{k}-a_{k}| \le \frac{b-a}{2^{k}}.
  $$
  En particular, $x_k\to \alpha$ cuando $k\to +\infty$.
\end{proof}

\begin{remark}
  La estimación de error~(\ref{eq:tema1:bisecc:2}) puede mejorarse: de hecho se verifica
    \begin{equation}
      \label{eq:tema1:bisecc:3}
      |x_k-\cero| \le \frac{b-a}{2^{k+1}}.
    \end{equation}
  En efecto, por definición del método de bisección, en la iteración $k+1$ se
  tiene que $x_k=a_{k+1}$ o $x_k=b_{k+1}$ y que
  $\cero \in (a_{k+1},b_{k+1})$. Teniendo esto en cuenta,
  $$
  |x_k-\cero|\le |b_{k+1}-a_{k+1}| \le \frac{b-a}{2^{k+1}}.
  $$
\end{remark}

\begin{remark}[Ventajas e inconvenientes del método de bisección]
  \label{rk:tema1:unicidad-bisecc}
  ~
  \begin{itemize}
  \item El teorema~\ref{thm:tema1:bisecc} es válido incluso si $f$
    tiene \emph{varios ceros} en $[a,b]$. Esto significa una
    \textbf{ventaja} del método de bisección frente a otras
    alternativas: siempre converge hacia una solución,
    independientemente de que ésta sea única, y sin más hipótesis
    sobre $f$ que su continuidad.

    De hecho, el método de bisección no utiliza ningún tipo de
    información acerca de la función y, solamente requiere la
    evaluación de $f$ en los puntos medios de los intervalos. En la
    práctica, es un método sencillo que puede ser útil como ``método
    de arranque'' (con el que realizar iteraciones previas antes de
    pasar a otros métodos de mayor orden), o como base para métodos
    más eficientes.

  \item Sin embargo, tiene como \textbf{inconveniente} su lenta
    \textbf{velocidad de convergencia}. En concreto, se puede
    demostrar que el método no alcanza el orden $1$ en general
    (aunque sí en algunos casos particulares), necesitando por tanto
    muchas más iteraciones que otros métodos que veremos más tarde.
    Este resultado no es inmediato, se pueden consultar los detalles
    en la bibliografía.

    De hecho, la sucesión que genera el método no garantiza siquiera
    que el error se reduzca en todas las iteraciones (podría ocurrir
    que, en una iteración concreta, $x_k$ sea muy cercano a $\alpha$
    pero $x_{k+1}$, el punto medio del nuevo intervalo, se aleje de
    $\alpha$).


  \end{itemize}
\end{remark}

\begin{remark}
  \label{rk:tema1:bisecc:iteraciones}
  La desigualdad~\eqref{eq:tema1:bisecc:2}, o mejor ~\eqref{eq:tema1:bisecc:3},  nos garantiza que, para $n$
  suficiente grande, podemos aproximar un cero de $f$ con un error tan
  pequeño como deseemos. Y, de hecho, dada una tolerancia
  $\varepsilon$, para que $|x_k-\alpha|<\varepsilon$, es suficiente
  hallar $k$ de forma que el segundo miembro
  de~(\ref{eq:tema1:bisecc:3}) sea menor que $\varepsilon$, es decir,
  realizar iteraciones hasta que $(b-a)/2^{k+1} < \varepsilon$.

  Para ello, planteamos la igualdad $(b-a)/2^{k+1} = \varepsilon$ y, despejando $k$, obtenemos:
  \begin{equation*}
    k=\log_2\left(\frac{b-a}{\varepsilon}\right)-1.
  \end{equation*}
  Como, en general, el resultado de la operación no es un número
  entero, el número de iteraciones que realizaremos, $k_0$, será igual
  al primer entero mayor o igual a la expresión anterior.
\end{remark}

\begin{test}[Función ``bisección'']
  El programa~\ref{pro:metodo-biseccion} muestra una implementación
  del método de bisección. Concretamente, la función
  \pythoninline{biseccion(f, a, b)} calcula una solución
  de~\eqref{eq:raiz} en un intervalo $(a,b)$. A través de parámetros
  opcionales se puede determinar una tolerancia, $\varepsilon$, de
  forma que el ciclo de iteraciones se detenga cuando
  $b_k-a_k<\varepsilon$. La igualdad~(\ref{eq:tema1:bisec:1})
  garantiza que así sera para $k$ suficientemente grande (dado
  concretamente en la
  observación~\ref{rk:tema1:bisecc:iteraciones}). Adicionalmente, la
  función permite fijar el número máximo de iteraciones permitidas y
  seleccionar si se actuará de forma silenciosa o bien con
  verbosidad al mostrar los resultados.

  Por ejemplo, el siguiente código \textit{Python} aproxima un cero de
  la función $f(x)$ introducida en el
  ejemplo~\ref{ex:tema1:separ-soluc-1}
  con tolerancia $10^{-2}$:
  \pythonexternal{tema1/src/biseccion-test1.py}
  \begin{pythonoutput}
    \pythonexternal[backgroundcolor=\color{white},title={Resultado}]{tema1/src/biseccion-test1.out}
  \end{pythonoutput}
\end{test}

\begin{program}
  \widepythonexternal{tema1/src/biseccion.py}
  \caption{Una implementación del método de bisección}
  \label{pro:metodo-biseccion}
\end{program}

\section{Los métodos de punto fijo}
\label{sec:metodos-de-punto-fijo}

La teoría de punto constituye una herramienta muy poderosa para
construir métodos numéricos.  En el contexto actual, un problema de
cálculo de raíces del tipo~(\ref{eq:raiz}) puede transformase en un
problema de punto fijo Por ejemplo, escribiendo $g(x)=x-f(x)$,
la ecuación $f(x)=0$ es equivalente a: hallar $x\in\Rset$ tal que
$$
g(x)=0.
$$
En general, habrá más manipulaciones algebraicas para transformar
problemas de cálculo de raíces en problemas de punto fijo y, de
ellas, deberemos elegir la más adecuada (para poder asegurar que el
problema de punto fijo esté bien planteado, véase el
ejemplo~\ref{ex:punto-fijo-1}).

\subsection*{Teoría de punto fijo. Funciones contractivas}

% Repasaremos aquí la teoría analítica de punto fijo de
% funciones de una variable.
\begin{definition}
  Llamamos \resaltar{punto fijo} de una función continua
  $g:I\subset\Rset\to\Rset$ a una solución del siguiente problema:
  \begin{equation}
    \tag{$P_{\text{PF}}$}
    \text{Hallar $\cero\in I$ tal que} \quad \cero=g(\cero).
    \label{eq:punto-fijo}
  \end{equation}
\end{definition}

Geométricamente, una solución de la ecuación $x=g(x)$ se corresponde
con determinar la abscisa correspondiente a un punto de corte entre la gráfica de
la función $y=g(x)$ y la recta $y=x$ (ver
figura~\ref{fig:ejemplo-punto-fijo-1}).

\begin{example}
  La función $g(x)=2-x^2$ tiene dos puntos fijos en $x=1$ y
  $x=-2$, pues $g(1)=2-1=1$ y $g(-2)=2-4=-2$. No tiene más puntos
  fijos en $\Rset$, pues éstos son raíces de $g(x)-x$, que en este
  caso es un polinomio de grado dos. En la
  figura~\ref{fig:ejemplo-punto-fijo-1} se representan geométricamente
  esta función y sus dos puntos fijos.
\end{example}

\begin{figure}
  \begin{graficaTikz}[width=18em, height=15em]
    \begin{axis}[\axisXYmiddle,
      % legend style = {anchor=north west, pos = north east}]
      legend pos = outer north east, legend cell align=left]
      % Draw a curve
      \addplot[domain=-2.4:2.4, blue, ultra thick, samples=40] {2-x^2};
      \addplot[domain=-2.5:2.5, gray, ultra thick, samples=40] {x};
      % Plot a label at curve root
      \node[coordinate, medium dot, pin=0:{\scriptsize$(1,1)$}]
      at (axis cs:1,1) {};
      \node[coordinate, medium dot, pin=-45:{\scriptsize$(-2,-2)$}]
      at (axis cs:-2,-2) {};
      \legend {$y=g(x)$,$y=x$};
    \end{axis}
  \end{graficaTikz}
  \caption{Puntos fijos de $g(x)=2-x^2$}
  \label{fig:ejemplo-punto-fijo-1}
\end{figure}

A continuación, estudiamos dos resultados, el primero de ellos de
\textit{existencia} y el segundo de \textit{unicidad} de solución para
problemas de punto fijo.

\begin{proposition}[Existencia de solución de~(\ref{eq:punto-fijo})]
  \label{pro:existencia-punto-fijo}
  Sea $g:[a,b]\to\Rset$ una función continua en $[a,b]$ y supongamos
  que
  \begin{equation}
    g([a,b])\subset [a,b].
    \label{eq:g[a,b].subset.[a,b]}
  \end{equation}
  Entonces existe al menos un punto fijo de $g$ en $[a,b]$.
\end{proposition}
\begin{proof}
  Debido a la hipótesis~(\ref{eq:g[a,b].subset.[a,b]}),
  \begin{extension}
    La hipótesis~(\ref{eq:g[a,b].subset.[a,b]}) se puede debilitar, en
    concreto es suficiente que $g(x)$ verifique
    $$(g(a)-a)(g(b)-b)\le 0$$.
  \end{extension}
  $g(a)\ge a$ y
  $g(b)\le b$, por lo tanto podemos aplicar el teorema de Bolzano
  (teorema~\ref{thm:bolzano}) a la función $f(x)=x-g(x)$ en $[a,b]$.
\end{proof}


Antes de estudiar la \textit{unicidad} de solución de~(\ref{eq:punto-fijo}),
introduciremos la siguiente definición:

\begin{definition}
  Una función $g$ continua en $[a,b]$ se dice \resaltar{contractiva} en
  $[a,b]$ si existe $\cteContract\in[0,1)$ tal que
  \begin{equation*}
    |g(x)-g(y)| \le \cteContract |x-y|, \quad \forall x,y \in [a,b].
    \label{eq:contractividad}
  \end{equation*}
  A $\cteContract$ se le llama constante de contractividad de $g$ en $[a,b]$.
  \label{def:funcion.contractiva}
\end{definition}

Puesto que $\cteContract<1$, toda función contractiva, $g$, <<contrae las
distancias>> en el sentido de que $|g(x)-g(y)|<|x-y|$ para todo
$x,y\in [a,b]$.


\begin{example}
  La función $g(x)=(x^2-1)/3$ es contractiva en $[-1,1]$, pues
  $$
  |g(x)-g(y)|=\left|\frac{x^2-y^2}{3}\right| = \frac{|x+y|}{3}|x-y| \le
  \cteContract |x-y|,
  $$
  para $\cteContract=\max \big\{ \frac{|x+y|}{3}\ /\ x,y\in [-1,1]
  \big\} =\frac23 <1$.
\end{example}

En la práctica, el utilizar directamente la definición anterior
es complicado. El siguiente resultado proporciona una condición que
se puede verificar más fácilmente, en general.
\begin{proposition}[Condición suficiente de contractividad]
  \label{pro:1}
  Supongamos que $g\in C^0([a,b])$ y derivable en $(a,b)$, tal que
  \begin{equation}
    \cteContract=\sup_{x\in(a,b)} |g'(x)|<1.
    \label{eq:L=sup|g'|<1}
  \end{equation}
  Entonces $g$ es contractiva en $[a,b]$ y $\cteContract$ es una
  constante de contractividad para $g$.
\end{proposition}
\begin{proof}
  Sean $x,y\in [a,b]$, por ejemplo $x<y$. Aplicando el teorema del
  valor medio en $[x,y]$, deducimos que existe $c\in (x,y)$ tal que
  \begin{equation*}
    |g(x)-g(y)|=|g'(c)(x-y)| \le \cteContract |x-y|,
  \end{equation*}
  donde $\cteContract$ viene dada por~(\ref{eq:L=sup|g'|<1}).
\end{proof}

\begin{remark}
  Si $g'$ es continua en $[a,b]$, para la
  condición~(\ref{eq:L=sup|g'|<1}) es suficiente\footnote{Y de hecho,
    ambas condiciones son equivalentes (demostración: ejercicio)} que
  $g'(x)<1$ para todo $x\in[a,b]$.  En efecto,
  $\sup_{x\in(a,b)}|g'(x)| \le \sup_{x\in[a,b]} |g'(x)|<1$ porque,
  debido al teorema de Weierstrass, $g'$ alcanza este supremo en algún
  punto $c\in [a,b]$. La implicación recíproca se deja como ejercicio.
  \label{rk:3}
\end{remark}

Ahora es sencillo demostrar la \textit{unicidad} de solución de
problemas de punto fijo (para funciones contractivas):
\begin{proposition}[Unicidad de solución de~(\ref{eq:punto-fijo})]
  \label{pro:unicidad-punto-fijo}
  Sea $g:[a,b]\to\Rset$ una función \emph{contractiva} en
  $[a,b]$. Entonces $g$ posee, a lo sumo, un punto fijo en $[a,b]$.
\end{proposition}

\begin{proof}
  Si suponemos que $g$ tiene dos puntos fijos, $\cero_1$ y
  $\cero_2$, llegamos inmediatamente a una contradicción:
  $$
  |\cero_1-\cero_2| = |g(\cero_1)-g(\cero_2)| \le \cteContract |\cero_1 -
  \cero_2| < |\cero_1-\cero_2|,$$
  donde $\cteContract<1$ es la constante de contractividad de $g$.
\end{proof}

\subsection*{Métodos numéricos de punto fijo}

Los problemas de punto fijo~(\ref{eq:punto-fijo}) dan lugar al
siguiente tipo de esquemas recursivos, conocidos como \textbf{métodos
  de aproximaciones sucesivas} o \resaltar{métodos de punto fijo}:
\begin{equation}
  \tag{$M_{\text{AS}}$}
  \left\{
    \begin{array}{l}
      \text{Dado } x_0\in [a,b], \\
      \text{calcular } x_{k+1}=g(x_k), \quad \forall k\ge 0.
    \end{array}
  \right.
  \label{eq:MAS}
\end{equation}
Véase que para que el método esté \textit{bien definido} es necesario
que $x_k$ se encuentre en el dominio de $g$ para todo $k$.
El siguiente teorema resume los resultados de existencia y unicidad
enunciados en las Proposiciones~\ref{pro:existencia-punto-fijo}
y~\ref{pro:unicidad-punto-fijo}, a la vez que garantiza el buen
planteamiento y la convergencia de~(\ref{eq:MAS}) bajo las hipótesis
de aquellas proposiciones.

\begin{theorem}[Teorema del punto fijo de Banach]
  \label{thm:punto-fijo-Banach}
  Sea $g:[a,b]\to\Rset$ tal que $g([a,b]) \subset [a,b]$ y supongamos
  que $g$ es contractiva en $[a,b]$ con constante de contractividad
  $\cteContract\in [0,1)$. Entonces:
  \begin{enumerate}
  \item
    \label{item:punto-fijo-Banach:1}
    La función $g$ tiene un \textsf{único punto fijo}, $\cero$, en
    $[a,b]$.
  \item
    \label{item:punto-fijo-Banach:2}
    Para todo $x_0\in [a,b]$, el método~(\ref{eq:MAS}) está bien
    definido y es \textsf{convergente} hacia $\alpha$.
  \item En cada etapa de~(\ref{eq:MAS}) se tienen las siguientes
    \textsf{estimaciones} del error absoluto:
    \label{item:punto-fijo-Banach:3}
    \begin{align}
      \label{eq:pto-fijo:cota-a-priori}
      |x_k-\cero| &\le \cteContract |x_{k-1}-\cero| \cdots \le \cteContract^k |x_0-\cero|,
      \\
      \label{eq:pto-fijo:cota-a-posteriori}
      |x_k-\cero| &\le \frac{\cteContract}{1-\cteContract}
                    |x_k-x_{k-1}| \le \dots \le
                    \frac{\cteContract^k}{1-\cteContract}
                    |x_1-x_0|.
    \end{align}
  \end{enumerate}
\end{theorem}

La acotación~(\ref{eq:pto-fijo:cota-a-priori}) llama
\textit{estimación a priori}, mientras que
las acotación del tipo~(\ref{eq:pto-fijo:cota-a-posteriori}) conoce como
\textit{estimación a posteriori} del error.

\begin{proof}~\par
  El punto~\ref{item:punto-fijo-Banach:1} resulta
  directamente de las Proposiciones~\ref{pro:existencia-punto-fijo}
  y~\ref{pro:unicidad-punto-fijo}. Respecto al
  punto~\ref{item:punto-fijo-Banach:2}, para cualquier $x_0\in [a,b]$,
  el método iterativo está bien definido, ya que para $k>1$,
  $x_{k}=g(x_{k-1})$ y $g([a,b])\subset [a,b]$. Acerca de la
  convergencia, dado $x_0\in [a,b]$ podemos aplicar repetidamente la
  función $g$ obteniendo:
  \begin{equation}
    \begin{aligned}
      |x_k-\cero| = &|g(x_{k-1})-g(\cero)| \le \cteContract
      |x_{k-1}-\cero| = \\
      = \cteContract &|g(x_{k-2})-g(\cero)| \le \cteContract^2
      |x_{k-2}-\cero| \le \cdots \le \cteContract^k |x_0-\cero|.
    \end{aligned}\label{eq:1}
  \end{equation}
  Así se tiene la estimación a
  priori~(\ref{eq:pto-fijo:cota-a-priori}) y, como
  $\lambda<1$, podemos concluir que $x_k\to\cero$.
  %
  La demostración la estimación~(\ref{eq:pto-fijo:cota-a-posteriori})
  es algo más técnica y se relega a una nota a pie de página\footnote{
    Para probar~(\ref{eq:pto-fijo:cota-a-posteriori}) comenzamos con
    un razonamiento análogo al anterior. Utilizando que $g$ es
    contractiva (y que $x_k\in [a,b]$ para todo $k\ge 0$) tenemos que
    para todo $n \ge 0$:
    \begin{equation*}
      |x_{n+1}-x_{n}| =
      |g(x_{n})-g(x_{n-1})|\le\cteContract|x_{n}-x_{n-1}| \le \cdots \le \cteContract^n|x_1-x_0|.
    \end{equation*}
    Sean $k,n\in\Nset$, con $n>k$. Lo anterior junto a la
    desigualdad triangular implican:
    \begin{align*}
      |x_n-x_k| &\le |x_n-x_{n-1}| + |x_{n-1}-x_{n-2}| + \cdots +
                  |x_{k+2}-x_{k+1}| + |x_{k+1}-x_{k}|
      \\
                &\le \left(\cteContract^{n-k} + \cteContract^{n-k+1} +\cdots+
                  \cteContract^{2} + \cteContract \right) |x_k-x_{k-1}|
      \\
                &=\left(\frac{\cteContract-\cteContract^{n-k+1}}{1-\cteContract}\right)
                  |x_{k}-x_{k-1}|.
    \end{align*}
    Tomando $n\to+\infty$ en los dos miembros de la desigualdad
    anterior, $x_n\to\alpha$ y $\lambda^{n-k+1}\to 0$, así:
    $$
    |x_k-\cero| \le \frac{\cteContract}{1-\cteContract}
    |x_{k}-x_{k-1}|.
    $$
    Y aplicando reiteradamente la contractividad, se obtienen las
    demás desigualdades de~(\ref{eq:pto-fijo:cota-a-posteriori}).
  }.
\end{proof}

% \begin{algorithm} \begin{python}
%     def punto_fijo(g, x0, tol, max_iters):
%     iter = 0
%     while iter < max_iters:
%     x1 = g(x0)
%     if abs(x1-x0) < tol: return x1
%     x0 = x1
%     iter = iter + 1
%     print "Fallo de convergencia en el método de punto fijo!"
%   \end{python}
%   \caption{Método de punto fijo (o de las aproximaciones sucesivas)}
%   \label{alg:metodo-punto_fijo}
% \end{algorithm}

\begin{remark}~
  \begin{enumerate}
  \item Las estimaciones a priori
    como~(\ref{eq:pto-fijo:cota-a-priori}) son importantes desde el
    punto de vista teórico.  Por ejemplo, en este caso, podemos
    deducir que los métodos de punto fijo son (al menos) de orden 1.
  \item Las estimaciones a posteriori son útiles en la práctica, por
    ejemplo~(\ref{eq:pto-fijo:cota-a-posteriori}) nos indica que,
    cuando el error entre dos iteraciones consecutivas es pequeño,
    entonces el error de truncamiento también es pequeño.  En
    concreto, si $|x_{k+1}-x_k|<\delta$, donde $\delta$ es una
    tolerancia prefijada, entonces
    $|x_{k+1}-\alpha|<\epsilon=\delta\cdot\lambda/(1-\lambda)$.
  \end{enumerate}
\end{remark}

\begin{example}
  Nos proponemos el formular en términos de problemas de punto fijo el
  cálculo de todos los ceros de $f(x)=\sen(x)-4x^2+1$.


  \textsf{Primera etapa:} Determinar cuántos ceros son y en qué
  intervalos se encuentran. Como $f$ es continua y derivable en todo
  $\Rset$, utilizaremos el algoritmo de separación de ceros enunciado
  en la observación~\ref{rk:tema1:separac-ceros}.
  \begin{enumerate}
  \item Debemos primero localizar los ceros de la primera
    derivada, planteando $f'(x)=\cos(x)-8x=0$, problema cuya solución
    no es inmediata.
  \item Aun así, aprovecharemos que $f'(x)$ es a su vez continua y
    derivable, con $f''(x)=-\sin(x)-8$. Como $f''(x)<0$ para todo
    $x\in\Rset$, el corolario~\ref{cor:tema1:exist+unic} (aplicado a
    $f'$) implica que $f'$ tiene un único cero en $\Rset$.
  \item Usando de nuevo el
    corolario~\ref{cor:tema1:exist+unic}, aplicado en esta ocasión a
    $f$, concluimos que $f$ tiene exactamente dos ceros en $\Rset$
    (separados por el cero de $f'$).
  \item Por último, el teorema de Bolzano nos permitirá determinar dos
    intervalos $[a_1,b_1]$ y $[a_2,b_2]$ que contengan estos dos
    ceros. Podemos proceder al análisis de las regiones donde $f$ es
    positiva y negativa, o bien utilizar un programa de ordenador para
    representar la gráfica como ayuda. En cualquier caso, tendremos
    que probar con distintos intervalos hasta encontrar aquellos en
    los que se verifique el Teorema de Bolzano (y más adelante, las
    hipótesis del Teorema de punto fijo). En este caso, una primera
    elección, válida para el Teorema de Bolzano es:
    \begin{itemize}
    \item $[a_1,b_1]=[0, \pi/2]$, pues $f(0)=1>0$ y $f(\pi/2)= 1-\pi^2+1<0$
    \item $[a_l,b_2]=[-\pi/2, 0]$, pues $f(-\pi/2)=-1-\pi^2+1<0$ y
      $f(0)>0$,
    \end{itemize}
    % El problema es que la función de punto fijo $g_1$ que definiremos
    % más abajo no es derivable en $x=-\pi/2$. Por ello, restringiremos
    % este intervalo, definiéndolo tal y como sigue:
    % \begin{itemize}
    % \item $[a_1,b_1]=[-\pi/4, 0]$. Seguimos estando en las hipótesis
    %   del Teorema de Bolzano, pues $f(-\pi/4)=
    %   -\sqrt{2}/2-\pi^2+1=--9.5767\dots<0$ y $f(0)=1>0$,
    % \end{itemize}
  \end{enumerate}

  \textsf{Segunda etapa}: intentamos reescribir, en cada uno de estos
  intervalos, el problema en términos de un esquema de punto fijo que
  esté en las hipótesis del teorema~\ref{thm:punto-fijo-Banach}. Para
  ello, podemos proceder de muchas formas, por ejemplo tomar
  $g(x)=f(x)+x$ o bien intentar despejar $x $ en la ecuación,
  $\sen(x)-4x^2+1=0$. En este caso, optamos por esta última
  posibilidad, llegando a $x=\pm \frac 12 \sqrt{1+\sen(x)}$.
  \begin{itemize}
  \item En $[a_1,b_1]=[0, \pi/2]$, definiremos el
    problema de punto fijo
    $$
    x=g_1(x)=+\dfrac 12 \sqrt{1+\sen(x)}.
    $$
    La función $g_1$ está bien definida (pues $1+\sen(x)\ge 0$) y es
    continua en todo $\Rset$. Veamos que está en las hipótesis del
    teorema de punto fijo. En primer lugar,
    \begin{equation}
      g_1'(x)=\frac{\cos(x)}{4\sqrt{1+\sen(x)}},
      \label{eq:6}
    \end{equation}
    que está bien definida para todo $x\in[0, \pi/2]$, ya que
    $\sen(x)\ge 0$ en este intervalo. Además:
    \begin{enumerate}
    \item $g_1([0,\pi/2])\subset [0,\pi/2]$:
      \begin{itemize}
      \item $g_1(0)=1/2 \in [0, \pi/2],$
      \item $g_1(\pi/2)= \frac12 \sqrt{1+\sen(\pi/2)}=\frac{\sqrt 2}2 \in
        [0, \pi/2]$ (pues $\sqrt 2 < 2 < \pi$)

      \item $g_1$ es monótona (en concreto, creciente) en $[0,\pi/2]$:
        En efecto, como $\cos(x)\ge 0$ si $x\in
        [0, \pi/2]$, $g_1'(x)\ge 0$ en este intervalo (ver~(\ref{eq:6})).
      \end{itemize}
      Por lo tanto,
      $g([0, \pi/2]) \subset [0, \pi/2]$.
    \item Veamos que $g_1$ es contractiva en $[0,\pi/2]$, para lo que
      es suficiente que
      $$|g'(x)|<1 \forall x\in [0, \pi/2]$$
      (ver la
      observación~\ref{rk:3}). Como $\sen(x)$ es creciente en
      $[0, \pi/2]$, en este intervalo se verifica que
      $$4\sqrt{1+\sen(x)}\ge 4\sqrt{1+\sen(0)}=4,$$
      así
      $$
      \left|g'(x)\right| = \frac{|\cos{x}|}{4\sqrt{1+\sen(x)}} \le
      \frac{1}{4\sqrt{\sen(x)+1}} \le \frac 1 4 < 1.
      $$
      En concreto, como constante de contractividad podemos escoger
      $$\lambda=\frac 1 4.$$
    \end{enumerate}
    % \item En $[a_1,b_1]=[-\pi/4, 0]$, como $x<0$, definiremos el
    %   problema de punto fijo
    %   $$
    %   x=g_1(x)=-\dfrac 12 \sqrt{1+\sen(x)}.
    %   $$
    %   La función $g_1$ está bien definida (pues $1+\sen(x)\ge 0$) y es
    %   continua en todo $\Rset$. Veamos que está en las hipótesis del
    %   teorema de punto fijo. En primer lugar,
    %   $$
    %   g_1'(x)=\frac{-\cos(x)}{4\sqrt{1+\sen(x)}},
    %   $$
    %   que está bien definida para todo $x\in[-\pi/4, 0]$, ya que
    %   $\sen(x)>-1$ en este intervalo. Además:
    %   \begin{enumerate}
    %   \item $g_1([a_1,b_1])\subset [a_1,b_1]$: En efecto, $g_1$ es
    %     decreciente en $[-\pi/4,0]$ (pues $\cos(x)\ge 0$ si $x\in
    %     [-\pi/4, 0]$ y por tanto $g_1'(x)\le 0$ en este intervalo).
    %     Como
    %     \begin{align*}
            %             g_1(-\pi/4)&= -\frac12 \sqrt{1+\sen(-\pi/4)}=-0.270\dots\in
                                       %                                        [-\pi/4,0]=[-0.785\dots,0],
                                       %             \\
            %             g_1(0)&=-1/2 \in [-\pi/4,0],
                                  %           \end{align*}
                                  %                                   se tiene que $g([-\pi/4,0])\subset
                                  %                                   [g(0),g(-\pi/4)] \subset [-\pi/4,0]$.
                                  %                                   \item Veamos que $g_1$ es contractiva en $[a_1,b_1]$, para lo que es
                                  %                                   suficiente que $|g'(x)|<1$ en $[a_1,b_1]=[-\pi/4,0]$ (según la
                                  %                                   proposición~\ref{pro:1} y a la observación~\ref{rk:3}).  Como
                                  %                                   $\sen(x)$ es creciente en $[-\pi/4,0]$, en este intervalo se
                                  %                                   verifica que
                                  %                                   $$4\sqrt{1+\sen(x)}\ge 4\sqrt{1+\sen(-\pi/4)}=2.164\dots >1,$$
                                  %                                   así
                                  %                                   $$
                                  %                                   \left|g'(x)\right| = \frac{|\cos{x}|}{4\sqrt{1+\sen(x)}} \le
                                  %                                   \frac{1}{4\sqrt{\sen(x)+1}} < 1.
                                  %                                   $$
                                  %                                   \end{enumerate}
  \item En $[a_2,b_2]=[-\pi/2,0]$ podemos proceder de forma similar,
    tomando $$g_2(x)=-\frac 12 \sqrt{1+\sen(x)}$$
    (necesitamos el signo
    negativo porque queremos que $g([-\pi/2,0])\subset\Rset^{-}$).

    Pero en el intervalo $[-\pi/2,0]$ tenemos una dificultad
    adicional, que aparecerá al considerar la derivada en este
    intervalo:
    $$
    g_2'(x)=\frac{-\cos(x)}{4\sqrt{1+\sen(x)}},
    $$
    En efecto, como $\sen(-\pi/2)=-1$, el denominador se hace cero en
    $[-\pi/2,0]$, por lo tanto la derivada no está bien definida.

    Este problema se resuelve fácilmente: basta considerar cualquier
    intervalo cuyo extremo izquierdo sea mayor que $-\pi/2$. Por
    ejemplo, redefinimos el intervalo como
    $$[a_2,b_2]=[-\pi/4,0].$$
    Procediendo como antes, demostraríamos que
    $g_2([-\pi/4,0])\subset[-\pi/4,0]$ y $g_2$ es contractiva en este
    intervalo, por lo que $g_2$ está en las hipótesis del Teorema del
    punto fijo en $[a_2,b_2]$. El desarrollo de este apartado se deja
    como ejercicio al lector.
  \end{itemize}

  \textsf{Tercera y última etapa}: aplicaremos el método de
  aproximaciones sucesivas~(\ref{eq:MAS}) para aproximar la solución
  contenida en el intervalo $[0,\pi/2]$ (para la otra solución, el
  proceso es análogo). Para ello, podemos utilizar el algoritmo
  ilustrado en el programa~\ref{pro:metodo-puntofijo} para la función
  $g_1$ anterior (partiendo, por ejemplo, de $x_0=1\in [0,\pi/2]$):
  \pythonexternal{tema1/src/puntofijo-test1.py}
  \begin{pythonoutput}
    \pythonexternal[backgroundcolor=\color{white},title={Resultado}]{tema1/src/puntofijo-test1.out}
  \end{pythonoutput}
  \label{ex:punto-fijo-1}
  \begin{program}
    \widepythonexternal{tema1/src/puntofijo.py}
    \caption{Una implementación en lenguaje Python del método de
      aproximaciones sucesivas para el cálculo un punto fijo $x=g(x)$}
    \label{pro:metodo-puntofijo}
  \end{program}

  Como vimos anteriormente, podemos tomar
  $\lambda=1/4$ como constante de contractividad y así usar la
  estimación a posteriori para acotar el error. En concreto, tomando
  $k=6$ en la primera desigualdad
  de~(\ref{eq:pto-fijo:cota-a-posteriori}) y usando que
  $$
  |x_{6}-x_{5}| \simeq 6.12623522578 \cdot 10^{-7},
  $$
  llegamos a que el error para la aproximación $x_6=1.40962396703$ verifica:
  $$
  |x_6-\cero| \le \frac{\cteContract}{1-\cteContract}
  |x_6-x_{5}|
  \simeq \frac{1}{3}\; 6.12623522578 \cdot 10^{-7}.
  $$
  Por último: utilizando las estimaciones a
  posterior~(\ref{eq:pto-fijo:cota-a-posteriori}) podríamos responder,
  realizando una sola iteración, a la siguiente pregunta: ¿cuántas
  iteraciones son necesarias para garantizar que el error sea menor que
  $10^{-15}$? El desarrollo de la respuesta queda como ejercicio
  propuesto.
\end{example}

\begin{remark}[Orden $p$ de~(\ref{eq:MAS})]
  Para hipótesis más restrictivas sobre $g$ se puede llegar a
  \resaltar{orden mayor que uno}. En concreto, se puede demostrar
  que si~(\ref{eq:MAS}) converge a un punto fijo, $\cero$, y si
  $g\in C^p([a,b])$ siendo $p \ge 1$ un entero, con
  \begin{extension}
    (En el caso $p=1$ es necesario exigir $g'(\cero)<1$).
  \end{extension}
  \begin{equation*}
    g'(\cero)=g''(\cero)=\cdots=g^{p-1)}(\cero)=0, \quad
    g^{p)}(\cero)\neq 0,
  \end{equation*}
  entonces~(\ref{eq:MAS}) tiene orden exactamente $p$.
  % \begin{extension}
  %   (Siempre que no ocurra que $x_k=\alpha\ \forall k\ge k_0$).
  % \end{extension}
  La demostración se basa en un desarrollo de Taylor de $g$ hasta
  orden $p$.
  \label{rk:MAS.orden.p}
\end{remark}
%     % \subsection{Ejemplos}
Para terminar, mostramos un resultado que garantiza la convergencia
de~(\ref{eq:MAS}) hacia un punto fijo conocido, sin utilizar hipótesis
globales sobre el intervalo. En concreto, en el siguiente teorema%
\footnote{La demostración del teorema consiste, simplemente, en
  aplicar el teorema~\ref{thm:punto-fijo-Banach} en un intervalo
  adecuado $[x_0-\rho,x_0+\rho]$. En concreto, como $g'(\cero)<1$ y
  $g \in C^1(\cero-\delta,\cero+\delta)$, podemos asegurar que para
  algún $\rho\in(0,\delta)$,
  $$
  |g'(x)|<1 \quad \forall x\in [\cero-\rho, \cero+\rho],
  $$
  luego $g$ es contractiva en $[\cero-\rho, \cero+\rho]$ (debido a
  la proposición~\ref{pro:1} y a la observación~\ref{rk:3}), con
  constante $\lambda<1$.

  Además es fácil ver que $g([\cero-\rho, \cero+\rho]) \subset
  [\cero-\rho, \cero+\rho]$, pues si $x\in[\cero-\rho, \cero+\rho]$
  se tiene $ |x-\cero|<\rho$ y por tanto
  $$
  |g(x)-\cero| = |g(x)-g(\cero)| \le \lambda |x-\cero|< \rho
  \Rightarrow g(x)\in [\cero-\rho, \cero+\rho].
  $$
}
solamente se utilizan hipótesis locales sobre la derivada de $g$.

\begin{theorem}[Convergencia local de los métodos de punto fijo]
  \label{thm:punto-fijo-convergencia-local}
  Sea $\cero$ un punto fijo de una función $g$. Supongamos que existe
  $\delta>0$ tal que $g\in C^1(\cero-\delta,\cero+\delta)$ y que
  $|g'(\cero)|<1$. Entonces:
  \begin{enumerate}
  \item Existe $\rho\in (0,\delta)$ tal que método de punto
    fijo~(\ref{eq:MAS}) está bien definido y converge hacia $\cero$
    (para cualquier inicialización $x_0 \in [x_0-\rho,x_0+\rho]$).
  \item Existe una constante de contractividad $\lambda\in [0,1)$ (que
    depende de $\rho$ y $|g'(\cero)|$) para la que se verifican las
    estimaciones a priori~(\ref{eq:pto-fijo:cota-a-priori}) y a
    posteriori(\ref{eq:pto-fijo:cota-a-posteriori}) en
    $[x_0-\rho,x_0+\rho]$.
  \end{enumerate}
\end{theorem}


\section{El método de Newton}
\label{sec:metodo-de-newton}
\begin{figure}
  % Código Python para calcular los datos de las rectas tangentes
  % f = lambda x: x+x**2+x**5
  % df = lambda x: 1+2*x+5*x**4
  % x0=0.85; y0=f(x0); r0 = lambda x: y0+(x-x0)*df(x0)
  % print "Punto 0:", (x0, y0, df(x0))
  % x1=x0-y0/df(x0); y1=f(x1); r1 = lambda x: y1+(x-x1)*df(x1)
  % print "Punto 1:", (x1, y1, df(x1))
  % x2=x1-y1/df(x1); y2=f(x2); r2 = lambda x: y2+(x-x2)*df(x2)
  % print "Punto 2:", x2
  \begin{graficaTikz}[width=25em, height=17em]
    \def\Ax{0.85} \def\Ay{ 2.0162053125} \def\mA{5.31003125}
    \def\Bx{0.47030255236256846} \def\By{0.7144954568706967} \def\mB{2.185217999486167}
    \def\Cx{0.143334965129}
    \begin{axis}[\axisXYmiddle,
      restrict y to domain=-1:3.5,
      legend pos = outer north east, legend cell align=left,
      ticks=none]
      % Draw a curve
      \addplot[domain=-0.2:1.1, blue, ultra thick, samples=40]
      {x+x^2+x^4};
      % Tangent 1
      \addplot[domain=-0.2:1.1, gray, thick, samples=40]
      {\Ay+(x-\Ax)*\mA};
      \node[coordinate, medium dot]  at (axis cs:\Ax,\Ay) {};
      \addplot[dashed] coordinates {(\Ax,0) (\Ax,\Ay)};
      \node[coordinate, medium dot, pin=-45:{$x_0$}]
      at (axis cs:\Ax,0) {};
      % Tangent 2
      \addplot[domain=-0.2:1.1, darkgreen, thick, samples=40]
      {\By+(x-\Bx)*\mB};
      \node[coordinate, medium dot]  at (axis cs:\Bx,\By) {};
      \addplot[dashed] coordinates {(\Bx,0) (\Bx,\By)};
      \node[coordinate, medium dot, pin=-45:{$x_1$}]
      at (axis cs:\Bx,0) {};
      \legend {$y=f(x)$};
      % Point 3
      \node[coordinate, medium dot, pin=-45:{$x_2$}]
      at (axis cs:\Cx,0) {};
    \end{axis}
  \end{graficaTikz}
  \caption{Interpretación geométrica del método de Newton}
  \label{fig:newton-interpretacion-geometrica}
\end{figure}
El método de Newton (o de Newton-Raphson, o
de la tangente) es uno de los métodos numéricos más utilizados y más
eficientes para el cálculo de raíces.

Históricamente, se dedujo a partir de la siguiente
construcción geométrica
(figura~\ref{fig:newton-interpretacion-geometrica}): Dada
$f:[a,b]\to\Rset$ continua en $[a,b]$ y derivable en $(a,b)$ y dado
$x_0\in [a,b]$, para cada $k\ge 0$ calculamos $x_{k+1}$ como la
abscisa de la intersección del eje $y=0$ con la recta tangente a $f$
en $(x_k,f(x_k))$. Como esta recta tangente viene dada por
$y-f(x_k) = f'(x_k)(x-x_k)$, el \resaltar{método de
  Newton} se formula de la siguiente forma:
\begin{equation}
  \tag{$M_{\text{N}}$}
  \left\{
    \begin{array}{l}
      \text{Dado } x_0\in [a,b], \\ \noalign{\medskip}
      \text{calcular } x_{k+1} = x_k - \dfrac{f(x_k)}{f'(x_k)} , \quad \forall k\ge 0.
    \end{array}
  \right.
  \label{eq:MetNewton}
\end{equation}
Obsérvese que para que el método de Newton esté bien definido debe ser
$f'(x_k)\neq 0$ para todo $k$, lo que en la práctica requiere asumir
$f'\neq 0$ en $\in [a,b]$. Es un método cuya implementación es
sencilla siempre y cuando sea conocida la derivada de  $f$.
% \begin{algorithm}  \begin{python}
%     def newton(f, df, x0, tol, max_iters):
%     iter = 0
%     while iter < max_iters:
%     x1 = x0 - f(x0)/df(x0)
%     if abs(x1-x0) < tol:
%     return x1
%     else:
%     x0 = x1
%     iter = iter + 1
%     print "Fallo de convergencia en el método de Newton!"
%   \end{python}
%   \caption{Método de Newton}
%   \label{alg:metodo-newton}
% \end{algorithm}
Además de su interpretación geométrica, existen otras formas de
introducir el método de Newton:
\begin{enumerate}
\item Mediante el \textbf{desarrollo de Taylor} de $f$: dado $x_n\in [a,b]$,
  \begin{align*}
    f(x)&=f(x_n) + f'(x_n)(x-x_n) + \frac 12 f''(\xi_n)(x-x_n)^2, \quad
          \text{con $\xi_n$ entre $x$ y $x_n$}.
    \\
    \intertext{En particular, para $x=\cero$,}
    0=f(\cero)&=f(x_n) + f'(x_n)(\cero-x_n) + \frac 12 f''(\xi_n)(\cero-x_n)^2.
  \end{align*}
  Suponiendo que $x_n$ sea ``una buena aproximación'' de $\cero$ el
  término $|\cero-x_n|$ es ``pequeño'', luego $(\cero-x_n)^2$ es
  ``mucho más pequeño'' y puede eliminarse el último sumando de la
  ecuación anterior, de donde
  \begin{equation*}
    0 \approx f(x_n) + f'(x_n)(\cero-x_n), \quad \text{es decir} \quad
    \cero \approx x_n - \frac{f(x_n)}{f'(x_n)},
  \end{equation*}
  lo cual justifica que la elección de $x_{n+1}$ dada
  en~(\ref{eq:MetNewton}) es una aproximación de $\cero$.
\item Como una técnica de \textbf{punto fijo} en la que se impone
  orden al menos cuadrático. % La idea es reescribir~(\ref{eq:raiz})
  % en la forma~(\ref{eq:punto-fijo}).
  Para ello observamos que dada $h:[a,b]\to\Rset$ tal que $h(x)\neq 0$
  para toda $x\in[a,b]$,
  \begin{align*}
    x \text{ es solución de~(\ref{eq:raiz})} \ \Leftrightarrow\ & h(x)f(x)=0
                                                                  \ \Leftrightarrow\ x-h(x)f(x)=x \\
    \ \Leftrightarrow\ & x \text{ es solución de~(\ref{eq:punto-fijo}) para }
                         g(x)=x-h(x)f(x).
  \end{align*}
  La idea del método de Newton es elegir $h$ de forma que
  $g'(\cero)=0$, lo que (asumiendo las hipótesis del teorema del punto
  fijo y la regularidad $C^2$ necesaria para poder aplicar la
  observación~\ref{rk:MAS.orden.p}) implicaría que el
  método~(\ref{eq:MAS}) tiene orden al menos dos. Por definición de
  $g$,
  $$
  0 = g'(\cero) = 1-h'(\cero)f(\cero)-h(\cero)f'(\cero)
  $$
  y como $f(\cero)=0$ nos queda
  $$
  0 = 1-h(\cero)f'(\cero).
  $$
  Así, tomando $h(x)=1/f'(x)$, es decir $g(x)=x-f(x)/f'(x)$, llegamos
  a un método de orden 2. El método de Newton no es más que un método
  de punto fijo~(\ref{eq:MAS}) aplicado a esta función.
\end{enumerate}

Esta última justificación de~(\ref{eq:MetNewton}) en términos de un
método de punto fijo nos permitiría, en principio, aplicar toda la
artillería desarrollada en la
sección~\ref{sec:metodos-de-punto-fijo}. El inconveniente es que, de
cara a la existencia y unicidad de solución, el Teorema de punto fijo
impone condiciones que no son fáciles de verificar.  El siguiente
resultado global (con hipótesis sobre el intervalo) ofrece condiciones suficientes
más apropiadas.

\begin{theorem}[Convergencia global del método de Newton]
  Sea $f\in C^2([a,b])$ para la que se verifican las siguientes
  hipótesis:
  \begin{enumerate}[label=($N_{\arabic*}$)]
  \item $f(a)f(b)<0$.
    \label{item:Newton.H1}
  \item  $f'(x)\neq 0$, para todo $x \in [a,b]$.
    \label{item:Newton.H2}
  \item
    El signo de $f''$ no cambia en $[a,b]$ (o sea,
    $f''(x)\ge 0 \ \forall x\in [a,b]$ o $f''(x)\le 0 \ \forall x\in [a,b]$).
    \label{item:Newton.H3}
  \item
    $\max\left\{ \dfrac{|f(a)|}{|f'(a)|}, \dfrac{|f(b)|}{|f'(b)|}
    \right\} \le b-a.$
    \label{item:Newton.H4}
  \end{enumerate}
  Entonces:
  \begin{enumerate}
  \item $f$ tiene una única raíz, $\alpha$, en $[a,b]$.
  \item (\ref{eq:MetNewton}) está bien definido (es decir, para todo
    $x_0\in [a,b]$ se tiene que $\{x_k\} \subset [a,b]$).
  \item (\ref{eq:MetNewton}) es convergente (para cualquier
    inicialización $x_0\in [a,b]$) y $\lim x_k = \cero$.
  \item El orden de (\ref{eq:MetNewton}) es, al menos, cuadrático. En
    concreto, para todo $k\ge 0$,
    \begin{equation}
      |x_{x+1} - \cero| \le \frac {M_2}{2m_1} |x_k-\alpha|^2,
      \label{eq:Newton.orden.2}
    \end{equation}
    donde definimos $m_1:=\min_{x\in[a,b]} |f'(x)|$ y $M_2:=\max_{x\in[a,b]}|f''(x)|$.
  \end{enumerate}
  \label{thm:Newton.convergencia.global}
\end{theorem}

\begin{proof}
  Como $f'$ es continua y la hipótesis~\ref{item:Newton.H2} especifica
  que $f'(x) \neq 0$, tendremos que $f'(x)>0$ o $f'(x)<0$ en $[a,b]$.
  Además, según la hipótesis~\ref{item:Newton.H3}, $f''(x)\ge 0$ o
  $f''(x)\le 0$ en $[a,b]$. En esta demostración supondremos que
  \begin{equation}
    f'(x)>0 \quad \text{y}\quad f''(x)\le 0
    \label{eq:f'>0.f''<=0}
  \end{equation}
  (es decir, $f$ es creciente y convexa, como en la
  figura~\ref{fig:newton-interpretacion-geometrica}). Para las otras
  tres combinaciones posibles ($f'>0$ y $f''\ge 0$, $f'<0$ y $f''\ge 0$,
  $f'<0$ y $f''\le 0$) la demostración es similar a la actual.
  En el caso~\eqref{eq:f'>0.f''<=0}, el hecho de que $f$ sea creciente implica que
  \begin{equation*}
    f(a)<0 \quad\text{y}\quad f(b)>0.
  \end{equation*}
  A continuación, probaremos consecutivamente los cuatro puntos del
  teorema.

  \punto{1} % --------------------------------------------------
  Es evidente que existe al menos una raíz, $\cero$, pues la
  hipótesis~\ref{item:Newton.H1} implica que podemos aplicar el
  Teorema de Bolzano en $[a,b]$. Más aún, como $f'(x)>0$, la raíz es única (véase el
  Corolario~\ref{cor:tema1:exist+unic}).

  \punto{2} % --------------------------------------------------
  El método de Newton es un
  método~(\ref{eq:MAS}), $x_{k+1}=g(x_k)$, para la siguiente función:
  \begin{equation*}
    g(x)=x-\frac{f(x)}{f'(x)}.
  \end{equation*}
  Para que este tipo de métodos estén bien definidos (o sea
  $\{x_k\}\subset [a,b]$) es suficiente que $g([a,b])\subset [a,b]$.
  % Veremos, en concreto, que
  % \begin{equation}
  %   g([a,b])\subset [a,\cero]\subset [a,b].
  %   \label{eq:3}
  % \end{equation}
  Para ello, estudiamos el crecimiento de $g$.
  Como $f\in C^2([a,b])$ y $f'(x)\neq 0$ en
  $[a,b]$, deducimos que $g\in C^1([a,b])$ y
  \begin{equation}
    g'(x)
    = 1-\frac{\left(f'(x)\right)^2-f(x)f''(x)}{\left(f'(x)\right)^2}
    = \frac{f(x)f''(x)}{\left(f'(x)\right)^2}.
    \label{eq:Newton.g'(x)}
  \end{equation}
  Debido a que $f''(x)\le 0$, el signo de $g'(x)$ depende sólo del
  signo de $f(x)$. Así
  \begin{itemize}%[label=\emph{\roman*)}]
  \item Si $x\in [a,\cero]$, $g'(x)\ge 0$, es decir $g$ es creciente en
    $[a,\cero]$.
  \item Si $x\in [\cero,b]$, $g'(x)\le 0$, esto es, $g$ es decreciente
    en $[\cero,b]$.
  \end{itemize}

  Por lo tanto, $\max_{x\in [a,b]} g(x)=g(\alpha)$, lo que implica que
  para todo
  $x\in [a,b]$ se tiene
  $$
  g(x)\le g(\alpha)=\alpha.
  $$
  En particular, como $\alpha\le b$, se tiene que $g(x)\le b$ para
  todo $x\in [a,b]$. Veamos que también $g(x)\ge a$ en $[a,b]$ y así
  concluimos que $g([a,b]) \subset [a,b]$, más en concreto
  \begin{equation}
    g([a,b])\subset [a,\alpha].\label{eq:2}
  \end{equation}

  \begin{itemize}
  \item Si $x\in [a,\cero]$, como $g$ es creciente en este intervalo,
    $$g(x)\ge  g(a) = a-f(a)/f'(a) > a.$$
  \item Si $x\in [\cero,b]$ no basta usar que $g$ es
    decreciente en el intervalo: necesitamos además la
    hipótesis~\ref{item:Newton.H4} que,
    con $f'$ es decreciente (pues $f''\le 0$), significa que:
    \begin{equation}
      \frac{f(b)}{f'(b)} \le b-a.
      \label{eq:Newton.H4.simplificada}
    \end{equation}
    Así
    $$g(x) \ge g(b)=b-f(b)/f'(b) \ge b-(b-a)=a.$$
  \end{itemize}
  Por tanto hemos visto que se verifica~\eqref{eq:2}.

  \punto{3} % --------------------------------------------------
  Veremos que la sucesión $\{x_k\}$ es convergente y su límite es
  $\alpha$ a través del siguiente razonamiento: \textit{(a)} $\{x_k\}$
  está acotada superiormente, \textit{(b)} $\{x_k\}$ es creciente (a
  partir del segundo término), luego existe $\lim x_k=\overline\cero$
  y \textit{(c)} $\overline\cero$ coincide con $\cero$.
  \begin{enumerate}[label=\emph{(\alph*)}]
  \item Para cualquier $x_0\in[a,b]$, consideremos la sucesión
    $\{x_k\}$ definida por~\eqref{eq:MetNewton}. Para cualquier $k\ge
    0$, $x_{k+1}=g(x_{k})$ y como $g([a,b])\subset [a,\cero]$
    (según vimos en~\eqref{eq:2}) tenemos que
    $x_{k+1}\in [a,\alpha]$.
  \item Para ver que la sucesión es creciente a partir del segundo
    término basta darse cuenta de que $f(x_k)\le 0$ para todo $k\ge 1$
    (no necesariamente para el primer término, $k=0$), pues $x_k=g(x_{k-1})\in
    [a,\alpha]$. Por tanto
    $$
    x_{k+1}=g(x_k)=x_k - \frac{f(x_k)}{f'(x_k)} \ge x_k.
    $$
  \item Sea entonces $\overline\alpha = \lim x_k$. Tomando límite en cada
    término de la igualdad
    \begin{equation*}
      x_{k+1}=x_k- \frac{f(x_k)}{f'(x_k)}
      % \end{equation*}
      \quad\text{ llegamos a }\quad
      % \begin{equation*}
      \overline\cero = \overline\cero -
      \frac{f(\overline\cero)}{f'(\overline\cero)},
    \end{equation*}
    por lo que $f(\overline\cero)=0$. Luego $\overline\cero=\cero$ (la
    raíz de $f$ es única).
  \end{enumerate}

  \punto{4} % --------------------------------------------------
  Para estudiar el orden de convergencia, aunque $g'(\cero)=0$ (debido
  a~\eqref{eq:Newton.g'(x)}), no podemos aplicar la
  observación~\ref{rk:MAS.orden.p} debido a que, en principio, $g$ no
  está en las hipótesis del Teorema~\ref{thm:punto-fijo-Banach} (de
  punto fijo) y $g\not\in C^2([a,b])$. Pero sí podemos contar con
  $f\in C^2([a,b])$, por lo que tenemos siguiente desarrollo de
  Taylor:
  \begin{align*}
    0 = f(\cero)&=f(x_k)+f'(x_k)(\cero-x_k)+\frac12 f''(\theta_k)(\cero-x_k)^2,
                  \quad\forall k\ge 0,
    \\
    \intertext{donde $\theta_k\in [a,b]$. Dividiendo por $f'(x_k)$:}
    0 &=  \frac{f(x_k)}{f'(x_k)} + \cero - x_k + \frac{f''(\theta_k)}{2f'(x_k)}(\cero-x_k)^2
  \end{align*}
  y como, por definición del método de Newton,
  $f(x_k)/f'(x_k)=x_k-x_{k+1}$:
  \begin{equation*}
    |\cero-x_{k+1}| =  \frac{|f''(\theta_k)|}{2|f'(x_k)|}|\cero-x_k|^2 \le \frac{M_2}{2m_1} |\cero-x_k|^2,
    \quad\forall k\ge 0.
  \end{equation*}
\end{proof}

\begin{remark}~
  \label{rk:4}
  \begin{enumerate}
  \item El Teorema anterior ofrece una estimación \textit{a
      priori} del error:~\eqref{eq:Newton.orden.2}.  Se puede demostrar%
    \footnote{No desarrollaremos la demostración, ésta se basa
    en un desarrollo de Taylor de orden $2$ entre $x_k$ y
    $x_{k+1}$} también
    la siguiente estimación \textit{a posteriori}:
    \begin{equation}
      |x_{k+1}-\alpha| \le \frac{M_2}{2m_1}|x_{k+1}-x_k|.
      \label{eq:newton.estimac.a.posteriori}
    \end{equation}
    Gracias a ella, si en el algoritmo de Newton realizamos
    iteraciones hasta que la diferencia $|x_{k+1}-x_k|$ es
    suficientemente sea pequeña, tenemos garantizada una buena
    aproximación de la solución exacta. Por ejemplo, dado
    $\varepsilon>0$,
    $$
    \text{si}\quad
    |x_{k+1}-x_k| < \frac{2m_1}{M_2} \varepsilon
    \quad\text{entonces}\quad
    |x_{k+1}-\cero| \le \frac{M_2}{2m_1}|x_{k+1}-x_k|<\varepsilon.
    $$
  \item En la demostración del Teorema anterior hemos probado que la
    sucesión $\{x_k\}$ es monótona a partir del primer segundo
    término, $x_1$. % En concreto, en el caso estudiado en la
    % demostración ($f'>0$ y $f''\le 0$) hemos visto que la sucesión es
    % monótona creciente. Se deja como ejercicio al lector el discutir
    % el crecimiento/decrecimiento en los otros tres casos posibles.
  \item La hipótesis~\ref{item:Newton.H4} es necesaria para (como
    hemos visto en la demostración) garantizar que el método de Newton
    está bien definido (pues $g([a,b]\subset [a,b]$).  El siguiente
    resultado proporciona una condición suficiente (pero no necesaria,
    ver ejercicio 5) muy útil para evitar la
    hipótesis~\ref{item:Newton.H4}:
  \end{enumerate}
\end{remark}

\begin{corollary}[Regla de Fourier]
  Sea $f \in C^2([a,b])$ una función que satisface las
  hipótesis~\ref{item:Newton.H1}--\ref{item:Newton.H3} del
  Teorema~\ref{thm:Newton.convergencia.global}. Si elegimos $x_0=a$
  o $x_0=b$ de forma que se verifique
  \begin{equation}
    \label{eq:4}
    f(x_0) f''(x_0) > 0,
  \end{equation}
  entonces $f$ tiene una única raíz $\alpha$ en $[a,b]$ y la
  sucesión $\{x_k\}$ definida por~\eqref{eq:MetNewton} está bien
  definida, converge monótonamente hacia $\cero$ y se verifican las
  acotaciones~\eqref{eq:Newton.orden.2}
  y~\eqref{eq:newton.estimac.a.posteriori}.
  \label{cor:regla.fourier}
\end{corollary}
La demostración de este resultado es sencilla y se incluye como nota a pie de página%
\footnote{
  Consideremos de nuevo $f'>0$ y $f''<0$ en $[a,b]$ (los otros tres
  casos son similares). Como hemos visto en la demostración del
  Teorema~\ref{thm:Newton.convergencia.global}, la
  hipótesis~\ref{item:Newton.H4} sólo se usa para demostrar que
  $g([a,b])\subset [a,\cero]$. Veremos que ahora podemos demostrar
  este hecho sin necesitar esta hipótesis.

  Como estamos suponiendo $f'>0$ y $f(a)f(b)<0$, tiene que ser
  $f(a)<0$.
  Como $f''<0$, para que se verifique~\eqref{eq:4} tenemos que partir
  de $x_0=a$. En consecuencia, $x_0\in[a,\cero]$.

  Procediendo por inducción, dado $k\ge 0$ tal que $x_k\in [a,\cero]$, y
  también $x_{k+1}\in [a,\cero]$, ya que:
  \begin{itemize}
  \item
    $x_{k+1}=x_k-\frac{f(x_k)}{f'(x_k)} \ge a - \frac{f(x_k)}{f'(x_k)}
    \ge a$,
    pues, $f(x_k) < 0$ (porque $x_k\in [a,\alpha]$) y
    $f'(x)>0$ en $[a,b]$.
  \item
    $x_{k+1}=x_k-\frac{f(x_k)}{f'(x_k)} \le
    \cero-\frac{f(\cero)}{f'(\cero)}=\alpha-0 =\alpha$.
  \end{itemize}
}.

\section{Ejercicios}
\label{sec:ejercicios}

\begin{EjerciciosResueltos}

  \begin{problema}
    Consideremos la siguiente ecuación:
    \begin{equation}
      \label{eq:cero}
      e^{2x-3} = 2x+1.
    \end{equation}

    \begin{enumerate}
    \item Demostrar que existen exactamente dos soluciones,
      $\alpha_1<\alpha_2$, y separarlas en intervalos.

    \item Localizar un intervalo $[a,b]$ que contenga a $\alpha_1$,
      de forma que la siguiente formulación de~(\ref{eq:cero})
      verifique las hipótesis del teorema del punto fijo:
      \begin{equation}
        \label{eq:pto-fijo}
        \text{Hallar } x=g(x), \quad \text{ con } g(x)=\frac{e^{2x-3}-1}2.
      \end{equation}

    \item Sea $x_0\in [a,b]$ y sea $x_k$  la sucesión de
      aproximaciones sucesivas para el problema de punto
      fijo~(\ref{eq:pto-fijo}). Estimar el número mínimo de
      iteraciones para las que el error absoluto es menor que
      $10^{-5}$.

    \item Demostrar que la aproximación mediante el método de Newton
      de~\eqref{eq:cero} converge en el intervalo $[2,3]$ si elegimos
      $x_0=3$.
      Aproximar $\alpha_2$ mediante dos iteraciones del algoritmo de
      Newton a partir de $x_0=3$.
    \end{enumerate}
  \end{problema}

  \begin{solucion}
    \begin{enumerate}

    \item
      Sea $$f(x)=e^{2x-3} - 2x-1.$$ Calculamos las raíces de la
      derivada:
      $$
      f'(x)=2e^{2x-3}-2=0,
      $$
      o sea $e^{2x-3}=1$, es decir, $f'$ tiene un solo cero, $x=3/2$
      y por lo tanto $f$ tiene a lo sumo dos ceros, $\alpha_1$ y
      $\alpha_2$, separados por $x=3/2$.

      Usando el teorema de Bolzano, podemos localizarlos en
      $[-1,3/2]$ y en $[3/2,3]$. O mejor aún:
      \begin{enumerate}
      \item $\alpha_1\in [-1,0]$, pues $f(-1)=e^{-5}+1>0$ y
        $f(0)=e^{-3}-1<0$.
      \item $\alpha_2\in [2,3]$, pues $f(2)=e-5<0$ y
        $f(3)=e^{3}-7 > 0$.
      \end{enumerate}

    \item Tomamos $[a,b]=[-1,0]$. Entonces:
      \begin{enumerate}
      \item $g([a,b])\subset [a,b]$. En efecto, $g$ es monótona
        (creciente) porque
        $$
        g'(x)=2e^{2x-3}>0,
        $$
        y además $g(-1)=e^{-5}/2 - 1/2 \in[-1,0]$ y
        $g(0)=e^{-3}/2-1/2 \in [-1,0]$.
      \item $g$ es contractiva con constante $\lambda<1$. Para ello
        es suficiente ver que $\lambda=\max_{x\in[a,b]} |g'(x)|< 1$.
        Como $$g''(x)=4e^{2x-3}>0,$$ entonces $g'$ es creciente,
        luego alcanza su máximo en $x=0$ y
        $g'(0)=2e^{-3} \approx 0.099<1$. Podemos tomar $\lambda=0.1$
        como cota de la derivada, y por tanto como constante de
        contractividad en $[a,b]$.
      \end{enumerate}

    \item
      Usando la estimación a priori del error~\eqref{eq:pto-fijo:cota-a-priori}:
      \begin{equation*}
        |x_k -\alpha_1| < \lambda^k |x_0 - \alpha_1| \le \lambda^k (b-a),
      \end{equation*}
      por tanto para que $|x_k -\alpha_1| < \varepsilon = 10^{-5}$,
      es suficiente que
      \begin{equation*}
        \lambda^k < \frac{\varepsilon}{(b-a)}.
      \end{equation*}
      Para ello, basta tomar $k$ como el primer entero mayor que
      $k_0$, siendo:
      \begin{equation*}
        k_0\in\Rset \text{tal que } \lambda^{k_0} = \frac{\varepsilon}{(b-a)}.
      \end{equation*}
      En nuestro caso, para $\varepsilon=10^{-5}$, $[a,b]=[-1,0]$ y
      $\lambda=0.1$,
      \begin{equation*}
        k_0 = \frac{\log(10^{-5})}{\log{(0.1)}}=5 \quad \Rightarrow \fbox{k=6}.
      \end{equation*}

    \item Tomamos
      \begin{align*}
        f(x)&=e^{2x-3} -2x-1, \\
        f'(x)&=2e^{2x-3} -2, \\
        f''(x)&=4e^{2x-3}. \\
      \end{align*}
      Para $x_0=3$, se satisfacen las hipótesis de la Regla de
      Fourier en $[2,3]$:
      \begin{enumerate}
      \item $f(2)=e-5<0$, $f(3)=e^3-7>0$.
      \item $f'$ es creciente, por lo tanto $f'(x)>f(2)=2e-2>0$ en
        $[2,3]$.
      \item $f''(x)>0$ en $[2,3]$.
      \item Como $f(3)>0$ y $f''(3)>0$, se verifica $f(3)f''(3)>0$.
      \end{enumerate}
      En consecuencia, el método de Newton es convergente.
      Realizaremos las dos primeras iteraciones:
      \begin{enumerate}
      \item
        $x_1 = x_0 - f(x_0)/f'(x_0) = 3 - (e^3-7)/(4e^3-2) \approx
        2.65718708947.$
      \item $x_2 = x_1 - f(x_1)/f'(x_1) \approx 2.44859042303.$
      \end{enumerate}
    \end{enumerate}
  \end{solucion}
\end{EjerciciosResueltos}

\begin{EjerciciosPropuestos}
  \begin{problema}
    Aproximar mediante el método de bisección las raíces del polinomio
    $$
    p(x) = x^3 + 4x^2 -3x -5,
    $$
    de forma que el error relativo en las estimaciones obtenidas sea
    menor que $10^{-2}$.
  \end{problema}
  \begin{problema}
    Nos proponemos encontrar la solución de la siguiente ecuación en
    el intervalo $[0,\pi]$:
    $$
    \int_0^x (\sen s)^2 \, ds = 1.
    $$

    \begin{enumerate}
    \item Demostrar que el problema es equivalente a hallar un punto
      fijo de la función:
      $$
      g(x)=2 + \frac{\sen(2x)}{2}
      $$
      en el intervalo $[0,\pi]$. \textit{Indicación: calcular la integral
      anterior, usando la siguiente identidad trigonométrica:
      $\sen(a+b)=\sen(a)\cos(b)+\cos(a)\sen(b)$}.
    \item Comprobar que la función $g$ satisface las hipótesis del
      teorema del punto fijo en el intervalo $\displaystyle [1.6,2]\subset
      \bigg(\frac{\pi}{2},\frac{3\pi}{4}\bigg)$.
    \item Determinar el número de iteraciones, $k$, para las que el
      error cometido sea menor que $10^{-8}$.
      \begin{quote}\em\small
        Indicación:
        $|x_k-\alpha|\le \lambda^k|x_0-\alpha|,
        \quad\lambda=\text{c. de contractividad}$
      \end{quote}
    \item Calcular las dos primeras iteraciones.
    \end{enumerate}
  \end{problema}

  % \begin{problema}
  %   Dada la ecuación $f(x)=e^x-(x-2)^2=0$, demostrar que sólo posee una
  %   raíz real.
  %   \begin{flushright}\em\small
  %     Idea: probar que $f'(x)>0$ para todo $x\in\Rset$. Para ello,
  %     cuando $x>2$, aplicar a $e^x$ el teorema del valor medio en
  %     $[2,x]$ para probar que $e^x>2(x-2)$.
  %   \end{flushright}
  % \end{problema}

  % \begin{problema}
  %   Consideremos la ecuación $$f(x)=(x-1)\tan(x)-1=0.$$
  %   \begin{enumerate}
  %   \item Demostrar esta ecuación posee infinitas raíces positivas,
  %     determinando los intervalos que contengan a cada una de ellas.
  %   \item Sea $\alpha>0$ la menor raíz positiva de $f(x)$ y
  %     consideremos el intervalo $[a,b]=[0,\pi/4]$. Reescribir la
  %     ecuación anterior en términos de punto fijo, $x=g(x)$, para
  %     $g(x)$ adecuada. Demostrar que $\alpha\in [a,b]$ y que $g(x)$
  %     verifica las hipótesis del teorema del punto fijo.
  %   \item Partiendo de $x_0=0$, realizar dos iteraciones del método
  %     de punto fijo. Determinar $n\in\Nset$ tal que, a partir de
  %     $x_n$, el error absoluto con la solución exacta sea menor que
   %     $\varepsilon = 10^{-3}$.
   %   \end{enumerate}
   % \end{problema}

   \begin{problema}
     Se sesea aplicar el método de punto fijo para calcular las raíces
     de la ecuación
     $$
     f(x)=2x^2+6e^{-x}-4=0.
     $$
     \begin{itemize}
     \item Reescribir (para cada raíz) la ecuación anterior en términos
       de punto fijo, demostrando que se verifican las hipótesis del
       teorema correspondiente.
     \item Realizar (para cada raíz) cinco iteraciones de punto fijo.
     \item Estimar el error cometido, en cada caso.
     \end{itemize}
   \end{problema}

   \begin{problema}
     Fijado $\gamma>0$, podemos usar el método de Newton para calcular
     raíces reales $n$--ésimas (para cualquier $n\in\Nset$ dado)
     de $\gamma$, resolviendo la ecuación:
     $$
     x^n - \gamma = 0.
     $$
     \begin{enumerate}
     \item Determinar un intervalo y un valor inicial para los que el
       método de Newton sea convergente
       \begin{quote}\em\small
         Indicación: distinguir los casos $\gamma>1$ y $\gamma<1$.
       \end{quote}
     \item Realizando tres iteraciones del método de Newton, aproximar
       los valores de $\sqrt{2}$ y $\sqrt[3]{1/3}$. Estimar, en cada
       caso, el error cometido.
     \end{enumerate}

   \end{problema}


   \begin{problema}
     Consideremos la ecuación
     $$
     x-1 + \log(1+x^2) = 0
     $$
     \begin{enumerate}
     \item Demostrar que existe una única solución, $\alpha\in\Rset$.
     \item Comprobar que se verifican las hipótesis de convergencia
       global del método de Newton en el intervalo $[0,1]$, pero no
       las hipótesis de la regla de Fourier (que es una condición
       suficiente pero no necesaria).
     \item Realizar tres iteraciones del método de Newton a partir de
       $x_0=1$. Si $x_3$ es la aproximación obtenida, acotar el error
       $|x_3-\alpha|$ en función de $|x_1-x_0|$.
     \end{enumerate}
   \end{problema}

   \begin{problema}
     Dada la ecuación $$e^x=(x+1)^2:$$
     \begin{enumerate}
     \item Estudiar sus raíces y localizarlas en intervalos
     \item Utilizar el método de bisección para aproximar la mayor de
       ellas con dos cifras decimales exactas
     \item Para esta raíz, localizar un intervalo en el que se pueda
       garantizar la convergencia del método de Newton. Utilizar este
       método para aproximar esta raíz con cuatro cifras decimales
       exactas.
     \end{enumerate}
   \end{problema}
 \end{EjerciciosPropuestos}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../tema1.tex"
%%% End:
