
\chapter[Ecuaciones de una variable]{Raíces de ecuaciones de una variable%
  \footnote{\licenseInfo}}
\label{cha:ecuaciones-una-variable}

\section{Introducción. Orden de convergencia}
\label{sec:intro-orden-convergencia}

En matemáticas y en disciplinas relacionadas con el cálculo científico
aparece con frecuencia el problema de hallar los \textit{ceros de una
  función}.
\begin{center}
  \begin{tikzpicture}
    \begin{axis}[ \axisXYmiddle, xtick=\empty, ytick=\empty, legend
      pos = south east ]
      % Draw a curve
      \addplot[domain=-1:3, blue, ultra thick] {-(x+3)*(x-1)*(x-5)};
      % Plot a label at curve root
      \node[coordinate, medium dot, pin=-30:{$\cero$}] at (axis
      cs:1,0) {};
      % Draw the name of curve
      \legend {$f$};
    \end{axis}
  \end{tikzpicture}
\end{center}

Recordemos que, dada una función real de una variable real, $f$, con
dominio $I\subset\Rset$, una \textit{raíz} (o un \textit{cero}) de $f$
es una solución del siguiente problema:
\begin{equation}
\label{eq:raiz}
\tag{P}
\text{Hallar $\cero\in I$ tal que} \quad f(\cero)=0.
\end{equation}

% El cero es simple si
% $f'(\cero)=0$ y múltiple si $f'(\cero)=0$.

Este no es un problema sencillo y, salvo en algunos casos concretos no
disponemos de algoritmos que nos permitan obtener raíces de una
ecuación en un número finito de pasos. Por ejemplo, no existen
fórmulas explícitas para hallar ceros de funciones polinómicas
arbitrarias de grado $n\ge 5$ y el problema es aún más difícil si $f$
no es polinómica. En general, no podemos plantearnos el hallar las
raíces exactas de una ecuación. Más aún cuando en <<problemas
reales>>, los coeficientes son conocidos sólo de forma aproximada.

Recurriremos a métodos numéricos que, usualmente, vendrán dados en
forma de \textit{algoritmos iterativos}, es decir, partiendo de uno o
más datos iniciales, intentaremos construir una sucesión
$\{x_k\}_{k=0}^{\infty}$ tal que
$$
x_k \to \cero,
$$
done $\cero$ es una raíz de $f$. La idea es elegir $N\in\Nset$
<<suficientemente grande>> de forma que $x_N$ sea <<una buena
aproximación>> de $\cero$; escribiremos
$$
\cero\approx x_N.
$$

Nuestro objetivo es estudiar algoritmos eficientes que nos permitan
determinar de forma aproximada los ceros de una función $f$ en un
intervalo $I=[a,b]$, de tal forma que el error respecto a la solución
exacta sea tan pequeño como deseemos. Como veremos, no existe
<<\textit{el mejor método}>> para el cálculo de ceros de funciones:
algunos son más rápidos, otros requieren una buena estimación inicial
de la raíz, o regularidad en la función... En general, un buen método
será muy general (es decir, podrá utilizarse para un rango de
funciones muy amplio) y a la vez poco costoso (exigirá pocos recursos
de ordenador hacer pequeño el error con la solución exacta). En el
caso de funciones de una variable, la memoria consumida en el
ordenador será pequeña y los recursos empleados se pueden identificar
con el tiempo de cálculo. Habitualmente, puesto que la naturaleza de
la función $f$ se escapa de nuestro ámbito, no podremos estimar con
exactitud el tiempo de cálculo (tampoco la memoria consumida). Por
ello, estimaremos el coste del algoritmo, más que en función del
número de operaciones en coma flotante, en términos del número de
evaluaciones de $f$. En todo caso, el precio de un aumento de los
recursos empleados por un algoritmo nos puede compensar si, a cambio,
aumenta el orden de convergencia.

\subsection*{Orden de convergencia}

En general, un método iterativo converge más rápidamente cuanto mayor
es su orden de convergencia:

\begin{definition}
  \label{def:orden-convergencia}  
  Diremos que un método iterativo, definido por la sucesión $\{x_k\}$
  tal que $\lim x_k=\alpha$, tiene \textit{orden de convergencia
    (exactamente)} $p\ge 1$ si existe una constante $C>0$ (y $C<1$ si
  $p=1$) tal que
  \begin{equation}
    \label{eq:orden-convergencia}
    \lim_{k\to+\infty} \frac{|x_{k+1}-\alpha|}{|x_k-\alpha|^p} = C.
  \end{equation}
\end{definition}
A $C$ se le llama constante asintótica de error. En el caso $p=1$ 
decimos que la convergencia es \textit{lineal}. Los casos $p>1$ se
llaman de convergencia \textit{superlineal} (cuadrática para $p=2$,
supercuadrática si $p>2$, cúbica para $p=3$, etc.).

Obsérvese que~\eqref{eq:orden-convergencia} implica que, cuando $x_k$
esté suficientemente cerca de $\alpha$,
\begin{equation}
  \label{eq:orden-convergencia-aprox}
   |x_{k+1}-\alpha| \approx C |x_{k}-\alpha|^p,
\end{equation}
en el sentido de que para todo $\varepsilon>0$ existe $k_0\in\Nset$ tal que
\begin{equation*}
   \label{eq:orden-convergencia-2}
  (C-\varepsilon) |x_{k}-\alpha|^p < |x_{k+1}-\alpha| < 
  (C+\varepsilon)  |x_{k}-\alpha|^p, \quad \forall k \ge k_0.
\end{equation*}

\begin{remark}
\label{rk:interpretacion-orden-convergencia}
  La expresión~(\ref{eq:orden-convergencia-aprox}) puede interpretarse
  en el siguiente sentido: puesto que $\lim x_k=\alpha$, para $k$
  suficientemente grande tendremos que el error $|x_k-\alpha|$ es
  menor que uno. Si $p>1$, el método hace decrecer el error en
  términos de esta potencia (multiplicado por la constante asintótica
  de error, $C$, que tiene menos importancia). Si $p=1$ debemos exigir
  $C=1$ para garantizar el descenso del error.\label{rk:1}
\end{remark}
% el que exige $C<1$ para garantizar la disminución del error)
% Con frecuencia, no será necesario precisar exactamente el orden de
% convergencia de un método numérico y bastará determinar el orden en el
% siguiente sentido:

\begin{definition}
  Diremos que un método iterativo, definido por la sucesión $\{x_k\}$ tal
  que $\lim x_k=\alpha$, tiene \textit{orden de convergencia al menos}
  $p\ge 1$ si existen una constante $C>0$ y un número entero $k_0>0$
  tal que
  \begin{equation}
    \label{eq:orden-convergencia-al-menos-p}
    |x_{k+1}-\alpha| \le C |x_k-\alpha|^p \quad \forall k\ge k_0.
  \end{equation}
\end{definition}

%\begin{remark}
  % La desigualdad~(\ref{eq:orden-convergencia-2}) significa que todo
  % método iterativo con orden de convergencia (exactamente) $p$ tiene
  % orden de convergencia al menos $p$ para una constante ligeramente
  % mayor, $C+\epsilon$.
  Nótese que no todo método iterativo que sea de orden al menos $p$ es
  un método de orden $p$. De hecho aunque la sucesión $x_k$ verifique
  la desigualdad~(\ref{eq:orden-convergencia-al-menos-p}), no tiene
  por qué existir el límite~(\ref{eq:orden-convergencia}). En concreto
  de~(\ref{eq:orden-convergencia-al-menos-p}) sólo podemos concluir
  que
  \begin{equation*}
   \limsup_{k\to+\infty} \frac{|x_{k+1}-\alpha|}{|x_k-\alpha|^p} < +\infty
  \end{equation*}
  De hecho, no es difícil comprobar que la sucesión $x_{k}$ tiene
  orden al menos $p$ si y solo si se verifica lo anterior.
%\end{remark}
  Por otro lado, la
  observación~\ref{rk:interpretacion-orden-convergencia} sigue siendo
  válida cuando el orden es al menos $p$.

El resto de este capítulo se estructura de la siguiente forma: En una
primera sección, repasamos algunos resultados teóricos para asegurar
la existencia y unicidad de solución de~(\ref{eq:raiz}). En el resto
de las secciones se estudian distintos métodos iterativos para el
cálculo de ceros de funciones de una variable, partiendo del que es,
conceptualmente, más sencillo (el método de Bisección) y llegando
hasta el método de Newton y algunas de sus variantes.  La última
sección ofrece algunas indicaciones sobre la generalización de estos
métodos para resolver sistemas de ecuaciones no lineales.

\section{Existencia y unicidad de solución. Separación de ceros}
\label{sec:tema1:exist-y-unic}

Antes abordar la resolución numérica de cualquier problema es
fundamental el realizar, en una etapa previa, un análisis del mismo
que nos permita determinar la existencia de solución. En caso
afirmativo, en la mayor parte de los casos será muy importante
asegurarnos de que esta solución es única\footnote{Por ejemplo,
  la falta de unicidad de sistemas de ecuaciones lineales está
  asociada con la singularidad de las matrices asociadas. O, en
  métodos iterativos, la no unicidad puede dar pie a
  oscilaciones espurias, es decir, a sucesiones oscilantes que no
  converjan a una solución.}.

Con frecuencia, el proceso previo para determinar los ceros de una
función consistirá en localizar intervalos en los que podamos
garantizar la existencia de una única solución (proceso que recibe el
nombre de \textit{separación de ceros o de soluciones}). Los
siguientes resultados nos proporcionan unas herramientas muy útiles
para ello.

También nos resultará de utilidad, como primera aproximación, la
representación gráfica de la función, aunque \textit{en ningún caso
  debemos confiar exclusivamente en las gráficas generadas por
  programas informáticos}: éstas deben estar respaldadas por un
análisis numérico que garantice la existencia y unicidad de solución.

\begin{theorem}[Bolzano]
  \label{thm:bolzano}
  Sea $f(x)$ una función continua en un intervalo $[a, b]$ y
  supongamos que $f (a)\cdot f (b) < 0$.  Entonces, existe
  $c\in(a, b)$ tal que $f (c) = 0$.
\end{theorem}

\begin{theorem}[Rolle]
  \label{thm:rolle}
  Sea $f(x)$ una función continua en $[a, b]$ y derivable en
  $(a, b)$ tal que $f(a) = f(b)$.
  Entonces existe al menos un valor $c \in (a, b)$ tal que $f'(c) = 0$.
\end{theorem}

% \begin{remark}[Sobre unicidad en los teoremas de Bolzano y Rolle]
%   \label{rk:tema1:unicidad-bolzano-rolle}
El teorema de Bolzano es un resultado bien conocido en el ámbito del
análisis matemático (como el teorema de Rolle) que proporciona
existencia pero no unicidad de solución. En efecto, una función
continua con distinto signo en $a$ y en $b$ podría tener muchos ceros
$c\in (a,b)$. Con respecto al teorema de Rolle, aunque tampoco implica
unicidad, su recíproco puede ser una herramienta muy útil en este
sentido. Este hecho se remarca en el siguiente
corolario.

% \end{remark}

% Combinando ambos teoremas, podemos deducir el siguiente resultado de
% existencia y unicidad:

\begin{corollary}
  \label{cor:tema1:exist+unic}
  Sea $f(x)$ continua en $[a, b]$ y derivable en $(a, b)$. Si
  $f'(x)\ne 0$, para todo $x\in (a, b)$, entonces existe a lo sumo una
  solución de~\eqref{eq:raiz} en el intervalo $I=[a,b]$.
\end{corollary}

\begin{proof}
  Por reducción al absurdo, si existieran dos soluciones
  de~\eqref{eq:raiz}, $\cero_1$ y $\cero_2$, tendríamos que
  $0=f(\cero_1)=f(\cero_2)$.  Puesto que $f$ se encuentra en las
  hipótesis del teorema de Rolle en el subintervalo
  $[\cero_1,\cero_2]$ (y suponiendo por ejemplo $\cero_1<\cero_2$), esto
  implicaría que existe $c$ entre $\cero_1$ y $\cero_2$ en el que
  $f'(c)=0$, lo que contradice las hipótesis del
  corolario~\ref{cor:tema1:exist+unic}.
\end{proof}

\begin{remark}
  \label{rk:tema1:exist+unic}
  El teorema anterior puede generalizarse a intervalos
  infinito-dimensionales, de la forma $(-\infty,b]$, $[a,+\infty)$ o
  $(-\infty,+\infty)$. En efecto, en estos casos podríamos repetir la
  demostración anterior, pues es en ella es suficiente el poder
  aplicar el teorema de Rolle dentro de cualquier subintervalo
  $[\cero_1,\cero_2]$.
\end{remark}

\begin{remark}[Separación de ceros]
  \label{rk:tema1:separac-ceros}
  % algoritmo para la separación de
  % soluciones de~\eqref{eq:raiz} que será utilizado en los próximos
  % ejemplos.
  Si además de la continuidad y derivabilidad de $f$, suponemos que la
  derivada, $f'$, \emph{es continua en $(a,b)$}, podemos
  enunciar un algoritmo para la separación de las raíces de $f(x)=0$:
  \begin{enumerate}
  \item Hallar todas las raíces de $f'(x)=0$, a las que llamaremos $\beta_1<\dots<\beta_n$.
  \item Considerar los intervalos $(a,\beta_1)$, $(\beta_1,\beta_2)$,
    $(\beta_2,\beta_3)$, \dots,$(\beta_n,b)$. Puesto que, dentro de
    ellos, $f'(x)\neq 0$, sabemos (según el Corolario~\eqref{eq:raiz})
    que existe, a lo sumo, un cero de $f$ en cada uno de estos
    intervalos.
  \item Utilizar el teorema de Bolzano para detectar en cuales de
    estos intervalos existe realmente una raíz de $f(x)=0$.
  \end{enumerate}
  El proceso anterior es válido incluso si $a=-\infty$ o $b=+\infty$,
  según la observación~\ref{rk:tema1:exist+unic}.  Sin embargo, aunque
  útil desde el punto de vista teórico, presenta un importante
  inconveniente en la práctica: no es fácil calcular las raíces de la
  ecuación $f'(x)=0$.  En realidad, estamos convirtiendo el problema
  del cálculo de ceros de $f$ en otro similar, el cálculo de ceros de
  $f'$, que podría tan complicado como el anterior (o más aún).
\end{remark}
Con frecuencia, se utilizan otros razonamientos para determinar
todos los intervalos, $(\beta_i,\beta_{i+1})$ en los que
$f(\beta_i)\cdot f(\beta_{i+1})<0$. Por ejemplo, procedimientos de
tipo bisección (que analizaremos en la próxima sección).

% A continuación, veremos algunos ejemplos en los que se aplican los
% resultados anteriores:

\begin{example}
  \label{ex:tema1:separ-soluc-1} 
  Nos planteamos el hallar las raíces de la ecuación
  $$
  f(x)=x^3-9x+3 =0.
  $$
  Siguiendo los pasos de la observación~\ref{rk:tema1:separac-ceros},
  calculamos la derivada de $f$,
  $$
  f'(x)=3x^2-9,
  $$
  que es continua en todo $\Rset$ y sólo se anula en $x=-\sqrt 3,
  x=+\sqrt 3$. Por lo tanto existen, a lo sumo, tres ceros de $f$ que
  están localizados en
  $$
  (-\infty,-\sqrt 3), (-\sqrt 3, +\sqrt 3) \text{ y } (+\sqrt 3,
  +\infty).
  $$

  Solamente resta utilizar el teorema de Bolzano para detectar en
  cuáles de estos intervalos existe una raíz de
  $f(x)=0$. Como apoyo, podemos utilizar un entorno informático
  para representar la gráfica (figura~\ref{fig:tema1:ejemplo-separ-soluc-1}),
  que en este caso nos sugiere que, concretamente, las raíces pueden
  ser localizadas en los intervalos $[-4,-3]\subset (-\infty,-\sqrt
  3)$, $[0,1]\subset (-\sqrt 3, +\sqrt 3)$ y $[2,3]\subset (+\sqrt 3,
  +\infty)$.
  \begin{figure}
    \label{fig:tema1:ejemplo-separ-soluc-1}
    \begin{graficaTikz}[width=23em, height=15em]
      \begin{axis}[\axisXYmiddle]
        % Draw a curve
        \addplot[domain=-4.0:4.0, blue, ultra thick, samples=40]
        {x^3-9*x+3};
        % Plot a label at curve root
        \node[coordinate, medium dot, pin=95:{$\cero_1$}] at 
        (axis cs:-3.15,0) {};
        \node[coordinate, medium dot, pin=85:{$\cero_2$}] at 
        (axis cs:0.33,0) {};
        \node[coordinate, medium dot, pin=95:{ $\cero_3$}] at 
        (axis cs:2.81,0) {};
      \end{axis}
    \end{graficaTikz}
    \caption{Gráfica de $f(x)=x^3-9x+3$}
  \end{figure}
  En efecto, utilizando el teorema de Bolzano:
  \begin{itemize}
  \item En $[-4,-3]$: $f(-4)=-25$ y $f(-3)=3$, por lo tanto existe (al
    menos) un valor   $\cero_1 \in (-4,3)$ tal que $f(\cero_1)=0$.
  \item En $[0,1]$: $f(0)=3$ y $f(1)=-5$, luego existe
    $\cero_2 \in (0,1)$ tal que $f(\cero_2)=0$.
  \item En $[2,3]$: $f(2)=-7$ y $f(3)=3$, por tanto existe
    $\cero_3 \in (2,3)$ tal que $f(\cero_3)=0$.
  \end{itemize}
\end{example}

\begin{example}
  Determinaremos el número de valores $x\in\Rset$ tales que
  $2x=cos(x)$ y localizaremos estos valores en intervalos (lo que, en
  las próximas secciones, se usará para aplicar métodos numéricos para
  aproximar las soluciones).

  Para ello, planteamos el problema~\eqref{eq:raiz} para la función
  continua y derivable
  $$
  f(x)=2x-\cos(x)
  $$ 
  y, siguiendo las ideas de la
  observación~\ref{rk:tema1:separac-ceros}, comenzamos estudiando la
  derivada
  $$
  f'(x)=2+\sen(x).
  $$
  Puesto que $|\sen(x)|\le 1$, tenemos $f'(x)\neq 0$ para todo
  $x\in\Rset$, luego existe, como máximo, una raíz de $f(x)=0$ en
  $(-\infty,+\infty)$.

  \begin{figure}
    \label{fig:tema1:ejemplo-separ-soluc-2}
    \begin{graficaTikz}[width=23em, height=15em]
      \begin{axis}[\axisXYmiddle] 
        % Draw a curve
        \addplot[domain=-pi:pi+0.3, blue, ultra thick, samples=40]
        {{2*x-cos(deg(x))}}; 
        % Plot a label at curve root 
        \node[coordinate, medium dot, pin=-87:{$\cero$}] at (axis cs:0.45,0) {};
      \end{axis}
    \end{graficaTikz}
    \caption{Gráfica de $f(x)=2x-\cos(x)$}
  \end{figure}
  La gráfica de $f$ (figura~\ref{fig:tema1:ejemplo-separ-soluc-2}) nos
  sugiere que esta raíz, $\alpha$, es positiva. Por ejemplo si
  aplicamos el teorema de bolzano en el intervalo $[0,\pi/2]$ (elegido
  por resultar sencilla la evaluación de $f$) tenemos: $f(0)=-1$ y
  $f(\pi/2)=\pi$.  Así, $x=\alpha$ es el único valor, concretamente
  localizado en $(0,\pi/2)$, tal que $2x=\cos(x)$.
\end{example}

\section{El método de bisección}
\label{sec:tema1:bisecc}

Comenzamos presentando el método más sencillo, basado en el teorema de
Bolzano. Para ello, supongamos que $f$ es una función continua en
$[a,b]$ tal que $f(a)f(b)<0$, por tanto existe algún cero
de $f$ en $(a,b)$, al que llamaremos $\alpha$.

Por simplicidad, supondremos que $\alpha$ es el único cero de $f$ en
$(a,b)$, en otro caso podríamos separar los ceros en subintervalos,
véase la sección~\ref{sec:tema1:exist-y-unic}. Aunque veremos que este
método sigue siendo válido aunque no haya unicidad de solución
(observación~\ref{rk:tema1:unicidad-bisecc}).

La idea del método de bisección es dividir por la mitad el intervalo y
considerar los dos subintervalos resultantes, para seleccionar aquél
en el que $f$ cambia de signo (y que contiene a la raíz, según el
teorema de Bolzano).

Con más detalle: el método consiste en construir una sucesión de
intervalos,
\begin{equation}
\label{eq:tema1:bisecc:0}
[a,b]=[a_0,b_0] \supset [a_1,b_1] \supset [a_2,b_2] \cdots \supset
[a_n,b_n] \supset \cdots
\end{equation}
definida a partir de sus puntos medios, $c_n=(a_n+b_n)/2$, tal y como sigue:
\begin{itemize}
\item Como inicialización, tomamos $a_0=a$ y $b_0=b$ y calculamos
  $c_0=(a_0+b_0)/2$.
\item En cada etapa $k\ge 1$, construimos un intervalo $[a_k,b_k]$ a
  partir del anterior, $[a_{k-1}, b_{k-1}]$, de la siguiente forma:
  \begin{enumerate}
  \item Dado el punto medio del intervalo anterior,
    $c_{k-1}=(a_{k-1}+b_{k-1})/2$, si $f(c_{k-1})=0$ hemos terminado
    (pues $c_{k-1}$ es el cero de $f$).
  \item En otro caso definimos $a_k$ y $b_k$ de forma que $f$ cambie de
    signo en $[a_k,b_k]$:
    \begin{enumerate}
    \item Si $f(a_{k-1})f(c_{k-1})<0$, elegimos $[a_k,b_k]=[a_{k-1}, c_{k-1}]$.
    \item Si $f(c_{k-1})f(b_{k-1})<0$, elegimos $[a_k,b_k]=[c_{k-1}, b_{k-1}]$.
    \end{enumerate}
  \item A continuación, definimos $c_k=(a_k+b_k)/2$ y pasamos a la
    siguiente etapa (incrementamos $k$).
  \end{enumerate}
\end{itemize}

\begin{example}
  Aplicaremos el método de bisección a la función $f(x)=x^3-9x+3$ en el intervalo
  $[0,1]$, donde sabemos que existe un único cero (según vimos en el
  ejemplo~\ref{ex:tema1:separ-soluc-1}). El método consiste en los
  siguientes pasos (resumidos en el cuadro~\ref{tab:tema1:bisecc}):

  \begin{itemize}
  \item Empezamos seleccionando $[a_0,b_0]=[0,1]$ y definiendo
    $c_0=(1-0)/2=0.5$.
  \item En la primera etapa, $k=1$, calculamos
    $f(c_0)=f(1/2)=-11/8=-1.375$. Como $f(a_0)=f(0)=3$, elegimos
    $[a_1,b_1]=[a_0,c_0]=[0,0.5]$.
  \item Repetimos el proceso para $k=2,3,...$, obteniendo los resultados que se
    muestran en el cuadro~\ref{tab:tema1:bisecc}.
  \end{itemize}

\end{example}
\begin{table}
  \centering
  \rule{0.99\linewidth}{1.6pt}
  \begin{equation*}
    \begin{array}{l<{\quad}rrrrr}%{>{$}r<{$}>{$}r<{$}>{$}r<{$}>{$}r<{$}>{$}r<{$}}    
     k &  [a_k,b_k] & c_k & f(a_k) & f(b_k) & f(c_k) 
      \\ \toprule%\mbox{}% \mbox{} for avoiding bug with "["
     0 & [0,1]  &  0.5 
      & 3 & -5 & -1.375
      \\ \noalign{\smallskip}
     1 &  [0,0.5] &  0.25
      & 3 & -1.375 & 0.76
      \\ \smallskip
     2 & [0.25,0.5] & 0.375
      & 0.76 & -1.375 & -0.32
     \\ \smallskip
     3& [0.25,0.375] & 0.3125
      & 0.76 & -0.32 & 0.21
      \\
      \hfill \vdots \hfill~ & \hfill \vdots \hfill~ & 
      \hfill \vdots \hfill~ & \hfill \vdots \hfill~ & \hfill \vdots \hfill~
    \end{array}
  \end{equation*}
  \rule{0.99\linewidth}{1.5pt}
  \caption{Método de bisección para $f(x)=x^3-9x-3$ en $[0,1]$.}
  \label{tab:tema1:bisecc}
\end{table}

Como se puede observar en el ejemplo anterior, el método de bisección
genera una sucesión de intervalos $[a_n,b_n]$ que
contienen la solución de~\eqref{eq:raiz} (pues, por construcción,
$f(a_n)f(b_n)<0$). Además, el tamaño $b_n-a_n$ converge a cero (puesto
que, en cada paso, el tamaño se divide por dos), de hecho:
\begin{equation}
  \label{eq:tema1:bisec:1}
  b_n-a_n = \frac{b-a}{2^n} \quad \forall n=0,1,2,...
\end{equation}

Así, se tiene el siguiente resultado:
\begin{theorem}
  \label{thm:tema1:bisecc}
  Sea $f$ una función continua en $[a,b]$ tal que $f(a)f(b)<0$.
  \begin{enumerate}
  \item La sucesión $\{c_n\}_{n=0}^\infty$ (definida por los puntos
    medios de los intervalos en el método de bisección) es convergente
    y su límite, $\cero$, es un cero de $f$ en $(a,b)$.
  \item De hecho, se verifica la siguiente cota del error:
    \begin{equation}
      \label{eq:tema1:bisecc:2}
      |c_n-\cero| \le \frac{b-a}{2^n}.
    \end{equation}
  \end{enumerate}
\end{theorem}

\begin{proof}
  Los intervalos $[a_n,b_n]$ generados por el método de bisección
  verifican~\eqref{eq:tema1:bisecc:0} y $f(a_n)f(b_n)<0$, por lo tanto
  todos ellos contienen un cero, $\cero\in (a,b)$. Además, como
  $\cero\in [a_n,b_n]$ (y también $c_n$),
  $$
  |c_n - \cero| < b_n - a_n.
  $$
  Esto, teniendo en cuenta~\eqref{eq:tema1:bisec:1},
  implica~\eqref{eq:tema1:bisecc:2}. Y, a su vez,
  \eqref{eq:tema1:bisecc:2} implica que $c_n$ converge a $\cero$.
\end{proof}

% \begin{remark}
%   \label{rk:tema1:unicidad-bisecc}
El teorema~\ref{thm:tema1:bisecc} es válido incluso si $f$ tiene
varios ceros en $[a,b]$, es decir el método de bisección tiene la
importante propiedad de que siempre converge hacia una solución,
independientemente de que ésta sea única y sin más hipótesis sobre $f$
que su continuidad.

Sin embargo, tiene el inconveniente de que su velocidad de
convergencia es lenta, necesitando muchas más iteraciones que otros
métodos que veremos más tarde. De hecho, el método no garantiza que el
error se reduzca en cada iteración: solamente que el intervalo se
reduce a la mitad. Por otro lado, el método no utiliza ningún tipo de
información acerca de la función $f$ (salvo su signo) y de hecho no
podemos esperar convergencia en una sola iteración, ni siquiera en el
caso en el que $f$ sea lineal.

De cualquier forma, el algoritmo de bisección es un método sencillo y
muy útil como ``método de arranque'', con el que realizar iteraciones
previas antes de pasar a otros métodos más eficientes.
% \end{remark}

\begin{remark}
  \label{rk:tema1:bisecc:iteraciones}
  La desigualdad~\eqref{eq:tema1:bisecc:2} nos garantiza que, para $n$
  suficiente grande, podemos aproximar un cero de $f$ con un error tan
  pequeño como deseemos. Y, de hecho, dada una tolerancia $\varepsilon$,
  para que $|c_n-\alpha|<\varepsilon$ podemos calcular el mínimo número
  de iteraciones tal que que $(b-a)/2^n < \varepsilon$. Efectivamente,
  despejando $n$ en~\eqref{eq:tema1:bisecc:2}, se trata del menor
  entero positivo $n_{\text{mín}}$ tal que:
  \begin{equation*}
    n_{\text{mín}}>\log_2\left(\frac{b-a}{\varepsilon}\right).
  \end{equation*}
\end{remark}


\begin{test}[Función ``bisección'']
  El programa~\ref{pro:metodo-biseccion} muestra un ejemplo de
  implementación del método de bisección, utilizando el lenguaje de
  programación \textit{Python}. Concretamente, la función
  \pythoninline{biseccion(f, a, b)} calcula una solución
  de~\eqref{eq:raiz} en un intervalo $(a,b)$. A través de parámetros
  opcionales se puede determinar una tolerancia, $\varepsilon$, de
  forma que el ciclo de iteraciones de detenga cuando
  $b_k-a_k<\varepsilon$. Adicionalmente, la función permite fijar el
  número máximo de iteraciones permitidas y seleccionar si, se actuará
  de forma silenciosa o bien con verbosidad, al mostrar los resultados.

  Por ejemplo, el siguiente código \textit{Python} aproxima un cero de
  la función $f(x)$ introducida en el
  ejemplo~\ref{ex:tema1:separ-soluc-1} 
  con tolerancia $10^{-2}$:
  \pythonexternal{tema1/src/biseccion-test1.py}
  \begin{pythonoutput}
    \pythonexternal[backgroundcolor=\color{white},title={Resultado}]{tema1/src/biseccion-test1.out}
  \end{pythonoutput}
\end{test}


\begin{program}
  \widepythonexternal{tema1/src/biseccion.py}
  \label{pro:metodo-biseccion}
  \caption{Método de bisección}
\end{program}


\section{Los métodos de punto fijo}
\label{sec:metodos-de-punto-fijo}

En este capítulo definimos un tipo de métodos que se definen a partir
de una reformulación de~(\ref{eq:raiz}) y son conocidos como métodos
de punto fijo o de aproximaciones sucesivas.

Llamamos \textit{punto fijo} de una función continua
$g:I\subset\Rset\to\Rset$ a una solución del siguiente  problema:
\begin{equation}
\tag{$P_{\text{PF}}$}
\text{Hallar $\cero\in I$ tal que} \quad \cero=g(\cero).
\label{eq:punto-fijo}
\end{equation}

Geométricamente, una solución de la ecuación $x=g(x)$ se corresponde
con la abscisa correspondiente a un punto de corte entre la gráfica de
la función $y=g(x)$ y la recta $y=x$ (ver
figura~\ref{fig:ejemplo-punto-fijo-1}).

\begin{example}
  La función $g(x)=2-x^2$ tiene puntos dos puntos fijos en $x=1$ y
  $x=-2$, pues $g(1)=2-1=1$ y $g(-2)=2-4=-2$. No tiene más puntos
  fijos en $\Rset$, pues éstos son raíces de $g(x)-x$, que en este
  caso es un polinomio de grado dos. En la
  figura~\ref{fig:ejemplo-punto-fijo-1} se representan geométricamente
  la función y sus dos puntos fijos.
\end{example}

\begin{figure}
  \begin{graficaTikz}[width=18em, height=15em]
    \begin{axis}[\axisXYmiddle, 
      % legend style = {anchor=north west, pos = north east}]
      legend pos = outer north east, legend cell align=left]
      % Draw a curve
      \addplot[domain=-2.4:2.4, blue, ultra thick, samples=40] {2-x^2};
      \addplot[domain=-2.5:2.5, gray, ultra thick, samples=40] {x};
      % Plot a label at curve root
      \node[coordinate, medium dot, pin=0:{\scriptsize$(1,1)$}] 
      at (axis cs:1,1) {};
      \node[coordinate, medium dot, pin=-45:{\scriptsize$(-2,-2)$}] 
      at (axis cs:-2,-2) {};
      \legend {$y=g(x)$,$y=x$};
    \end{axis}
  \end{graficaTikz}
  \caption{Puntos fijos de $g(x)=2-x^2$}
  \label{fig:ejemplo-punto-fijo-1}
\end{figure}

Si $\cero$ es una solución de un problema de punto fijo, entonces
$\cero$ es solución de un problema de cálculo de raíces del
tipo~(\ref{eq:raiz}) para $f(x)=x-g(x)$. Recíprocamente un problema de
cálculo de raíces puede ser transformado en un problema del
tipo~(\ref{eq:punto-fijo}) de numerosas formas, por ejemplo
escribiendo $g(x)=x-f(x)$ o empleando otro tipo de manipulaciones
algebraicas. Algunas de estas transformaciones en ecuaciones de tipo
punto fijo pueden dar lugar a potentes técnicas iterativas.

A continuación, estudiamos resultados de existencia y unicidad de
punto fijo.

\begin{proposition}[Existencia de solución de~(\ref{eq:punto-fijo})]
  \label{pro:existencia-punto-fijo}
  Sea $g:[a,b]\to\Rset$ una función continua en $[a,b]$ y supongamos
  que
  \begin{equation}
  g([a,b])\subset [a,b].
  \label{eq:g[a,b].subset.[a,b]}  
\end{equation}
  Entonces existe al menos un punto fijo de $g$ en $[a,b]$.
\end{proposition}

\begin{proof}
  Debido a la hipótesis~(\ref{eq:g[a,b].subset.[a,b]}), $g(a)\ge a$ y
  $g(b)\le b$, por lo tanto podemos aplicar el teorema de Bolzano
  (teorema~\ref{thm:bolzano}) a la función $f(x)=x-g(x)$, que también
  es continua en $[a,b]$.
\end{proof}

\begin{remark}
  Como se puede apreciar en la demostración anterior, la
  hipótesis~(\ref{eq:g[a,b].subset.[a,b]}) se puede debilitar, en
  concreto es suficiente que la función $g(x)$ verifique
  $(g(a)-a)(g(b)-b)\le 0$.
\end{remark}

Para estudiar la unicidad de solución de~(\ref{eq:punto-fijo}),
introduciremos la siguiente definición:

\begin{definition}
  Una función $g$ continua en $[a,b]$ se dice \textit{contractiva} en
  $[a,b]$ si existe $\cteContract\in[0,1)$ tal que
  \begin{equation*}
    |g(x)-g(y)| \le \cteContract |x-y|, \quad \forall x,y \in [a,b].
  \end{equation*}
  A $\cteContract$ se le llama constante de contractividad de $g$ en $[a,b]$.
  \label{def:funcion.contractiva}
\end{definition}

\begin{example}
La función $g(x)=(x^2-1)/3$ es contractiva en $[-1,1]$, pues
$$
|g(x)-g(y)|=\left|\frac{x^2-y^2}{3}\right| = \frac{|x+y|}{3}|x-y| \le
\cteContract |x-y|,
$$
para $\cteContract=\max_{[-1,1]}\frac{|x+y|}{3}=\frac23 <1$.
\end{example}

\begin{remark}~
  \begin{itemize}
  \item Puesto que $\cteContract<1$, toda función contractiva, $g$, <<contrae las
    distancias>> en el sentido de que $|g(x)-g(y)|<|x-y|$ para todo
    $x,y\in [a,b]$.
  \item En particular, toda función contractiva en $[a,b]$
    uniformemente continua en ese mismo intervalo (es decir, para
    todo $\varepsilon>0$ existe $\delta>0$ tal que $|x-y|<\delta
    \Rightarrow |f(x)-f(y)|<\varepsilon$).
  \end{itemize}
\end{remark}
x-k
Es sencillo demostrar que las funciones contractivas no pueden tener
más de un punto fijo:

\begin{proposition}[Unicidad de solución de~(\ref{eq:punto-fijo})]
  \label{pro:unicidad-punto-fijo}
  Sea $g:[a,b]\to\Rset$ una función \emph{contractiva} en
  $[a,b]$. Entonces $g$ posee, a lo sumo, un punto fijo en $[a,b]$.
\end{proposition}

\begin{proof}
  Si suponemos que $g$ tiene dos puntos fijos, $\cero_1$ y
  $\cero_2$, llegamos inmediatamente a una contradicción:
  $$
  |\cero_1-\cero_2| = |g(\cero_1)-g(\cero_2)| \le \cteContract |\cero_1 -
  \cero_2| < |\cero_1-\cero_2|,$$
  donde $\cteContract$ es la constante de contractividad de $g$.
\end{proof}

Los problemas de punto fijo~(\ref{eq:punto-fijo}) dan lugar al
siguiente tipo de esquemas recursivos, conocidos como métodos de punto
fijo o de aproximaciones sucesivas:
\begin{equation}
  \tag{MAS}
  \text{Partiendo de } x_0\in [a,b], \text{ calcular } x_{k+1}=g(x_k).
  \label{eq:MAS}
\end{equation}
Véase que para que el método esté bien definido es necesario
que $x_k$ se encuentre en el dominio de $g$ para todo $k$.

El siguiente teorema resume los resultados de existencia y unicidad
enunciados en las Proposiciones~\ref{pro:existencia-punto-fijo}
y~\ref{pro:unicidad-punto-fijo}, a la vez que garantiza el buen
planteamiento y la convergencia de~(\ref{eq:MAS}) bajo las hipótesis
de estas proposiciones.

\begin{theorem}
  \label{thm:punto-fijo-Banach}
  Sea $g:[a,b]\to\Rset$ tal que $g([a,b]) \subset [a,b]$ y supongamos
  que $g$ es contractiva en $[a,b]$ con constante de contractividad
  $\cteContract\in [0,1)$. Entonces:
  \begin{enumerate}
  \item 
    \label{item:punto-fijo-Banach:1}
    La función $g$ tiene un \textsf{único punto fijo}, $\cero$, en
    $[a,b]$.
  \item 
    \label{item:punto-fijo-Banach:2}
    Para todo $x_0\in [a,b]$, el método~(\ref{eq:MAS}) está bien
    definido y es \textsf{convergente} hacia $\alpha$.
  \item En cada etapa de~(\ref{eq:MAS}) se tienen las siguientes
    \textsf{estimaciones} del error absoluto:
    \label{item:punto-fijo-Banach:3}
    \begin{align}
      \label{eq:pto-fijo:cota-a-priori}
      |x_k-\cero| \le \cteContract^k &|x_0-\cero|,
      \\
      \label{eq:pto-fijo:cota-a-posteriori}
      |x_k-\cero| \le \frac{\cteContract^k}{1-\cteContract}
      &|x_1-x_0|.
    \end{align}
  \end{enumerate}
\end{theorem}

La acotación~(\ref{eq:pto-fijo:cota-a-priori}) llama
\textit{estimación a priori}, mientras que
a~(\ref{eq:pto-fijo:cota-a-posteriori}) se la conoce como
\textit{estimación a posteriori} del error.

\begin{proof}~\par
  El punto~\ref{item:punto-fijo-Banach:1} no es más que una
  reiteración de las Proposiciones~\ref{pro:existencia-punto-fijo}
  y~\ref{pro:unicidad-punto-fijo}.  Respecto al
  punto~\ref{item:punto-fijo-Banach:2}, para cualquier $x_0\in [a,b]$,
  el método iterativo está bien definido, ya que para $k>1$,
  $x_{k}=g(x_{k-1})$ y $g([a,b])\subset [a,b]$.
  Acerca de la convergencia, dado $x_0\in
  [a,b]$ podemos aplicar repetidamente la función $g$ obteniendo:
  \begin{align*}
    |x_k-\cero| = &|g(x_{k-1})-g(\cero)| \le \cteContract
    |x_{k-1}-\cero| = \\
    = \cteContract &|g(x_{k-2})-g(\cero)| \le \cteContract^2 
    |x_{k-2}-\cero| \le \cdots \le \cteContract^k |x_0-\cero|.
  \end{align*}
  Así se tiene la estimación a
  priori~(\ref{eq:pto-fijo:cota-a-priori}) y, como
  $L<1$, podemos concluir que $x_k\to\cero$.

  Para demostrar la estimación~(\ref{eq:pto-fijo:cota-a-posteriori})
  en el punto~\ref{item:punto-fijo-Banach:3}, comenzamos con un
  razonamiento análogo al de la acotación anterior. Utilizando que $g$
  es contractiva (y que $x_k\in [a,b]$ para todo $k\ge 0$) tenemos que
  para todo $n \ge 0$:
  \begin{equation*}
    |x_{n+1}-x_{n}| =
    |g(x_{n})-g(x_{n-1})|\le\cteContract|x_{n}-x_{n-1}| \le \cdots \le \cteContract^n|x_1-x_0|.
  \end{equation*}
  Sean ahora $k,n\in\Nset$ y supongamos $n>k$. Aplicando la
  desigualdad triangular junto a la desigualdad anterior obtenemos:
  \begin{align*}
     |x_n-x_k| &\le |x_n-x_{n-1}| + |x_{n-1}-x_{n-2}| + \cdots +
     |x_{k+2}-x_{k+1}| + |x_{k+1}-x_{k}|
     \\
     &\le \left(\cteContract^{n-1} + \cteContract^{n-2} +\cdots+
       \cteContract^{k+1} + \cteContract^{k} \right) |x_1-x_0|
     \\
     &=\left(\frac{\cteContract^k-\cteContract^n}{1-\cteContract}\right)
     |x_1-x_0|
     \le \frac{L^k}{1-L} |x_1-x_0|.
  \end{align*}
  Y por último, como sabemos que $x_n\to\cero$,
  podemos llegar a~(\ref{eq:pto-fijo:cota-a-posteriori}).
\end{proof}

El siguiente corolario es inmediato a partir de la estimación a
priori~(\ref{eq:pto-fijo:cota-a-priori}).
\begin{corollary}
  El método~(\ref{eq:MAS}) tiene orden de convergencia mayor o igual a
  uno.
\end{corollary}

\section{El método de ``Regula Falsi''}
\label{sec:regula-falsi}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../apuntes-MNII.tex"
%%% End: 
