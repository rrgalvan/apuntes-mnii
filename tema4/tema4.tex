\renewcommand{\tt}{t}
\newcommand{\yy}{y}
\newcommand{\yn}{{\yy_n}}
\newcommand{\ynn}{{\yy_{n+1}}}
\newcommand{\ta}{a}
\newcommand{\tb}{b}
\newcommand{\tn}{{\tt_n}}
\newcommand{\tnn}{{\tt_{n+1}}}
\newcommand{\zn}{{z_n}}
\newcommand{\znn}{{z_{n+1}}}
\newcommand{\ycero}{{y_a}}
% \newcommand{\sol}{\varphi}
\newcommand{\sol}{g}
\newcommand{\lipschitz}{$y$--Lipschitz\xspace}
\newcommand{\locLipschitz}{localmente \lipschitz}
\newcommand{\globLipschitz}{\lipschitz}
\newcommand{\errCons}{{\cal E}}
\newcommand{\RK}{Runge--Kutta\xspace}
\newcommand{\AB}{Adams--Bashforth\xspace}
\newcommand{\AM}{Adams--Moulton\xspace}


\section{Introducción}

Son muy numerosos los problemas (con orígenes diversos como la física,
química, biología o economía) que se formulan matemáticamente en
términos de ecuaciones diferenciales, es decir, ecuaciones cuya
incógnita, una función $y(x)$, describe un fenómeno dado a través de
una ley que relaciona a esta función con sus derivadas. La formulación
adecuada de esta ley junto a algunos datos adicionales (condiciones
iniciales) garantizarán el buen planteamiento del problema.

Este tema se centra en la resolución numérica de problemas de valor
inicial (o problemas de Cauchy) asociados a ecuaciones diferenciales
ordinarias (EDO). Este tipo de problemas se escriben de la siguiente
forma:
\begin{equation}
  \label{eq:pvi}
  \tag{PVI}
  \left\{
  \begin{aligned}
    &y' = f(\tt,\yy), \quad \tt\in[\ta,\tb],
    \\
    &y(\ta) = \ycero,
  \end{aligned}
  \right.
\end{equation}
donde la función (continua) $f:[\ta,\tb]\times\Rset\to\Rset$ y el
estado inicial $\ycero\in\Rset$ son datos conocidos. En el próximo
tema se estudiará el caso general, donde
$f:[\ta,\tb]\times\Rset^n\to\Rset^n$ e $\ycero\in\Rset^n$, con
$n\in\Nset$. Se ha optado por denotar $t\in [a,b]$ a la variable
independiente.  En numerosos problemas de la ciencia y la ingeniería,
esta variable representa el tiempo y, aunque no siempre es
sea así, en muchas ocasiones puede ser conveniente
hablar de $\tt$ como la <<variable temporal>>).

\begin{definition}
  \label{def:sol-pvi}
  Llamamos solución de~\eqref{eq:pvi} a toda función $\sol\in
  C^1([\ta,\tb])$ tal que $\sol'(\tt)=f(\tt,\sol(\tt))$ para todo
  $\tt\in[\ta,\tb]$ y además $\sol(\ta)=\ycero$.\label{def:3}
\end{definition}

Nota: con frecuencia, denotaremos por $y(t)$ o $y(x)$ a la solución (como
  en numerosos recursos bibliográficos).

Obsérvese que, según esta definición, el concepto de solución tiene un
sentido global (en todo el intervalo $[a,b]$), y no local (en un entorno
de $\ta$). Como paso previo a la resolución numérica
de~\eqref{eq:pvi}, deberemos garantizar que este problema está bien
planteado, es decir asegurar la existencia y unicidad de solución
de~(\ref{eq:pvi}), en el sentido de la definición~\ref{def:sol-pvi}.

\section{Resultados teóricos preliminares}
\label{sec:tema4:resultados-teoricos}

Antes de comenzar a estudiar métodos numéricos para la resolución de
problemas de Cauchy, recordaremos en esta sección algunos
resultados relacionados con la existencia y unicidad de solución
de~(\ref{eq:pvi}), que resultarán útiles para garantizar existencia de
solución en los problemas que aparecerán en las siguientes secciones.
La clave en estos resultados será el analizar si la función $f(x,y)$
verifica la condición de Lipschitz que se define a continuación:

\begin{definition}
  \label{def:lipschitz}
  Decimos que una función $f(\tt,y)$ verifica la condición de
  \resaltar{Lipschitz uniformemente} (o globalmente) respecto a $y$ en
    $[\ta,\tb]$ (o simplemente que $f$ es \globLipschitz)
    si existe $L>0$ tal que
  \begin{equation*}
    |f(\tt,y) - f(\tt,z)| \le L |y-z|, \quad \forall \tt\in [\ta,\tb],
    \quad  \forall y,z\in \Rset.
  \end{equation*}
  Decimos que una función $f(\tt,y)$ verifica la condición de
  \resaltar{Lipschitz localmente} respecto a $y$ en
  $[\ta,\tb]$ (o que $f$ es \locLipschitz) si para todo compacto
  $K\subset\Rset$ existe $L_K>0$ tal que
  \begin{equation*}
    |f(\tt,y) - f(\tt,z)| \le L_K |y-z|, \quad \forall \tt\in [\ta,\tb],
    \quad  \forall y,z\in K.
  \end{equation*}
\end{definition}

Fijaremos la siguiente
\textbf{notación}: $\dy f$ designará a la derivada parcial de $f$
respecto a $y$, es decir $\dy f(\tt,y) = \frac{\partial f}{\partial
  \yy}(\tt,y)$. Para el resto de las variables se usarán notaciones
similares.
  En la práctica, para estudiar si una función es (local o
  globalmente) Lipschitz se suelen usar las siguientes condiciones:
  \begin{proposition}~
    \begin{enumerate}
    \item Si $\dy f$ es continua en $[\ta,\tb]\times\Rset$, entonces
      $f$ es \locLipschitz.
    \item Si $\dy f$ es continua y además está acotada en
      $[\ta,\tb]\times\Rset$, entonces $f$ es uniformemente
      \globLipschitz.
    \item Recíprocamente, si $\dy f$ es continua y $f$ es
      uniformemente \globLipschitz, entonces $\dy f$ está acotada en
      $[\ta,\tb]\times\Rset$.
    \end{enumerate}
  \end{proposition}
  \begin{proof}
    Tan solo comprobaremos la segunda de estas afirmaciones. El resto
    de ellas se deja como ejercicio\footnote{Indicación: primera
      afirmación se demuestra de forma análoga (usando que $\dy f$
      está acotada en todo compacto $K\subset\Rset$) mientras que la
      tercera se basa en la definición de derivada parcial, que se
      puede acotar usando la condición de Lipschtiz.}.

    Supongamos $\dy f$ es
    continua en $[\ta,\tb]\times\Rset$. Entonces en cada <<instante>>
    $t\in[\ta,\tb]$ la función
  $$\dy f(t,\cdot):\Rset \to \Rset$$
  es continua. Aplicando el teorema del valor medio, se tiene que
  dados $y$, $z\in\Rset$ existe $y^*$ entre $y$ y $z$ tal que
  \begin{equation*}
    |f(\tt,y)-f(\tt,z)| = |\dy f(\tt,y^*) \cdot (y-z)|=|\dy f(\tt,y^*)| \cdot |y-z|.
  \end{equation*}
  Como además $\dy f$ está acotada, existe $L>0$ tal que
  $|\dy f(\tt,y)|\le L$ para todo $(\tt,y)\in [\ta,\tb]\times\Rset$ y
  entonces
  \begin{equation*}
    |f(\tt,y) - f(\tt,z)| \le L |y-z|.
  \end{equation*}
\end{proof}

\begin{example}
  La función $$f(x,y)=y^2$$ es \locLipschitz en cualquier intervalo
  $[\ta,\tb]$, pues su derivada parcial $\dy f(x,y)=2y$ es continua. Sin
  embargo, esta función no está acotada cuando $y\in\Rset$, por lo que
  no tenemos garantías de que $f(x,y)$ sea uniformemente \globLipschitz.

  Veamos que realmente $f(x,y)$ no es uniformemente \globLipschitz,
  comprobando que para cualquier constate $L>0$, podemos encontrar dos
  valores, $y_L$, $z_L\in\Rset$ de forma que
  \begin{equation*}
  |f(\tt,y_L)-f(\tt,z_L)| >  L  |y_L-z_L|.
 \end{equation*}
 En concreto, dada $L>0$ tomamos $y_L=2L$ y $z_L=0$. Así
 \begin{align*}
   L|y_L-z_L|&=2L
   \intertext{y evaluando $|f(\tt,y_L) - f(\tt,z_L)|$ tenemos que, para todo $\tt\in[a,b]$:}
   |f(\tt,y_L)-f(\tt,z_L)|&=|(2L)^2-0^2|=4L^2 > 2L^2 =  L|y_L-z_L|.
 \end{align*}
\end{example}


A continuación se resumen los resultados teóricos fundamentales de
existencia y unicidad para problemas de Cauchy. El primero utiliza
hipótesis más débiles (\locLipschitz), aunque sin alcanzar
directamente la existencia y unicidad de solución global en todo
$[a,b]$. El Teorema~\ref{thm:existencia-unif-lipschitz} garantiza la
existencia y unicidad de solución de~\eqref{eq:pvi} en $[a,b]$, aunque
a costa de imponer una hipótesis muy fuerte (Lipschitz uniforme), que
reduce considerablemente el rango de problemas diferenciales
abarcados.
\begin{proposition}
  \label{thm:existencia-loc-lipschitz}
  Sea $f$ continua en $[\ta,\tb]\times\Rset$ y \locLipschitz. Entonces, para
  cualquier inicialización $\ycero\in\Rset$:
  \begin{enumerate}
  \item Existe una única solución local\footnote{Esta propiedad se puede ver como un
      corolario del clásico Teorema de Picard, que garantiza
      existencia y unicidad en un entorno de $a$.} (que definiremos como una
    solución definida sólo en $[\ta,\ta+\varepsilon]$ para algún
    $\varepsilon>0$)
  \item Existe una única solución maximal\footnote{Una solución
      maximal (por la derecha) es una solución local de~(\ref{eq:pvi}), definida en
      $[\ta,\ta+\varepsilon]$, que no es prolongable a ningún
      intervalo $[\ta,c]$ (con $c>\ta+\varepsilon$).}
    de~\eqref{eq:pvi}.
  \item Además: si esta solución maximal $\sol(\tt)$ no está definida en todo
    $[\ta,\tb]$ es porque explota en tiempo finito (es decir existe
    $c\in (\ta,\tb)$ tal que $\lim_{\tt\to c^-} \sol(\tt) = \infty$).
  \end{enumerate}
\end{proposition}

\begin{proposition}
  \label{thm:existencia-unif-lipschitz}
  Sea $f$ continua en $[\ta,\tb]\times\Rset$ y (globalmente) \globLipschitz.
  Entonces, para cualquier inicialización $\ycero\in\Rset$, existe una
  única solución maximal de~\eqref{eq:pvi} (definida en todo el
  intervalo $[\ta,\tb]$).% Además, para todo $\tt\in[\ta,\tb]$
  % se tiene la siguiente desigualdad:
  % \begin{equation*}
  %   |y(\tt)-y(\ta)| \le (\tb-\ta) \Big( \max_{\tt\in[\ta,\tb]}
  %     f(\tt,y(\ta))\Big) e^{L(\tt-\ta)}.
  % \end{equation*}
\end{proposition}


En lo que sigue, estudiaremos el análisis numérico de problemas
diferenciales en los que, en principio, la función $f(\tt,y)$ es tan
solo \locLipschitz. Por tanto el estudio de la existencia de solución
en $[\ta,\tb]$ no será una tarea inmediata. % La técnica usual será
% demostrar que la solución maximal del problema~(\ref{eq:pvi}) está
% acotada en $[\ta,\tb]$ y por tanto no explota en tiempo finito. Así,
% el Teorema~\ref{thm:existencia-loc-lipschitz} garantizará que la
% solución maximal está definida en todo $[\ta,\tb]$ (ver en el
% ejemplo~\ref{ex:existencia-unicidad-loc-lipschitz}).

% Pero demostrar que la solución maximal del problema~(\ref{eq:pvi})
% está acotada en $[\ta,\tb]$ no es fácil. Por tanto, introducimos aquí
% algunas propiedades interesantes de las ecuaciones diferenciales que
% nos pueden resultarán de utilidad para este propósito. Para su
% demostración, consultar la bibliografía.

% \begin{proposition}[Comparación de soluciones de~(\ref{eq:pvi})]
% \label{pro:propiedades-pvi}
%   Bajo las hipótesis del teorema~\ref{thm:existencia-loc-lipschitz}
%   ($f$ \locLipschitz):
%   \begin{enumerate}
%   \item Sean $\sol_1(\tt)$, $\sol_2(\tt)$ las respectivas soluciones
%     de dos problemas del tipo~\eqref{eq:pvi} para la \textbf{misma
%       ecuación diferencial} $y'=f(\tt,y)$ pero con \textbf{distintas
%       condiciones iniciales}, es decir:
%     $$\sol_1(\ta)<\sol_2(\ta)$$
%     Entonces las gráficas de $\sol_1$ e $\sol_2$ no se cortan en
%     ningún punto. En concreto, si $\sol_1$ y $\sol_2$ están
%     definidas, respectivamente, en los intervalos $I_1$ e $I_2$,
%     entonces
%     \begin{equation*}
%       \sol_1(\tt) < \sol_2(\tt), \quad \forall \tt \in I_1
%       \cap I_2.
%     \end{equation*}

%   \item Sean $\sol_1(\tt)$, $\sol_2(\tt)$ las soluciones de dos
%     problemas del tipo~\eqref{eq:pvi} con la \textbf{misma condición
%       inicial} pero con \textbf{ecuaciones diferenciales distintas},
%     $y'=f_1(\tt,y)$, $y'=f_2(\tt,y)$. Si
%     $$
%     f_1(\tt,y) \le f_2(\tt,y) \quad \forall \tt\in[\ta,\tb], \quad
%     \forall y\in\Rset,
%     $$
%     entonces
%     $$
%     \sol_1(\tt)\le \sol_2(\tt) \quad \forall \tt \in
%     I_1 \cap I_2.
%     $$
%   \end{enumerate}
% \end{proposition}

% \begin{example}
%   \renewcommand{\tt}{x}
%   \label{ex:existencia-unicidad-loc-lipschitz}
%   Estudiaremos la existencia y unicidad de solución del siguiente
%   problema de valor inicial:
%   \begin{equation*}
%     \left\{
%       \begin{aligned}
%         &y' = -y^2+2y\,\cos \tt, \quad \tt\in [0,2],\\
%         &y(0)=1.
%       \end{aligned}
%     \right.
%   \end{equation*}
%   En primer lugar, $f(x,y)=-y^2+2y\,\cos \tt$ y
%   $\dy f(t,y)=-2y+2\cos t$ son continuas en $[0,2]\times\Rset$, luego
%   $f$ es \locLipschitz, por lo que (según el
%   teorema~\ref{thm:existencia-loc-lipschitz}) existe una única
%   solución maximal, a la que denominaremos $\sol(\tt)$ definida bien
%   en todo $I=[0,2]$ o bien en $I=[0,c)$ con $c<2$. Pero esta última
%   posibilidad no se verifica, pues entonces $\sol(\tt)$ debería
%   en $c$. Y esto no ocurre porque $\sol(\tt)$ está acotada:
%   \begin{enumerate}
%   \item Sea $\sol_1$ la solución del problema de valor inicial para la
%     misma ecuación diferencial pero para la condición inicial
%     $y(0)=0$. Está claro que su única solución es $\sol_1(\tt)=0$
%     para todo $\tt\in\Rset$. Y como $0=\sol_1(0)<\sol(0)=1$, entonces
%     (según la proposición~\ref{pro:propiedades-pvi}, parte 2) podemos
%     asegurar que $\sol$ está acotada inferiormente:
%     \begin{equation*}
%       0=\sol_1(\tt)<\sol(\tt), \quad \forall \tt\in I.
%     \end{equation*}
%   \item Sea ahora $f_2(\tt,y)=-y^2+2y$. Como $\cos\tt \le 1$  (y como
%     ya podemos suponer $y>0$):
%     $$f(\tt,y)=-y^2+2y\cos\tt \le -y^2+2y = f_2(\tt,y)\quad
%     \forall \tt\in I, \forall y\in\Rset^+.$$
%     Ahora podemos calcular
%     directamente la solución $\sol_2$ del problema de valor inicial
%     $y'=f_2(\tt,y)$, $y(0)=1$ (ya que la ecuación es autónoma),
%     obteniendo:
%     $$
%     \sol_2(\tt)=\frac{2e^{2\tt}}{1+e^{2\tt}},
%     $$
%     función continua y, en consecuencia, acotada superiormente en $[0,2]$.
%     Según la proposición~\ref{pro:propiedades-pvi} (parte 1)
%     \begin{equation*}
%       \sol(\tt)\le\sol_2(\tt),
%     \end{equation*}
%     luego $\sol(\tt)$ está también acotada superiormente en
%     $[0,2]$. Por lo tanto, $\sol$ está definida en todo $[0,2]$.
%   \end{enumerate}
%   \renewcommand{\tt}{t}
% \end{example}


\section{El método de Euler}

A partir de ahora, nuestro objetivo será la descripción, análisis e
implementación de distintos métodos numéricos para la aproximación de
la solución de~\eqref{eq:pvi}. Estos métodos partirán con la
definición de una partición del intervalo $[\ta,\tb]$, formada por
$N+1$ puntos:
\begin{equation*}
  \ta=\tt_0 < \tt_1 < \cdots < \tt_N=\tb.
\end{equation*}
Por simplicidad, supondremos que la partición es uniforme, es decir,
\begin{equation*}
  \text{ si } h=\frac{b-a}{N}, \quad \text{entonces} \quad
  \tt_n=\ta+n\cdot h,\quad \forall n=0\dots,N
\end{equation*}
(en particular, $\tt_0=a$ y $\tt_N=b$). A continuación procedemos como
sigue:
\begin{enumerate}
\item Usamos el dato inicial $y_a$ para arrancar el método numérico,
  definiendo $y_0=y_a$ (más generalmente, $y_0\approx y_a$)
\item Seguidamente definimos una sucesión de forma que $y_{n+1}$ se
  calcula a partir de $y_n$ (y posiblemente de $y_{n-1}$, \dots,
  $y_{n-k}$ para algún $k\ge 1$), con el fin de que
  $y_n\approx\sol(\tn)$.
\end{enumerate}
Nuestro objetivo será que el método sea \textit{convergente}, en el
sentido de que la solución aproximada $\{\yn\}_n$ ``converja a la
solución exacta'' $\{\sol(\tn)\}_n$ cuando $h\to 0$. Íntimamente
relacionados con la convergencia están los conceptos de
``\textit{estabilidad}'' y ``\textit{consistencia}'', que estudiaremos
más adelante.
% \begin{enumerate}
% \item El método es \textit{consistente}, si la solución exacta
%   $\{\sol(\tn)\}_n$ ``verifica aproximadamente'' el esquema
%   numérico
% \item El método es \textit{estable} si ``responde continuamente'' a
%   perturbaciones en los datos iniciales.
% \end{enumerate}
% Más adelante distinguiremos entre métodos de un paso, en los que
% para calcular $y_{n+1}$ utilizamos solamente de $y_n$ y métodos
% multipaso, en los que  $y_{n+1}$ depende de los datos en varias etapas
% anteriores ($y_{n}$, $y_{n-1}$, \dots, $y_{n-k}$ para algún $k\ge 1$.)

El método más sencillo es el de Euler (más precisamente, el método
de Euler explícito):
\begin{equation}
  \tag{EU}
  \label{eq:metodo-euler}
  \left\{
    \begin{aligned}
      &y_0 = \ycero, \\
      &y_{n+1} = \yn + h f(\tn, \yn), \quad n=0,\dots,N-1.
    \end{aligned}
  \right.
\end{equation}

Este método admite varias interpretaciones:

\subsection*{Interpretaciones del método de Euler}

A continuación se introducen algunas interpretaciones heurísticas (es
decir, formales y no rigurosas) al método de Euler. Este método se
justificará totalmente cuando, en la
sección~\ref{sec:error-y-convergencia-euler}, se estudie su error y su
convergencia hacia la solución exacta.

\subsubsection*{Interpretación geométrica}
En la ecuación $y(\tt)'=f(\tt,y(\tt))$ se puede interpretar que
$f(\tt,y(\tt))$ marca la pendiente (la derivada) de la solución,
$\sol(\tt)$ en cada instante $\tt\in [\ta,\tb]$.

\begin{center}
  \begin{graficaTikz}[width=23em, height=17em]
    \begin{axis}[ \axisXYmiddle, xtick=\empty, ytick=\empty, legend
      pos = north east, xlabel=$t$ ]
      % Draw a curve
      \addplot[domain=1.3:3.3, blue, ultra thick] {20+x*x*x*x};
      \addplot[domain=1.5:3.1, red, thick] {36+32*(x-2)};
      % Plot a label at curve root
      \node[coordinate, medium dot,
      pin={[fill=blue!10!white]120:{\scriptsize $\sol(\tn)$}}] at
      (axis cs:2,36) {};
      \node[coordinate, medium dot, blue,
      pin={[fill=blue!10!white]120:{\scriptsize $\sol(\tt_{n+1})$}}] at
      (axis cs:2.7,73.144) {};
      % \node[coordinate, medium dot, blue, pin=100:{\scriptsize
      % $\sol(\tt_{n+1})$}] at (axis cs:2.7,73.144) {};
      \addplot[dashed] coordinates {(2,7) (2,36)};
      \addplot[dashed] coordinates {(2.7,7) (2.7,58.4)};
      \node[coordinate, medium dot, pin=-30:{$y_{n}$}] at
      (axis cs:2,36) {};
      \node[coordinate, medium dot, pin=-30:{$y_{n+1}$}] at
      (axis cs:2.7,58.4) {};
      % Draw the name of curve
      \legend {$y(t)$};
    \end{axis}
  \end{graficaTikz}
\end{center}

Supongamos que $\yn=\sol(\tn)$ (en general será
$\yn\approx\sol(\tn)$). Para calcular $\ynn$:
\begin{enumerate}
\item Calculamos la recta, $r$, que pasa por $(\tn,\yn)$ y tiene pendiente
  $f(\tt_n, \yn)$:
  $$
  r(\tt) = \yn +  f(\tt_n, \yn) (\tt-\tt_n).
  $$
\item Calculamos $y_{n+1}$ como la ordenada de esta recta en
  $\tt=\tt_{n+1}$ es decir:
  $$
  y_{n+1} = r(\tt_{n+1}) =
  \yn +  f(\tt_n, \yn) (\tt_{n+1}-\tt_n) = \yn +  f(\tt_n, \yn) \cdot h.
  $$
\end{enumerate}
Así obtenemos el método~\eqref{eq:metodo-euler}.

\subsubsection*{Desarrollo de Taylor}

Suponiendo que $\sol(t)$ (la solución exacta de~(\ref{eq:pvi})) es de
$C^2([\ta,\tb])$, podemos hacer su desarrollo de Taylor en $\tn$:
\begin{equation}
  \sol(\tnn)=\sol(\tn+h) = \sol(\tn) + h\sol'(\tn) + \frac{h^2}{2} \sol''(\xi),
  \label{eq:euler.taylor.0}
\end{equation}
donde $\xi\in(\tn,\tnn)$. Como $\sol''$ está acotada, el último
término (el resto) será despreciable cuando $h$ es suficientemente
pequeño (pues contiene el factor $h^2$). Así
\begin{align}
  \label{eq:euler.taylor.1}
  \sol(\tnn)&\approx \sol(\tn) + h\sol'(\tn),
              \intertext{es decir,}
              \notag
              \sol(\tnn)&\approx \sol(\tn) + h f(\tn,y(\tn)).
                          \intertext{Y teniendo en cuenta que $\sol(\tn)\approx \yn$, $\sol(\tnn)\approx\ynn$:}
                          \notag
                          \ynn &\approx \yn + h f(\yn,\tn).
\end{align}

\paragraph{Notación.}
Para cualquier sucesión $\{y_n\}_n$, se utilizará la siguiente
notación para el cociente incremental (derivada discreta o diferencia
finita retrógrada de $\ynn$):
\begin{equation}
  \label{eq:derivada-discreta}
  \ddt\ynn = \frac{\ynn-\yn}{h}.
\end{equation}
Así el método de Euler se puede escribir como
\begin{equation*}
  \ddt\ynn = f(\tn,\yn).
\end{equation*}

Utilizando diferentes formulaciones de la derivada discreta, se podrían
haber expuesto ahora algunas variantes del método de Euler, pero éstas
serán introducidas a través de fórmulas de integración.

\subsubsection*{Integración numérica}
La solución exacta de~(\ref{eq:pvi}) verifica $\sol'(t) = f(t, g(t))$,
$t\in [a,b]$. Puesto que $\sol\in C^1([a,b])$ y $f\in C([a,b])$,
podemos integrar la igualdad anterior en $[t_n,t_{n+1}] \subset [a,b]$
obteniendo la siguiente expresión integral:
% forma (formulación integral de la solución de~(\ref{eq:pvi})):
% \begin{equation*}
%   \sol(t) = \sol(\ta) + \int_\ta^t f(s,\sol(s))\, ds.
% \end{equation*}
% La formulación anterior sigue siendo válida si sustituimos $a$ por cualquier otro punto de $[a,b]$. En particular, para
% $a=\tn$ y $t=\tnn$:
\begin{equation}
  \label{eq:tema4.5}
  \sol(\tnn) = \sol(\tn) + \int_\tn^\tnn f(t,\sol(t))\, dt.
\end{equation}
Para obtener el método de Euler implícito aproximamos la última
integral mediante la siguiente fórmula de cuadratura (regla
rectangular izquierda):
\begin{equation*}
  \int_a^b g(s)\,ds \approx hg(a), \quad \text{ con $h=b-a$}.
\end{equation*}
Aplicándola en el intervalo $[\tn,\tnn]$ para la formulación integral
anterior,
\begin{equation*}
  \ynn =  \yn + h f(\tn,\yn),
\end{equation*}
con $\yn\approx\sol(\tn)$, $\ynn\approx\sol(\tnn)$.

%======================================================================
%\end{document}
%======================================================================

\subsubsection*{Algunas variantes del método de Euler.}

Utilizando otras fórmulas de cuadratura, para aproximar la integral
$$\int_\tn^\tnn f(s,\sol(s))\, ds$$ en~(\ref{eq:tema4.5}),
se obtienen métodos numéricos
distintos al de Euler explícito. Por ejemplo:
\begin{enumerate}
\item Aproximando $\int_\tn^\tnn f(s,\sol(s))\, ds$ por la regla
  rectangular derecha, definida como
  \begin{equation*}
    \int_a^b g(s)\,ds \approx hg(b),
  \end{equation*}
  se obtiene el \textit{Método de Euler implícito}:
  \begin{equation}
    \label{eq:euler-implicito}
    \tag{EI}
    \left\{
      \begin{aligned}
        &y_0=\ycero,\\ &\ynn = \yn + h f(\tn,\ynn).
      \end{aligned}
    \right.
  \end{equation}
  El método anterior se dice \textit{implícito} porque, en general,
  $\ynn$ no puede calcularse explícitamente, pues aparece en el
  segundo miembro. Sin embargo, este tipo de métodos tienen buenas
  propiedades desde el punto de vista de la estabilidad y existen
  casos particulares muy interesantes en el que los métodos implícitos
  son muy usados (por ejemplo, cuando $f$ es lineal).

\item Aproximando $\int_\tn^\tnn f(s,\sol(s))\, ds$ por la regla del
  punto medio (que tiene orden mayor que las reglas rectangular
  derecha e izquierda):
  \begin{equation*}
    \ynn = \yn + h \; f(\tt_{n+1/2},\sol(\tt_{n+1/2})).
  \end{equation*}
  \begin{enumerate}
  \item Si aproximamos seguidamente $\sol(\tt_{n+1/2})$ por
    $\frac12\big(\sol(\tn)+\sol(\tnn)\big)$, se tiene el siguiente
    método implícito, conocido como método de \textit{Crank-Nicolson}
    (primera versión):
    \begin{equation}
      \label{eq:crank-nicolson-1}
      \tag{CN$_1$}
      \left\{
        \begin{aligned}
          &y_0=\ycero,\\
          &\ynn = \yn + h \; f\big(\tt_{n+1/2},\; \frac{\yn+\ynn}{2}\big).
        \end{aligned}
      \right.
    \end{equation}
  \item El método anterior es implícito y por tanto (en general) $\ynn$ no podrá
    obtenerse directamente. Una variante consiste en aproximar $\sol(\tt_{n+1/2})$
    por $\sol(\tn)+\frac{h}{2}f(\tn,\sol(\tn))$ llegando al método
    explícito conocido como de \textit{Euler--Cauchy} (o del
    \textit{punto medio} o de \textit{Euler modificado}):
    \begin{equation}
      \label{eq:euler-cauchy}
      \tag{EC}
      \left\{
        \begin{aligned}
          &y_0=\ycero,\\
          &\ynn = \yn + h \; f\big(\tt_{n+1/2},\;
          \yn+\frac{h}{2}f(\tn,\yn) \big).
        \end{aligned}
      \right.
    \end{equation}
  \end{enumerate}
\item Si aproximamos $\int_\tn^\tnn f(s,\sol(s))\, ds$ por la regla
  del trapecio se llega al siguiente método implícito (segunda
  variante del método de Crank-Nicolson):
  \begin{equation}
    \label{eq:crank-nicolson-2}
    \tag{CN$_2$}
    \left\{
      \begin{aligned}
        &y_0=\ycero,\\
        &\ynn = \yn + \frac h2 \; \big\{f(\tn,y_n))+f(\tnn,y_{n+1})\big\}.
      \end{aligned}
    \right.
  \end{equation}
  Si además aproximamos $\sol(\tnn)$ por
  $\sol(\tn)+hf(\tn,\sol(\tn))$ se llega al método explícito
  conocido como \textit{método de Heun} (o de \textit{Euler mejorado}):
  \begin{equation}
    \label{eq:euler-mejorado}
    \tag{EM}
    \left\{
      \begin{aligned}
        &y_0=\ycero,\\
        &\ynn = \yn + \frac{h}{2} \; \left\{ f(t_n,y_n) + f\big(\tnn,\;
        \yn+h f(\tn,\yn) \big)  \right\}.
      \end{aligned}
    \right.
  \end{equation}
\end{enumerate}
Estos métodos numéricos se pueden escribir en una notación
algo más sencilla (y más cercana a la ecuación diferencial original),
utilizando la derivada discreta definida
en~\eqref{eq:derivada-discreta}. Por ejemplo, el método de Euler
implícito~\eqref{eq:euler-implicito} se sintetiza en:
\begin{equation*}
  y_0=\ycero,\quad \ddt\ynn = f(\tnn,\ynn).
\end{equation*}


\section{Error y convergencia en el método de Euler}
\label{sec:error-y-convergencia-euler}
En esta sección nos centraremos en el método de Euler (explícito), que
será usado como modelo para ilustrar la teoría de métodos numéricos
para la resolución de~(\ref{eq:pvi}).

En concreto, demostraremos que el método de Euler es convergente, es
decir el error entre la solución exacta y la solución aproximada
converge a cero (en un sentido que, más adelante, definiremos con
precisión). Para ello veremos que el estudio de la convergencia se puede
desglosar en el análisis de dos conceptos: \textit{consitencia} y
\textit{estabilidad}, que estudiamos a continuación.

\subsection{Consistencia del método de Euler}
\label{sec:consistencia-euler}

En lo que sigue supondremos existencia y unicidad de solución
$\sol\in C^1([a,b])$ del problema de valor inicial~(\ref{eq:pvi}).
Consideremos el método de Euler (en una partición $\{t_n\}_{n=0}^N$ de
$[a,b]$, donde $N\in\Nset$ y $h=(b-a)/N$), que usando el cociente
incremental (derivada discreta) definido
en~(\ref{eq:derivada-discreta}) se escribe como:
hallar $\{y_{n}\}_{n=0}^N$ como solución de: $y_0=y_a$,
\begin{align}
  \notag
  \ddt \ynn := \frac{\ynn-\yn}{h} = f(\tn,\yn),
  \intertext{es decir,}
  \label{eq:tema4.4}
  \ddt \ynn - f(\tn,\yn)=0,
\end{align}
para cada $n=0,\dots,N-1$.

La idea de consistencia de un método numérico está relacionada con el
grado en el que la solución exacta verifica el método numérico, en este caso representado por~(\ref{eq:tema4.4}). Es decir,
sea $\sol(t)$ a la solución exacta de~(\ref{eq:pvi}) y, en la
ecuación~(\ref{eq:tema4.4}) consideremos la sucesión de valores exactos
$$
\sol(t_0),\ \sol(t_1),\ \dots,\ \sol(\tn),\ \sol(\tnn),\ \dots,\ \sol(t_N).
$$
Estos valores no verificarán, en general, la ecuación
(\ref{eq:tema4.4}), es decir tendremos
\begin{align*}
  \ddt \sol(\tnn) &- f(\tn,\sol(\tn))
  \\
  &= \frac{\sol(\tnn)-\sol(\tn)}{h}
  - f(\tn,\sol(\tn))
    \neq 0
  % &= \frac{h^2}2 g''(\xi), \quad \xi\in(a,b).
\end{align*}
en general (aunque, como veremos, podemos esperar que la expresión anterior tienda a cero cuando $h\to 0$).
% \begin{equation*}
%   \ddt \sol(\tnn) :=
%   \frac{\sol(\tnn)-\sol(\tn)}{h} \approx f(\tn,\sol(\tn)).
% \end{equation*}
% Y de hecho, como $f(\tn,\sol(\tn))=\sol'(\tn)$, % en algún sentido
% % esperamos que la aproximación anterior mejore cuando $h\to 0$.
% el desarrollo de Taylor~(\ref{eq:euler.taylor.0}) nos indica que
% En cada instante $\tn$, definiremos el error de consistencia como la
% diferencia entre $\ddt\ynn$ y $\ddt\sol(\tnn)$:
A esta expresión la llamaremos error de consistencia en el instante $\tn$:
\begin{definition}[Consistencia en $\tn$ del método de Euler]
  \label{def:error-consistencia-euler}
  Para cada $n=0,1,...,N-1$, se define el error de consistencia del método de
  Euler en el instante $\tn$ como
  \begin{equation}
    \label{eq:consistencia-euler}
    \errCons_{n} = \ddt \sol(\tnn)-f(\tn,\sol(\tn)).
  \end{equation}
\end{definition}
\begin{remark}
  Obviamente, para un problema de valor inicial, el error de
  consistencia dependerá de $h$, es decir $\errCons_n=\errCons_n(h)$.
  Además, puesto que $\sol$ es solución exacta de la EDO la
  ecuación~(\ref{eq:consistencia-euler}) se puede escribir como
  $$\errCons_{n} = \errCons_{n}(h) = \ddt \sol(\tnn)-\sol'(\tnn).$$
  Es decir, el error de discretización se puede interpretar como la
  diferencia entre la derivada discreta de la sucesión de soluciones
  exactas y la derivada de la solución exacta en $t_n$.  Es de esperar
  que cuando $h\to 0$, el cociente incremental $\ddt \sol(\tnn)$
  tienda a la derivada y así el error de consistencia tienda a
  cero. Este hecho se formalizará en la demostración del
  Teorema~\ref{thm:consistencia-euler}, conde veremos que
  efectivamente esta convergencia se verifica de forma uniforme cuando
  $h\to 0$.
\end{remark}
%  \begin{remark}
%   Como $\sol(t)$ es la solución exacta, $\sol'(\tn)=f(\tn,\sol(\tn))$, así
%   podemos en el método de Euler podemos escribir:
%   \begin{align*}
%     \ddt \sol(\tnn) &\approx \sol'(\tn).
%   \end{align*}
% \end{remark}

\begin{definition}[Consistencia de un método numérico para~(\ref{eq:pvi})]
  \label{def:consitencia-metodo-pvi}
  Dado un método numérico para~(\ref{eq:pvi}), se define el error de consistencia
  como:
  \begin{equation*}
    \errCons = \errCons(h) = (\errCons_0, \errCons_1,\dots,\errCons_{N-1}).
  \end{equation*}
  Decimos que un método es consistente si
  \begin{equation*}
    \lim_{h\to 0} ||\errCons(h)||_\infty = 0.
  \end{equation*}
  Decimos que un método numérico para~(\ref{eq:pvi}) tiene orden de
  consistencia $p$ si existen $C, h_0>0$ tales que
  \begin{equation*}
    ||\errCons(h)||_\infty \le C\, h^p, \quad \forall h<h_0.
  \end{equation*}
\end{definition}

\begin{theorem}
  \label{thm:consistencia-euler}
  Si la solución de~(\pageref{eq:pvi}) es $\sol\in C^2([\ta,\tb])$
  entonces el método de Euler es consistente. Más aún, se verifica
  \begin{equation*}
    ||\errCons(h)||_\infty \le \frac{h}{2} ||\sol''||_\infty,
  \end{equation*}
  por tanto el  orden de consistencia del método de Euler es igual a uno.
\end{theorem}

\begin{proof}
  Mediante un desarrollo de Taylor de la solución
  exacta:
  \begin{align*}
    \sol(\tnn)=\sol(\tn+h) &= \sol(\tn) + h \sol'(\tn) + \frac{h^2}{2} \sol''(\xi),
    \\ &= \sol(\tn) + h f(\tn,\sol(\tn)) + \frac{h^2}{2} \sol''(\xi),
    \quad \text{con $\xi$ entre $\tn$ y $\tnn$,}
  \intertext{por lo que}
    \frac{\sol(\tnn)-\sol(\tn)}{h} &- f(\tn,\sol(\tn)) = \frac{h}{2} \sol''(\xi),
    \quad\text{$\xi$ entre $\tn$ y $\tnn$.}
  \end{align*}
  Y entonces para todo $n=0,\dots,N-1$:
  \begin{equation*}
  |\errCons_n| \le \frac{h}{2} \max_{t\in[\ta,\tb]} |\sol''(t)|.
\end{equation*}
\end{proof}

\subsection{Estabilidad del método de Euler}
\label{sec:estabilidad-euler}
% En general, la solución exacta de~(\ref{eq:pvi}) no verifica la regla
% de recurrencia del método de Euler~(\ref{eq:metodo-euler}). En su
% lugar,
En la definición de consistencia, la
ecuación~(\ref{eq:consistencia-euler}) nos indica que, para cada $h>0$,
el conjunto $\{\sol(\tn)\}_{n=0}^{N-1}$ definido por soluciones
exactas de~(\ref{eq:pvi}) verifica el siguiente esquema recursivo:
\begin{align}
  \notag
    \ddt \sol(\tnn)&=f(\tn,\sol(\tn))+\errCons_n,
    \intertext{o equivalentemente,}
    \label{eq:tema4:7}
    \sol(\tnn)&=\sol(\tn)+ h f(\tn,\sol(\tn))+ h\,\errCons_n,
\end{align}
lo que se puede entender como una pequeña perturbación (de talla
$h\errCons_n$) del método de Euler.
 % Utilizaremos este hecho para
% estudiar la diferencia entre la solución que proporciona el método de
% Euler y las soluciones exactas.
A continuación estudiaremos el modo en que pequeñas
perturbaciones del método de Euler afectan a su solución, contemplando
incluso el caso en el que se produce una pequeña perturbación del dato
en el instante inicial (por ejemplo, debida a errores de
redondeo). Así, planteamos el siguiente esquema perturbado:
\begin{equation}\left\{
\begin{aligned}
  &z_0 = \ycero + \mu_0 \\
  &\ddt \znn = f(\tn,\zn) + \delta_n,\quad n=0,\dots,N-1
\end{aligned}\right.
\label{eq:metodo-euler-perturbado}
\end{equation}
o, equivalentemente,
\begin{equation}
\left\{
\begin{aligned}
  &z_0 = \ycero + \mu_0 \\
  &\znn = \zn + h f(\tn,\zn) + h \delta_n,\quad n=0,\dots,N-1.
\end{aligned}
\right.
\label{eq:tema4:6}
\end{equation}

\begin{definition}
  \label{def:estabilidad}
  Para un problema diferencial~(\ref{eq:pvi}), el método de Euler se
  dirá estable si para todo $\varepsilon>0$ existe $\delta>0$ tal que,
  siempre que $|\mu_0|<\delta$ y $||\delta_n||_\infty<\delta$:
  \begin{equation*}
    ||\zn - \yn||_\infty<\varepsilon,
  \end{equation*}
  donde $\zn$ e $\yn$ son, respectivamente, las soluciones
  de~(\ref{eq:metodo-euler}) y de~(\ref{eq:tema4:6}).
\end{definition}

\begin{theorem}
  \label{thm:estabilidad-euler}
  Si $f$ es globalmente \globLipschitz (con constante $L$), entonces el método de
  Euler es estable. Más aún, se verifica la siguiente estimación de
  estabilidad:
  \begin{equation*}
    ||\zn - \yn||_\infty \le C_1|\mu_0| + C_2 ||\delta_n||_\infty,
  \end{equation*}
  donde
  \begin{equation}
    \label{eq:euler-ctes-estimacion-estabilidad}
    C_1=e^{(b-a)L}, \qquad C_2=\frac{e^{(b-a)L}-1}{L}.
  \end{equation}
\end{theorem}

\begin{proof}
  Este teorema es consecuencia del siguiente resultado (cuya
  demostración, basada en la suma parcial de cierta progresión
  geométrica, será omitida por brevedad, véase [Doubova-Guillén] en la
  bibliografía) que nos permite reducir, escribiendo en términos del
  valor inicial, una acotación recursiva:
  \begin{quote}
    \begin{lemma}
      \label{lem:2}
      Sean $\{a_n\}_{n\ge 0}$ y $\{b_n\}_{n\ge 0}$ dos sucesiones de
      números reales no negativos y sea $\Lambda>0$ de forma que:
      \begin{align*}
        a_{n+1} &\le (1+h\Lambda)a_n + h b_n, \quad \forall n\ge 0.
        \intertext{Entonces} a_n &\le e^{nh\Lambda}a_0 +
        \frac{e^{nh\Lambda}-1}{\Lambda}\,\max_{0\le k \le n-1}|b_k|,
        \quad \forall n\ge 0.
      \end{align*}
    \end{lemma}
  \end{quote}
  Para  acotar $z_n-y_n$, restaremos las
  expresiones de $\znn$ en~(\ref{eq:tema4:6}) y de $\ynn$
  en~(\ref{eq:metodo-euler}):
  \begin{align*}
    |\znn-\ynn| &\le |\zn-\yn| + h|f(\tn,\zn)-f(\tn,\yn)| + h |\delta_n|
    \\
                &\le |\zn-\yn| + hL |\zn-\yn| + h |\delta_n|
                \\
                &=(1+hL)|\zn-\yn| + h |\delta_n|,
  \end{align*}
  donde se ha usado el carácter \lipschitz de $f$.
  Aplicando el lema anterior para las sucesiones $a_n=z_n-y_n$ y $b_n=|\delta_n|$:
  \begin{align*}
    |z_n-y_n| &\le e^{nhL}\mu_0 +
    \frac{e^{nhL}-1}{L}\,||\delta_n||_\infty
    \\
    &\le e^{(b-a)L}\mu_0 +
    \frac{e^{(b-a)L}-1}{L}\,||\delta_n||_\infty,
  \end{align*}
  donde en la última igualdad se usó que (como $\tn = a + nh$)
  $nh=\tn-a\le (b-a)$.
\end{proof}

\subsection{Convergencia del método de Euler}
\label{sec:convergencia-euler}

Utilizando los resultados de consistencia y estabilidad anteriores, es
fácil demostrar la convergencia del método de Euler hacia la solución
exacta, en el sentido que definiremos a continuación.

\begin{definition}
  \label{def:error-discretizacion-pvi}
  Dado un método numérico para~(\ref{eq:pvi}), se llama error de
  discretización en el instante $\tn$ a la diferencia
  \begin{equation*}
    e_n = \sol(\tn)-\yn.
  \end{equation*}
  Por supuesto, $e_n=e_n(h)$, es decir, el error de discretización
  depende del paso $h$. Se llama error de discretización (global) al
  vector
  \begin{equation*}
    E = E(h) = (e_0,e_1,\dots,e_n).
  \end{equation*}
\end{definition}
\begin{definition}
  \label{def:convergencia-euler}
  Dado un problema de valor inicial, decimos que el método de Euler es
  convergente si (siempre que $\lim_{h\to 0} |e_0|=0$) se verifica:
  \begin{equation*}
    \lim_{h\to 0} ||E(h)||_\infty = 0.
  \end{equation*}
  Precisando más, diremos que el orden de convergencia es (al menos)
  $p$ si existen $C, C_0, h_0>0$ tales que
  \begin{equation*}
    ||E(h)||_\infty \le C h^p \quad \forall h<h_0,
  \end{equation*}
  siempre que el método parta de una condición inicial $y_0$ en la que
  $|e_0|\le C_0 h^p$.
\end{definition}

La clave para determinar la convergencia y el orden del método de
Euler está en el siguiente teorema, que es una consecuencia del
Teorema~\ref{thm:estabilidad-euler} (estabilidad del método de Euler).
\begin{theorem}
  \label{thm:euler:estimacion-error-discretizacion}
  Supongamos que $f$ es \globLipschitz. Entonces se verifica la
  siguiente estimación del error de discretización:
  \begin{equation}
    \label{eq:estimacion-discretizacion}
    ||E||_\infty \le C_1 |e_0| + C_2 ||\errCons||_\infty,
  \end{equation}
  donde $C_1$ y $C_2$ vienen dadas
  en~(\ref{eq:euler-ctes-estimacion-estabilidad}).
\end{theorem}
\begin{proof}
  Consideremos la sucesión de soluciones exactas $z_n=\sol(\tn)$. Según
  comprobamos en la sección~\ref{sec:estabilidad-euler}, esta sucesión
  es solución del esquema recursivo~(\ref{eq:tema4:7}), en el que la
  perturbación en el instante $\tn$ viene dada por
  $\delta_n=\errCons_n$ (el error de consistencia en el instante
  $\tn$), mientras que la (eventual) perturbación inicial es
  $\mu=\sol(a)-y_0=e_0$.

  Por tanto, si aplicamos el Teorema~\ref{thm:estabilidad-euler} para
  esta sucesión, tenemos que
  $$
  ||E_n|| = ||\zn - \yn||_\infty \le C_1|\mu_0| + C_2
  ||\delta_n||_\infty  = C_1|e_0| + C_2 ||\errCons||_\infty.
  $$
\end{proof}

Para terminar, la convergencia del método de Euler se puede obtener
como un simple corolario de la consistencia y de la estimación de
estabilidad dada en el teorema anterior:

\begin{corollary}[Convergencia y orden del método de Euler]
  \label{cor:euler:convergencia-y-orden}
  Si $f$ es \globLipschitz y además $\sol\in C^2([\ta,\tb])$, entonces el método de Euler es
  convergente y tiene orden 1. En concreto, si $|e_0|\le C_0 h$ (para
  alguna constante $C_0\ge 0$), entonces:
  \begin{equation}
    \label{eq:tema4:9}
    ||E||_\infty \le C_0 C_1 \, h + C_2 ||\sol''||_\infty \frac h2,
  \end{equation}
\end{corollary}

\begin{proof}
  Bajo estas condiciones, el teorema de
  consistencia~\ref{thm:consistencia-euler} nos garantiza que
  $||\errCons||_\infty \le \frac h2 ||\sol''||_\infty$. Aplicando el la
  estimación~(\ref{eq:estimacion-discretizacion}) junto a
  $|e_0|\le C_0 h$ llegamos directamente a~(\ref{eq:tema4:9}).
\end{proof}

\begin{remark}
  \label{rk:8}
  En la practica, es habitual que no exista ninguna perturbación
  inicial, es decir que tengamos exactamente que $y_0=\sol(a)$, por lo
  que $e_0=0$ y podemos suponer $C_0=0$. Por lo tanto, la
  desigualdad~(\ref{eq:tema4:9}) suele adquirir una expresión más
  sencilla:
    \begin{equation}
    \label{eq:tema4:9}
    ||E||_\infty \le C_2 ||\sol''||_\infty \frac h2.
  \end{equation}
\end{remark}

\begin{remark}
  Los resultados anteriores se pueden generalizar al esquema de Euler Implícito. Véase
  Véase [Doubova-Guillén] en la bibliografía.
\end{remark}

\section{Error y convergencia en métodos generales de un paso}
\label{sec:convergencia-metodos-1paso}

Por simplicidad, enunciaremos  el resto de los resultados para
funciones \globLipschitz, aunque todos ellos se pueden generalizar para
funciones \locLipschitz (en esquemas acotados). % Más aún, estudiaremos
% el orden de convergencia de este método, comprobando que el error
% converge a cero como una constante por $h$ (orden $1$).


Todos los métodos explícitos que hemos enunciado hasta el momento son
problemas de un paso, en los que para aproximar la solución en el
instante $\tnn$ se utiliza exclusivamente la información en la etapa
anterior (instante $\tn$). Específicamente:
\begin{definition}
  Un esquema de un paso para~(\ref{eq:pvi}) es todo aquel que se puede
  expresar de la siguiente forma:
  \begin{equation}
    \label{eq:pb-general-1paso}
    \left\{
      \begin{aligned}
        \text{Dado}\quad &y_0 = \ycero,\\
        \text{hallar}\quad &\ddt\ynn = \Phi(\tn,\yn, h), \quad
        n=0,\dots, N-1,
      \end{aligned}
    \right.
  \end{equation}
  donde $\Phi:[\ta,\tb]\times \Rset \times [0,h^*] \to \Rset$ es una
  función continua dada (que depende de $f$) y $h^*>0$.
\end{definition}

Los conceptos de consistencia, estabilidad y convergencia estudiados
en la sección anterior se pueden generalizar a problemas generales de
un paso, del tipo~(\ref{eq:pb-general-1paso}).

\begin{definition}
  Se llama error de consistencia (en la etapa $t_n$) del problema
  general de un paso~(\ref{eq:pb-general-1paso}) a:
  $${\cal E}_n=\ddt \sol(\tnn) - \Phi(\tn,\sol(\tn),h).$$
  El método es consistente si
  $$
  \lim_{h\to 0} ||{\cal E}||_\infty  = 0, \quad \text{donde } {\cal
    E}=({\cal E}_0,\dots,{\cal E}_{N-1}).
  $$
\end{definition}

\newcommand{\Dtotal}[2]{#2^{(#1)}}%\frac{d^{#1}}{dt^{#1}}#2}
Para enunciar el siguiente teorema de convergencia, definimos la
derivada total de $f$ de orden $n$ con respecto a $t$ (denotando por
$y=\sol(t)$ a la solución de la ecuación diferencial $y'=f(t,y)$ y
usando la regla de la cadena):
\begin{align*}
  % \Dtotal 0 f &= f \\
  \Dtotal 1 f(t,y) & = \frac{d}{dt}f(t,y) =
                     \dt f(t,y)
                     + f(t,y)\ \dy f (t,y).
  \\
  \vdots
  \\
  \Dtotal n f(t,y) &= \frac{d}{dt}\Dtotal{n-1}f(t,y) =
                     \dt \Dtotal {n-1} f(t,y)
                     + f(t,y) \ \dy\Dtotal {n-1}f (t,y).
\end{align*}
Recordemos el siguiente resultado de teoría de ecuaciones en derivadas
parciales:
\begin{quotation}
  \emph Si $f\in C^n([\ta,\tb]\times\Rset)$, entonces la solución
  maximal, $\sol(x)$, de~(\ref{eq:pvi}) verifica $\sol\in
  C^{n+1}([\ta,\tb]\times\Rset)$ y además $$g^{n+1)}(t)=\Dtotal n f(t,
  \sol(t)).$$
\end{quotation}

\begin{theorem}[Consistencia]~
  \label{thm:consitencia-pb-general-1paso}
  \begin{enumerate}
  \item Si $f\in C^1([\ta,\tb]\times\Rset)$,
    entonces~(\ref{eq:pb-general-1paso}) es consistente si y solo si
    \begin{equation*}
      \Phi(t,y,0) = f(t,y)\quad \forall y\in[\ta,\tb]\times\Rset.
    \end{equation*}
  \item Más aún, si $f\in C^p([\ta,\tb]\times\Rset)$ y las derivadas
    parciales con respecto a $h$ existen y son continuas hasta orden
    $p$, entonces
    $$ ||{\cal E}||_\infty<C h^p \quad \text{para algún $C>0$}$$
    si y sólo si:
    \begin{align*}
      \Phi(t,y,0) &= f(x,y), \\
      \frac{\partial\Phi}{\partial h}(t,y,0) &=
                                               \frac{1}{2}\Dtotal1 f(t,y),
      \\
      \frac{\partial^2\Phi}{\partial h^2}(t,y,0) &=
                                                   \frac{1}{3}\Dtotal2 f(t,y),
      \\
                  &\vdots
      \\
      \frac{\partial^{p-1}\Phi}{\partial h^{p-1}}(t,y,0) &=
                                                           \frac{1}{p} \Dtotal{p-1} f(t,y).
    \end{align*}
  \end{enumerate}
\end{theorem}

\begin{proof}
  Véase [Crouzeix-Mignot] (bibliografía).
\end{proof}

\begin{example}~
  \begin{enumerate}
  \item
    El método de Euler es consistente, con  $ ||{\cal E}||_\infty<C h $
  \item El método de Euler--Cauchy es consistente, con $ ||{\cal
      E}||_\infty<C h^2 $
  \end{enumerate}
\end{example}

Consideremos el siguiente esquema perturbado:

\begin{equation}
  \label{eq:pb-perturbado-general-1paso}
  \left\{
    \begin{aligned}
      \text{Dado}\quad &z_0 = \ycero+\mu_0,\\
      \text{hallar}\quad &\ddt z_{n+1} = \Phi(\tn,z_n, h) + \delta_n, \quad
      n=0,\dots, N-1,
    \end{aligned}
  \right.
\end{equation}

Como en la sección anterior, decimos que el esquema~(\ref{eq:pb-general-1paso})
es estable si para todo $\epsilon>0$ es posible conseguir
$$
||\yn - z_n||_\infty < \epsilon
$$
tomando $|\mu_0|<\delta$ y $||\delta_n||_\infty<\delta$.

\begin{theorem}
  Si $\Phi$ es globalmente \lipschitz,
  entonces~(\ref{eq:pb-general-1paso}) es estable. En concreto:
  $$
  ||\yn - z_n||_\infty < C_1|y_0-z_0| + C_2 ||\delta_n||_\infty,
  $$
  donde $C_1$ y $C_2$ están definidas como en el teorema de
  estabilidad de la sección anterior.
\end{theorem}
\begin{proof}
  La demostración es similar a la sección anterior: se trata de una
  consecuencia directa del lema de Gronwall discreto
  (Lema~\ref{lem:2}) junto al hecho de que $\Phi$ sea \lipschitz.
\end{proof}
A partir de lo anterior, podemos conseguir la convergencia de los
métodos generales de un paso. En concreto, si definimos el error de
discretización como en la sección anterior
(definición~\ref{def:error-discretizacion-pvi}), podemos demostrar
fácilmente el siguiente teorema de Convergencia:

\begin{theorem}[Teorema de Lax]
  Si el método general de un paso~(\ref{eq:pb-general-1paso}) es
  consistente y estable, entonces es convergente.
\end{theorem}

\begin{proof}
  Como en el caso del método de Euler, basta aplicar el Teorema de
  estabilidad a las sucesiones $\yn$ resultantes del
  método~(\ref{eq:pb-general-1paso}) y $z_n=\sol(\tn)$, teniendo en
  cuenta que en este caso $\delta_n={\cal E}_n$, el error de
  consistencia, que tiende a cero por ser el método convergente.
\end{proof}

\section{Métodos de Taylor. Métodos de \RK}

\subsection*{Métodos de Taylor}
\label{sec:metodos-de-taylor}

Suponiendo que la solución exacta, $y=\sol(t)$, de (\ref{eq:pvi})) es de
$C^{p+1}[\ta,\tb])$, podemos hacer su desarrollo de Taylor en $\tn$:
\begin{align*}
  \sol(\tnn)&=\sol(\tn+h) \\&= \sol(\tn) + h\sol'(\tn) + \frac{h^2}{2}
                              \sol''(\tn)
                              + \cdots
                              + \frac{h^p}{p!} \sol^{p)}(\tn)
                              + \frac{h^{p+1}}{(p+1)!} \sol^{p+1)}(\xi)
  \\&= \sol(\tn) + hf(\tn,\sol(\tn)) + \frac{h^2}{2}
      \Dtotal 1 f(\tn,\sol(\tn))
      + \cdots
      + \frac{h^p}{p!} \Dtotal {p-1} f(\tn,\sol(\tn))
      + O(h^{p+1}).
\end{align*}
Pasando $\sol(\tn)$ al primer miembro, dividiendo por
$h$ y aproximando $\yn\approx \sol(\tn)$, $\ynn\approx \sol(\tnn)$
llegamos a :
\begin{align*}
  \ddt \ynn &= \Phi(\tn,\yn,h),
              \intertext{con}
              \Phi(t,y,h) &= f(t,y) + \frac{h}{2}
                            \Dtotal 1 f(t,y)
                            + \cdots
                            + \frac{h^{p-1}}{p!} \Dtotal {p-1} f(t,y)
\end{align*}

Para esta $\Phi$ se verifica trivialmente el teorema de
consistencia. Además, es fácil ver que $\Phi$ es \lipschitz siempre
que las derivadas totales de $f$, $\Dtotal i f$, lo sean. Luego, en
este caso, el método de Taylor es de orden $p$.

Sin embargo, este método tiene un serio inconveniente que hace que no
se utilice en caso general: las expresiones de las derivadas totales
son complejas y difíciles de calcular (para cada función $f$, es
decir, para EDP considerada, requieren la evaluación de todas
derivadas parciales hasta orden $p-1$).


\subsection*{Métodos de \RK}

Los métodos de \RK se basan en la idea de intentar obtener
esquemas de un orden, $p$, sustituyendo el cálculo de derivadas
parciales de $f$ por la evaluación de $f$ en distintos puntos del
intervalo $[\tn,\tnn]$. Así, su formulación es del tipo:
\begin{align}
  \notag
  \text{Dado}\ y_0 &= \ycero,\\
  \label{eq:R-K-formula-general}
  \ddt y_{n+1} &= \sum_{i=1}^r b_ik_i\quad
                 n=0,\dots, N-1,
  \\
  \notag
  \text{donde}\ k_i &= f(t_n+h\, c_i,\ y_n + h\, d_{i}), \text{\
                      con }
                      \left\{
                      \begin{aligned}
                        & c_i\in [0,1], \text{ con } c_1=1,
                        \\ &d_{i} \text{ combinación
                          lineal de } k_1,\dots,k_{i-1}.
                      \end{aligned}
                             \right.
\end{align}
Si $b_1+b_2+\cdots+b_r=1$, entonces el método es consistente, pues
entonces $\Phi(t,y,0)=\sum_{i=1}^r b_ik_i=f(x,y)$. No es difícil
demostrar que si $f$ es globalmente \lipschitz, entonces el método es
estable (se deja como ejercicio). Por lo tanto, los métodos de
\RK son convergentes y el orden de convergencia dependerá de
los parámetros que definen a cada método concreto.

\begin{example}[Euler--Cauchy y Euler mejorado]
  Euler--Cauchy~(\ref{eq:euler-cauchy}) (de orden 2) es un método de \RK
  con $r=2$. En efecto, cada iteración en tiempo se puede escribir
  como:
  \begin{align*}
    k_1 &= f(\tn,\yn), \\ k_2 &= f\big(\tt_{n}+\frac{h}{2},\; \yn+\frac{h}{2}k_1 \big),
    \\
    \ynn &= \yn + h \; \big( b_1k_1 + b_2 k_2 \big),
           \quad
           \text{ con } b_1=0,\ b_2=1.
  \end{align*}

  De forma similar, Euler mejorado~(\ref{eq:euler-mejorado}) se puede escribir como un método de
  \RK con $r=2$.
\end{example}

\begin{example}[Método clásico de \RK]
  Pueden construirse métodos de \RK de orden arbitrario
  (aunque el proceso de construcción es laborioso). El método de este
  tipo más utilizado, por ser sencillo y fácil de programar, tiene
  orden $4$ y se conoce como \resaltar{método clásico de \RK}:
  \begin{center}
    \includegraphics[width=0.9\linewidth]{tema4/R-K-clasico}
  \end{center}
\end{example}

\section{Métodos multipaso}
\label{sec:metodos-multipaso}

Los métodos numéricos de $k$ son aquellos en los que, para calcular
$y_{n+k}$ (una aproximación de $\sol(t_{n+k})$), se utilizan
aproximaciones de la solución en $k$ etapas de tiempo anteriores,
\begin{equation*}
  y_n,\  y_{n+1},\ \dots, y_{n+k-1} \quad \longmapsto \quad y_{n+k}.
\end{equation*}
Específicamente:
\begin{definition}
  Un método numérico (lineal) de $k$ pasos para aproximar la solución
  de~(\ref{eq:pvi}) es todo aquel en el que la solución en
  la etapa $n+k$ viene dada por una expresión recursiva de la forma:
  \begin{align}
    \label{eq:metodo-multipaso}
    \ddt y_{n+k}=&\sum_{j=0}^k \beta_j f(t_{n+j},y_{n+j}), \quad
    n=0,\dots,N-k, \intertext{donde $\ddt y_{n+k}$ es un aproximación
      de $\sol'(t_{n+k})$ usando un cociente incremental con $k$ pasos:}
    \ddt
    \label{eq:cociente-incremental-multipaso}
    y_{n+k}&=\frac{1}{h}\sum_{j=0}^k\alpha_j y_{n+j}.
  \end{align}
\end{definition}
En las expresiones anteriores, $\alpha_j, \beta_j\in\Rset$, con
$\alpha_k\neq 0$. Por simplificar, supondremos $$\alpha_k=1$$ (lo que
equivale a dividir todos los coeficientes por $\alpha_k$).

Por ejemplo, los métodos de Euler (explícito e implícito) y el esquema
de Crank-Nicolson~(\ref{eq:crank-nicolson-2}) son métodos de este tipo
con un único paso $k=1$. Más adelante se muestran algunos métodos con
$k>1$.
Con el fin de abreviar, con frecuencia usaremos la siguiente notación:
$$f_n=f(\tn,\yn).$$

Puesto que en un método de $k$ pasos del
tipo~(\ref{eq:metodo-multipaso}) se necesitan $k$ aproximaciones
previas para calcular $y_{n+k}$, en la primera iteración ($n=0$) es
necesario disponer de $k$ valores de arranque, $y_0$, $y_1$, \dots,
$y_{k-1}$ (que determinarán $y_k$). Pero en~(\ref{eq:pvi}) sólo
contamos con un dato inicial, $\ycero$, de forma que necesitaremos
usar algún método numérico para aproximar $y_1$, $y_2$, \dots,
$y_{k-1}$. Habitualmente se usarán métodos de un paso, con especial
cuidado en que el orden de estos métodos sea (al menos) igual al
del método multipaso (pues, de otra forma, perderíamos calidad en la
aproximación inicial).

\begin{enumerate}
\item Si $\beta_k=0$, el método se dice \emph{explícito}. El término
  $f_{n+k}=f(t_{n+k},y_{n+k})$ no aparece en el segundo miembro
  de~(\ref{eq:metodo-multipaso}) del método multipaso, en
  consecuencia, usando~(\ref{eq:cociente-incremental-multipaso}),
  $y_{n+k}$ puede expresarse explícitamente:
  \begin{equation*}
    y_{n+k}=
    h\sum_{j=0}^{k-1} \beta_j f_{n+j}-\sum_{j=0}^{k-1}\alpha_j y_{n+j}.
  \end{equation*}
\item Si $\beta_k\neq 0$, el método se dice \emph{implícito}. No podemos
  expresar explícitamente el término $y_{n+k}$ y, en cada paso,
  para calcular este término será necesario resolver la ecuación (no
  lineal, en general):
  \begin{equation*}
    y_{n+k} - \beta_k f(t_{n+j},y_{n+j})=
     h\sum_{j=0}^{k-1} \beta_j f(t_{n+j},y_{n+j})- \sum_{j=0}^{k-1}\alpha_j y_{n+j} .
  \end{equation*}
  Para resolver este tipo de ecuaciones existen distintas
  posibilidades, las más habituales de las cuales son el método de
  aproximaciones sucesivas (o de punto fijo) y el método de Newton
  (para asegurar la convergencia, de estos métodos, será necesario
  imponer hipótesis adicionales a $f$).
\end{enumerate}

\subsection*{Construcción de métodos multipaso lineales}

Para deducir algunos métodos multipaso utilizaremos la formulación
integral de la ecuación diferencial
$$
y'=f(t,y),
$$
es decir fijados $t,t^*\in[a,b]$ escribimos
\begin{equation*}
  \sol(t^*)-\sol(t) = \int_{t}^{t^*} f(x,\sol(x))\, dx.
\end{equation*}
Para cada $n=0,\dots,N-k$, elegimos $t=x_{n+k-1}$ y $t^*=x_{n+k}$ y
denotando $y_{n+j}\approx \sol(t_{n+j})$ a las aproximaciones que
deseamos calcular,  ponemos en práctica la siguiente idea:
  % realizar, en esta formulación integral las siguiente
% aproximación:
\begin{enumerate}
\item Elegir $t=x_{n+k-1}$ y $t^*=x_{n+k}$
% \item Elegir $t$ y $t^*$ adecuadamente.
% \item Sustituir la función $F(t)=f(t,\sol(t))$ por su polinomio de
%   interpolación de Lagrange, $P(t)$, en los nodos $t_n,\ t_{n+1},\
%   \dots, t_{n+k}$.
\item Sustituir la función $F(x)=f(x,\sol(x))$ por su polinomio de
  interpolación de Lagrange, $P(x)$, en un soporte
  $S\subset \{t_{n+j}\}_{j=0}^k$ adecuado:%  interpola los valores
  % $\{f(t_{n+j},y_{n+j})\}_{j=0}^k$.
  \begin{enumerate}
  \item Métodos de \emph{\AB}:
    \begin{equation*}
      S= \{t_{n+j}\}_{j=0}^{k-1}.
    \end{equation*}
    Como veremos, se trata de métodos explícitos, pues en la
    aproximación no aparece el término
    $f(t_{n+k},y_{n+k})$.
  \item  Métodos de \emph{\AM}:
    \begin{equation*}
      S= \{t_{n+j}\}_{j=0}^{k},
    \end{equation*}
    dando lugar a métodos implícitos.
  \end{enumerate}
\end{enumerate}

\begin{example}[Método de \AB de dos pasos (orden 2)]
  \label{sec:AB-dos-pasos}
  Consideremos la siguiente expresión integral:
  \begin{equation}
   \label{eq:tema4:8}
   \sol(t_{n+2})-\sol(t_{n+1}) \approx \int_{t_{n+1}}^{t_{n+2}} P(t)\,dt,
  \end{equation}
  donde $P(t)$ es el polinomio de interpolación de $F(t)=f(t,\sol(t))$ en
  $t_n$ y $t_{n+1}$. Para construirlo, podemos usar la fórmula de
  diferencias dividas de Newton o bien la fórmula de Lagrange. En este
  último caso:
  \begin{equation*}
   P(t)=F(t_n)\frac{t-t_{n+1}}{t_n-t_{n+1}} +
   F(t_{n+1})\frac{t-t_{n}}{t_{n+1}-t_{n}}.
  \end{equation*}
  Con un simple cálculo (puede ser útil el cambio de variable $t =
  t_{n+1}+h\,s$, que transforma $t\in [t_{n+1},t_{n+2}]$ en $s\in [0,1]$)
  el segundo miembro de~(\ref{eq:tema4:8}) resulta:
  \begin{equation*}
  \int_{t_{n+1}}^{t_{n+2}} P(t)\,dt = \frac{h}{2} \big[
  3(f(t_{n+1},\sol(t_{n+1})) - f(t_{n},\sol(t_{n})) \big].
  \end{equation*}
  Planteamos, por tanto, la siguiente aproximación: dados $y_n$,
  $y_{n+1}$, calcular $y_{n+2}$ como solución de la
  siguiente fórmula de recurrencia:
  \begin{equation}
    \tag{AB2}
    \label{eq:AB2}
    y_{n+2}-y_{n+1} = \frac{h}{2} \big[
    3f(t_{n+1},y_{n+1}) - f(t_{n},y_{n}) \big],
  \end{equation}
  de donde, para cada $n=0,\dots,N-2$ podemos obtener explícitamente el
  valor $y_{n+2}$.

  El valor $y_0$ se puede obtener a partir del dato inicial $\ycero$,
  mientras que para el cálculo de $y_1$ será necesario utilizar un
  método numérico de un paso. Se puede demostrar que este método
  de \AB tiene orden 2, por tanto el método para inicializar debe
  tener el mismo orden (por ejemplo, Euler--Cauchy o Euler mejorado).
\end{example}

\begin{example}[Método de \AM de dos pasos (orden 2)]
  Si consideramos la ecuación integral (\ref{eq:tema4:8}) pero
  definimos $P(t)$ como el polinomio de interpolación de
  $f(t,\sol(t))$ en $\{t_n,t_{n+1},t_{n+2}\}$, obtenemos el método
  de \AM de dos pasos. Procediendo como en el ejemplo anterior (lo que
  se deja como ejercicio) se obtiene:
  \begin{equation}
    \tag{AM2}
    \label{eq:AM2}
      y_{n+2}-y_{n+1} = \frac{h}{12} \big[
      5f(t_{n+2},y_{n+2}) + 8 f(t_{n+1},y_{n+1}) - f(t_{n},y_{n}) \big].
  \end{equation}
  Se puede demostrar que el orden de este método es igual a
  $2$. Puesto que se trata de un método implícito, en principio será
  necesario usar algún método numérico (por ejemplo, iteraciones de
  punto fijo) en cada instante $\tn$ para hallar $y_{n+2}$.
\end{example}

\begin{example}[Método predictor-corrector~(\ref{eq:AB2})--(\ref{eq:AM2})]
  Con frecuencia, los métodos de \AB y \AM del mismo orden se suelen
  combinar para dar lugar a nuevos métodos numéricos. La idea es, en
  cada etapa $t_n$:
  \begin{enumerate}
  \item Utilizar el método de \AB para obtener, de forma explícita una
    primera aproximación (predicción) de $y_{n+k}$.
  \item Utilizar la predicción obtenida para aproximar
    $f(t_{n+k},y_{n+k})$ y así calcular, mediante el método de \AM,
    una nueva aproximación (corrección) de $y_{n+k}$.
  \end{enumerate}

  En el caso~(\ref{eq:AB2})--(\ref{eq:AM2}) cada etapa del método
  queda:
  \begin{equation*}\left\{
      \begin{aligned}
        y^*_{n+2} &= y_{n+1} + \frac{h}{2} \big[ 3f(t_{n+1},y_{n+1}) -
        f(t_{n},y_{n}) \big] \quad \text{(predicción),}
        \\
        y_{n+2} &= y_{n+1} + \frac{h}{12} \big[ 5f(t_{n+2},y^*_{n+2})
        + 8 f(t_{n+1},y_{n+1}) - f(t_{n},y_{n}) \big] \quad
        \text{(corrección).}
      \end{aligned}\right.
  \end{equation*}
  Así obtenemos un método explícito que saca partida a las
  características de los esquemas de \AB y AM.
  \end{example}

  % ---------------------------
  \begin{EjerciciosPropuestos}
    %--------------------------

    \begin{problema}
      Consideremos el problema de valor inicial $y'=\cos(t+y)$ en
      $[0,\pi]$, $y(0)=1$. Calcular\footnote{Comprobar que
        $|\partial_y f(t,y)|\le 1$ y por tanto se puede tomar la
        constante de Lipschitz $L=1$.} el paso, $h$, necesario para que
        la aproximación de este problema mediante el método de Euler
        (explícito) tenga un error de discretización menor que
        $10^{-5}$.
    \end{problema}

    \begin{problema}
      \begin{enumerate}
      \item Demostrar el siguiente lema técnico\footnote{Idea: desarrollar la
        sucesión recursiva, acotar todos los $b_n$ por $\max_n|b_n|$ y calcular
        la suma de la serie geométrica correspondiente.}: sean dos sucesiones de
        números reales no negativos, $\{a_n\}_{n\ge 0}$ y
        $\{b_n\}_{n\ge 0}$ y sea $K>0$ para los que se verifica la
        siguiente relación recursiva:
        $$
        a_{n+1} \le K a_n + b_n.
        $$
        Entonces:
        \begin{equation*}
         a_n \le K^n a_0 + \frac{K^n-1}{K-1}\max_n|b_n|.
        \end{equation*}

      \item Utilizar el resultado anterior para demostrar\footnote{Idea: usar que $1+x\le e^x$ para todo $x\ge 0$.} el Lema~\ref{lem:2}.
      \end{enumerate}
    \end{problema}

  \begin{problema}
    Consideremos el método de Euler Implícito definido
    en~(\ref{eq:euler-implicito}) para un problema de valor
    inicial~(\ref{eq:pvi}) dado por una función
    $f\in C([a,b]\times\Rset)$ globalmente Lispschitziana (respecto
    a su segunda variable, $y$). Sea $h>0$ tal que se verifica la
    condición $hL<1$ (donde $L$ es la constante de Lipschtiz para
    $f$).
    \begin{enumerate}
    \item Demostrar que, en estas condiciones, el método de Euler
      implícito está bien definido, es decir, para cada $n\in\{0,\dots,N-1\}$
      existe una única $\ynn$ solución
      de~(\ref{eq:euler-implicito})\footnote{Idea: demostrar la
        contractividad de una función adecuada.}.
    \item Para este método define el error de consistencia $\errCons(h)=(\errCons_0,\dots,\errCons_{N-1})$, donde
      \begin{equation*}
        \label{eq:consistencia-euler-impl}
        \errCons_{n} = \ddt \sol(\tnn)-f(\tnn,\sol(\tnn)).
      \end{equation*}
      Demostrar que el método de Euler Implícito es consistente con
      orden de consistencia igual a uno. Y en concreto\footnote{Idea:
        adaptar la demostración del
        Teorema~\ref{thm:consistencia-euler}.}:
      $$
      \|\errCons(h)\|_{\infty} \le \frac h 2  \|g''\|_{\infty}.
      $$

    \item Definir el concepto de estabilidad aplicado al método de
      Euler Implícito y demostrar que este método es
      estable\footnote{Intentar escribir de forma recursiva el error
        respecto a una sucesión perturbada de tal forma que se pueda
        aplicar el Lema~\ref{lem:2}.}.

      \item Utilizado lo anterior, demostrar que el método de Euler
        implícito es convergente con orden 1.
    \end{enumerate}

  \end{problema}

  \begin{problema}
    Demostrar que para toda función $g\in C^3([a,b])$ existe una
    constante $C$ (independiente de $h$) para la que se verifican las
    siguientes desigualdades\footnote{Solución: para~(\ref{eq:ejer.4}) aproximar
      $g(t_{n})$ y $g(t_{n+1})$ mediante desarrollos de Taylor en
      $t_{n+1/2}$. Para~(\ref{eq:ejer.3}), aproximar $g(t_n)$ y
      $g(t_{n+1})$ mediante desarrollos en $t_{n+1}$ y $t_n$ y
      restar.}:
    \begin{align}
      \label{eq:ejer.3}
      \Big| \ddt g(t_{n+1}) &- \frac{g'(t_{n})+ g'(t_{n+1})}2 \Big| \le C h^2,
      \\
      \Big| \ddt g(t_{n+1}) &- g'(t_{n+1/2}) \Big| \le C h^2,
      \label{eq:ejer.4}
    \end{align}
    donde se usó la notación usual, $t_n, t_{n+1}\in [a,b]$,
    $h=t_{n+1}-t_n$, $t_{n+1/2}=t_{n}+h/2$.  \textit{Observación: las
      desigualdades anteriores pueden interpretarse de la siguiente
      forma: $\ddt g(t_{n+1})$ es una aproximación de orden 2 de $g'$
      en el punto medio.}
  \end{problema}

  \begin{problema}
    Consideremos el método de Crank-Nicolson definido en~(\ref{eq:crank-nicolson-2})
    \begin{enumerate}
    \item Justificar la siguiente definición error de consistencia (en
      el instante $t_{n+1}$ para este método:
      \begin{equation*}
        \errCons_n = \ddt g(t_{n+1}) - \frac 12 \big( f(t_n, g(t_n)) + f(t_{n+1},g(t_{n+1}))\big),
      \end{equation*}
      donde $g(\cdot)$ denota a la solución exacta del PVI.
    \item Demostrar que, si la solución exacta, $g$, verifica
      $g\in C^3([a,b])$, el método anterior es consistente, con orden
      de consistencia igual a dos.
    \end{enumerate}
  \end{problema}

  \begin{problema}
    Consideremos el método de Euler-Cauchy~(\ref{eq:euler-mejorado})
    \begin{enumerate}
    \item Introducir el concepto de error de consistencia para este método.
    \item Demostrar que si la solución exacta $g$ es de clase
      $C^3([a,b])$, entonces el método tiene orden de consistencia 2.
    \item Comprobar que, si $f$ es \globLipschitz, entonces el método
      de Euler-Cauchy es estable\footnote{Comprobar que la función
        $\Phi$ que define al método de Euler-Cauchy como método de un
        paso es Lipschitziana.}
    \item Definir el error de discretización para este método y
      concluir que el esquema es convergente y su error de
      discretización tiene orden dos.
    \end{enumerate}
  \end{problema}
  % \begin{problema}
  %   Demostrar que si la solución exacta $g$ del problema de valor
  %   inicial~(\ref{eq:pvi}) verifica $g\in C^3([a,b])$, entonces el
  %   método de Heun es consistente
  % \end{problema}
\end{EjerciciosPropuestos}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../tema4.tex"
%%% End:
